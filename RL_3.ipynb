{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de45611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24: episode 1 done, reward=23.00, epsilon=1.00\n",
      "39: episode 2 done, reward=15.00, epsilon=0.93\n",
      "64: episode 3 done, reward=25.00, epsilon=0.72\n",
      "74: episode 4 done, reward=10.00, epsilon=0.66\n",
      "93: episode 5 done, reward=19.00, epsilon=0.54\n",
      "112: episode 6 done, reward=19.00, epsilon=0.45\n",
      "134: episode 7 done, reward=22.00, epsilon=0.36\n",
      "144: episode 8 done, reward=10.00, epsilon=0.32\n",
      "157: episode 9 done, reward=13.00, epsilon=0.28\n",
      "171: episode 10 done, reward=14.00, epsilon=0.25\n",
      "181: episode 11 done, reward=10.00, epsilon=0.22\n",
      "194: episode 12 done, reward=13.00, epsilon=0.20\n",
      "203: episode 13 done, reward=9.00, epsilon=0.18\n",
      "213: episode 14 done, reward=10.00, epsilon=0.16\n",
      "224: episode 15 done, reward=11.00, epsilon=0.15\n",
      "234: episode 16 done, reward=10.00, epsilon=0.13\n",
      "243: episode 17 done, reward=9.00, epsilon=0.12\n",
      "251: episode 18 done, reward=8.00, epsilon=0.11\n",
      "260: episode 19 done, reward=9.00, epsilon=0.10\n",
      "270: episode 20 done, reward=10.00, epsilon=0.09\n",
      "281: episode 21 done, reward=11.00, epsilon=0.08\n",
      "291: episode 22 done, reward=10.00, epsilon=0.07\n",
      "302: episode 23 done, reward=11.00, epsilon=0.07\n",
      "312: episode 24 done, reward=10.00, epsilon=0.06\n",
      "322: episode 25 done, reward=10.00, epsilon=0.05\n",
      "333: episode 26 done, reward=11.00, epsilon=0.05\n",
      "344: episode 27 done, reward=11.00, epsilon=0.04\n",
      "353: episode 28 done, reward=9.00, epsilon=0.04\n",
      "362: episode 29 done, reward=9.00, epsilon=0.04\n",
      "371: episode 30 done, reward=9.00, epsilon=0.03\n",
      "380: episode 31 done, reward=9.00, epsilon=0.03\n",
      "393: episode 32 done, reward=13.00, epsilon=0.03\n",
      "403: episode 33 done, reward=10.00, epsilon=0.02\n",
      "414: episode 34 done, reward=11.00, epsilon=0.02\n",
      "426: episode 35 done, reward=12.00, epsilon=0.02\n",
      "435: episode 36 done, reward=9.00, epsilon=0.02\n",
      "445: episode 37 done, reward=10.00, epsilon=0.02\n",
      "455: episode 38 done, reward=10.00, epsilon=0.01\n",
      "464: episode 39 done, reward=9.00, epsilon=0.01\n",
      "474: episode 40 done, reward=10.00, epsilon=0.01\n",
      "485: episode 41 done, reward=11.00, epsilon=0.01\n",
      "494: episode 42 done, reward=9.00, epsilon=0.01\n",
      "504: episode 43 done, reward=10.00, epsilon=0.01\n",
      "514: episode 44 done, reward=10.00, epsilon=0.01\n",
      "524: episode 45 done, reward=10.00, epsilon=0.01\n",
      "534: episode 46 done, reward=10.00, epsilon=0.01\n",
      "543: episode 47 done, reward=9.00, epsilon=0.01\n",
      "552: episode 48 done, reward=9.00, epsilon=0.01\n",
      "562: episode 49 done, reward=10.00, epsilon=0.00\n",
      "575: episode 50 done, reward=13.00, epsilon=0.00\n",
      "586: episode 51 done, reward=11.00, epsilon=0.00\n",
      "597: episode 52 done, reward=11.00, epsilon=0.00\n",
      "609: episode 53 done, reward=12.00, epsilon=0.00\n",
      "617: episode 54 done, reward=8.00, epsilon=0.00\n",
      "628: episode 55 done, reward=11.00, epsilon=0.00\n",
      "642: episode 56 done, reward=14.00, epsilon=0.00\n",
      "653: episode 57 done, reward=11.00, epsilon=0.00\n",
      "664: episode 58 done, reward=11.00, epsilon=0.00\n",
      "675: episode 59 done, reward=11.00, epsilon=0.00\n",
      "685: episode 60 done, reward=10.00, epsilon=0.00\n",
      "700: episode 61 done, reward=15.00, epsilon=0.00\n",
      "712: episode 62 done, reward=12.00, epsilon=0.00\n",
      "722: episode 63 done, reward=10.00, epsilon=0.00\n",
      "732: episode 64 done, reward=10.00, epsilon=0.00\n",
      "743: episode 65 done, reward=11.00, epsilon=0.00\n",
      "753: episode 66 done, reward=10.00, epsilon=0.00\n",
      "764: episode 67 done, reward=11.00, epsilon=0.00\n",
      "775: episode 68 done, reward=11.00, epsilon=0.00\n",
      "786: episode 69 done, reward=11.00, epsilon=0.00\n",
      "799: episode 70 done, reward=13.00, epsilon=0.00\n",
      "811: episode 71 done, reward=12.00, epsilon=0.00\n",
      "821: episode 72 done, reward=10.00, epsilon=0.00\n",
      "833: episode 73 done, reward=12.00, epsilon=0.00\n",
      "846: episode 74 done, reward=13.00, epsilon=0.00\n",
      "857: episode 75 done, reward=11.00, epsilon=0.00\n",
      "868: episode 76 done, reward=11.00, epsilon=0.00\n",
      "880: episode 77 done, reward=12.00, epsilon=0.00\n",
      "893: episode 78 done, reward=13.00, epsilon=0.00\n",
      "906: episode 79 done, reward=13.00, epsilon=0.00\n",
      "923: episode 80 done, reward=17.00, epsilon=0.00\n",
      "936: episode 81 done, reward=13.00, epsilon=0.00\n",
      "947: episode 82 done, reward=11.00, epsilon=0.00\n",
      "957: episode 83 done, reward=10.00, epsilon=0.00\n",
      "971: episode 84 done, reward=14.00, epsilon=0.00\n",
      "984: episode 85 done, reward=13.00, epsilon=0.00\n",
      "997: episode 86 done, reward=13.00, epsilon=0.00\n",
      "1019: episode 87 done, reward=22.00, epsilon=0.00\n",
      "1032: episode 88 done, reward=13.00, epsilon=0.00\n",
      "1045: episode 89 done, reward=13.00, epsilon=0.00\n",
      "1058: episode 90 done, reward=13.00, epsilon=0.00\n",
      "1072: episode 91 done, reward=14.00, epsilon=0.00\n",
      "1088: episode 92 done, reward=16.00, epsilon=0.00\n",
      "1103: episode 93 done, reward=15.00, epsilon=0.00\n",
      "1119: episode 94 done, reward=16.00, epsilon=0.00\n",
      "1135: episode 95 done, reward=16.00, epsilon=0.00\n",
      "1154: episode 96 done, reward=19.00, epsilon=0.00\n",
      "1170: episode 97 done, reward=16.00, epsilon=0.00\n",
      "1188: episode 98 done, reward=18.00, epsilon=0.00\n",
      "1205: episode 99 done, reward=17.00, epsilon=0.00\n",
      "1217: episode 100 done, reward=12.00, epsilon=0.00\n",
      "1229: episode 101 done, reward=12.00, epsilon=0.00\n",
      "1243: episode 102 done, reward=14.00, epsilon=0.00\n",
      "1287: episode 103 done, reward=44.00, epsilon=0.00\n",
      "1314: episode 104 done, reward=27.00, epsilon=0.00\n",
      "1333: episode 105 done, reward=19.00, epsilon=0.00\n",
      "1357: episode 106 done, reward=24.00, epsilon=0.00\n",
      "1375: episode 107 done, reward=18.00, epsilon=0.00\n",
      "1400: episode 108 done, reward=25.00, epsilon=0.00\n",
      "1432: episode 109 done, reward=32.00, epsilon=0.00\n",
      "1453: episode 110 done, reward=21.00, epsilon=0.00\n",
      "1470: episode 111 done, reward=17.00, epsilon=0.00\n",
      "1495: episode 112 done, reward=25.00, epsilon=0.00\n",
      "1517: episode 113 done, reward=22.00, epsilon=0.00\n",
      "1562: episode 114 done, reward=45.00, epsilon=0.00\n",
      "1598: episode 115 done, reward=36.00, epsilon=0.00\n",
      "1616: episode 116 done, reward=18.00, epsilon=0.00\n",
      "1659: episode 117 done, reward=43.00, epsilon=0.00\n",
      "1716: episode 118 done, reward=57.00, epsilon=0.00\n",
      "1733: episode 119 done, reward=17.00, epsilon=0.00\n",
      "1780: episode 120 done, reward=47.00, epsilon=0.00\n",
      "1818: episode 121 done, reward=38.00, epsilon=0.00\n",
      "1873: episode 122 done, reward=55.00, epsilon=0.00\n",
      "1911: episode 123 done, reward=38.00, epsilon=0.00\n",
      "1989: episode 124 done, reward=78.00, epsilon=0.00\n",
      "2040: episode 125 done, reward=51.00, epsilon=0.00\n",
      "2091: episode 126 done, reward=51.00, epsilon=0.00\n",
      "2140: episode 127 done, reward=49.00, epsilon=0.00\n",
      "2162: episode 128 done, reward=22.00, epsilon=0.00\n",
      "2192: episode 129 done, reward=30.00, epsilon=0.00\n",
      "2222: episode 130 done, reward=30.00, epsilon=0.00\n",
      "2254: episode 131 done, reward=32.00, epsilon=0.00\n",
      "2291: episode 132 done, reward=37.00, epsilon=0.00\n",
      "2335: episode 133 done, reward=44.00, epsilon=0.00\n",
      "2376: episode 134 done, reward=41.00, epsilon=0.00\n",
      "2430: episode 135 done, reward=54.00, epsilon=0.00\n",
      "2500: episode 136 done, reward=70.00, epsilon=0.00\n",
      "2536: episode 137 done, reward=36.00, epsilon=0.00\n",
      "2561: episode 138 done, reward=25.00, epsilon=0.00\n",
      "2609: episode 139 done, reward=48.00, epsilon=0.00\n",
      "2670: episode 140 done, reward=61.00, epsilon=0.00\n",
      "2712: episode 141 done, reward=42.00, epsilon=0.00\n",
      "2740: episode 142 done, reward=28.00, epsilon=0.00\n",
      "2771: episode 143 done, reward=31.00, epsilon=0.00\n",
      "2813: episode 144 done, reward=42.00, epsilon=0.00\n",
      "2867: episode 145 done, reward=54.00, epsilon=0.00\n",
      "2903: episode 146 done, reward=36.00, epsilon=0.00\n",
      "2955: episode 147 done, reward=52.00, epsilon=0.00\n",
      "2985: episode 148 done, reward=30.00, epsilon=0.00\n",
      "3023: episode 149 done, reward=38.00, epsilon=0.00\n",
      "3057: episode 150 done, reward=34.00, epsilon=0.00\n",
      "3088: episode 151 done, reward=31.00, epsilon=0.00\n",
      "3113: episode 152 done, reward=25.00, epsilon=0.00\n",
      "3177: episode 153 done, reward=64.00, epsilon=0.00\n",
      "3207: episode 154 done, reward=30.00, epsilon=0.00\n",
      "3255: episode 155 done, reward=48.00, epsilon=0.00\n",
      "3322: episode 156 done, reward=67.00, epsilon=0.00\n",
      "3441: episode 157 done, reward=119.00, epsilon=0.00\n",
      "3499: episode 158 done, reward=58.00, epsilon=0.00\n",
      "3583: episode 159 done, reward=84.00, epsilon=0.00\n",
      "3629: episode 160 done, reward=46.00, epsilon=0.00\n",
      "3673: episode 161 done, reward=44.00, epsilon=0.00\n",
      "3761: episode 162 done, reward=88.00, epsilon=0.00\n",
      "3812: episode 163 done, reward=51.00, epsilon=0.00\n",
      "3876: episode 164 done, reward=64.00, epsilon=0.00\n",
      "3968: episode 165 done, reward=92.00, epsilon=0.00\n",
      "4019: episode 166 done, reward=51.00, epsilon=0.00\n",
      "4085: episode 167 done, reward=66.00, epsilon=0.00\n",
      "4177: episode 168 done, reward=92.00, epsilon=0.00\n",
      "4283: episode 169 done, reward=106.00, epsilon=0.00\n",
      "4426: episode 170 done, reward=143.00, epsilon=0.00\n",
      "4589: episode 171 done, reward=163.00, epsilon=0.00\n",
      "Whee!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from ptan.experience import ExperienceFirstLast, ExperienceSourceFirstLast\n",
    "import ptan\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import typing as tt\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "TGT_NET_SYNC = 10\n",
    "GAMMA = 0.9\n",
    "REPLAY_SIZE = 1000\n",
    "LR = 1e-3\n",
    "EPS_DECAY = 0.99\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def unpack_batch(batch: tt.List[ExperienceFirstLast], net: Net, gamma: float):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.as_tensor(np.stack(states))\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards)\n",
    "    last_states_v = torch.as_tensor(np.stack(last_states))\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v * gamma + rewards_v\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.ArgmaxActionSelector()\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1, selector=selector)\n",
    "    agent = ptan.agent.DQNAgent(net, selector)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n",
    "    optimizer = optim.Adam(net.parameters(), LR)\n",
    "\n",
    "    step = 0\n",
    "    episode = 0\n",
    "    solved = False\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "        buffer.populate(1)\n",
    "\n",
    "        for reward, steps in exp_source.pop_rewards_steps():\n",
    "            episode += 1\n",
    "            print(f\"{step}: episode {episode} done, reward={reward:.2f}, \"\n",
    "                  f\"epsilon={selector.epsilon:.2f}\")\n",
    "            solved = reward > 150\n",
    "        if solved:\n",
    "            print(\"Whee!\")\n",
    "            break\n",
    "        if len(buffer) < 2*BATCH_SIZE:\n",
    "            continue\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        states_v, actions_v, tgt_q_v = unpack_batch(batch, tgt_net.target_model, GAMMA)\n",
    "        optimizer.zero_grad()\n",
    "        q_v = net(states_v)\n",
    "        q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        loss_v = F.mse_loss(q_v, tgt_q_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        selector.epsilon *= EPS_DECAY\n",
    "\n",
    "        if step % TGT_NET_SYNC == 0:\n",
    "            tgt_net.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a026b2d",
   "metadata": {},
   "source": [
    "## A random agent on the CliffWalking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0b1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_random\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_random\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_random\\rl-video-episode-0.mp4\n",
      "Total reward: -496.0, Total steps: 100\n",
      "Video of the random agent has been saved in the 'videos_cliffwalking_random' folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Create the Atari environment Breakout.\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Wrap the environment to record a video.\n",
    "# The video will be saved in a \"videos_cliffwalking_random\" folder.\n",
    "env = RecordVideo(env, video_folder=\"./videos_cliffwalking_random\")\n",
    "\n",
    "total_reward = 0.0 \n",
    "total_steps = 0\n",
    "# The following line is needed to start the recording.\n",
    "# The reset method returns the initial observation.\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Run the environment for a total of 1000 steps.\n",
    "for _ in range(100):\n",
    "    # Take a random action from the environment's action space.\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # The step method returns the next observation, the reward, whether the episode is terminated or truncated, and additional info.\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    # If the episode is over, reset the environment.\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Close the environment.\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}, Total steps: {total_steps}\") \n",
    "print(\"Video of the random agent has been saved in the 'videos_cliffwalking_random' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15551a",
   "metadata": {},
   "source": [
    "## Fathoming naive DQN algorithm in depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965dc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abe6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = nn.Linear(4, 3)\n",
    "target_net = nn.Linear(4, 3) # Here both the policy net and target net are equivalent to linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bc5b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=3, bias=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61047c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample(here two samples are used) of (state, action, reward, next_state, done) tuples, the state \n",
    "# has 4 features and there are 3 possible actions (0, 1, 2)\n",
    "states = torch.tensor([[0.1, 0.2, 0.3, 0.4],\n",
    "                       [0.5, 0.6, 0.7, 0.8]], dtype=torch.float32) # Current state\n",
    "actions = torch.tensor([[1], [2]]) # Actions taken in the current state\n",
    "rewards = torch.tensor([[1.0], [0.5]]) # Rewards received after taking the actions\n",
    "next_states = torch.tensor([[0.9, 0.1, 0.2, 0.3],\n",
    "                            [0.4, 0.5, 0.6, 0.7]], dtype=torch.float32) # Next state after taking the actions\n",
    "dones = torch.tensor([[0], [1]], dtype=torch.float32) # 1 if the episode ended after this step, else 0  \n",
    "gamma = 0.9 # Discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfea1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2728,  0.1779,  0.3559],\n",
       "        [-0.1902,  0.2848,  0.6769]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = policy_net(states) # returns the Q-values for each action in the current states\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028a5e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1779],\n",
       "        [0.6769]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value = q_values.gather(1, actions) # Gather the Q-values corresponding to the actions taken\n",
    "q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18628d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6384,  0.2381,  0.3531],\n",
       "         [-0.4915,  0.2273,  0.4723]]),\n",
       " tensor([[0.3531],\n",
       "         [0.4723]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    next_q_values = target_net(next_states) # Q-values for the next states from the target network\n",
    "    next_q_value = next_q_values.max(1, keepdims=True)[0] # Max Q-value for the next states\n",
    "next_q_values, next_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function = (1 / N) * sum((Q(s, a) - (r + gamma * max_a' Q'(s', a') * (1 - done)))^2)\n",
    "# Q(s, a) is the predicted Q-value for the current state and action\n",
    "# Q'(s', a') is the target Q-value for the next state and action\n",
    "# N is the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329ca75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.3178],\n",
       "         [0.5000]]),\n",
       " tensor(0.6653, grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "target = rewards + gamma * next_q_value * (1 - dones) # Compute the target Q-value\n",
    "loss = F.mse_loss(q_value, target) # Compute the loss, mean sqaured error between current Q-value and target Q-value\n",
    "loss.backward() # Compute gradients\n",
    "optimizer.step() # Update the policy network parameters\n",
    "target, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a25e24",
   "metadata": {},
   "source": [
    "## TD(0)(one step temporal differenece) prediction on the FrozenLake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001cda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ef383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space dimension: 16, Action space dimension: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "print(f\"State space dimension: {n_states}, Action space dimension: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6489ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "num_episodes = 10000  # Number of episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6165b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value function\n",
    "V = np.zeros(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eccf9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a06fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0(num_episodes, policy=random_policy):\n",
    "    for _ in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # TD(0) update V[state] = (1 - alpha) * V[state] + alpha * (reward + gamma * V[next_state])\n",
    "            V[state] = (1- alpha) * V[state] + alpha * (reward + gamma * V[next_state])\n",
    "            state = next_state\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330573e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = TD_0(num_episodes, random_policy).reshape((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b4f681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01025745, 0.00438898, 0.00738122, 0.00517198],\n",
       "       [0.01507457, 0.        , 0.01988743, 0.        ],\n",
       "       [0.02726076, 0.04603213, 0.11986327, 0.        ],\n",
       "       [0.        , 0.08434613, 0.30618266, 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5408e5c",
   "metadata": {},
   "source": [
    "## Double Q learning on FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ffa044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251b019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, epsilon_decay, min_epsilon):\n",
    "        self.env = env\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.Q1 = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.Q2 = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"This function selects an action using the epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample() # Explore\n",
    "        else:\n",
    "            Q_sum = self.Q1[state] + self.Q2[state]\n",
    "            return np.argmax(Q_sum)  # Exploit\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"This function performs the Double Q-Learning update.\"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            best_next_action = np.argmax(self.Q1[next_state])\n",
    "            td_target = reward + (0 if done else self.gamma * self.Q2[next_state][best_next_action])\n",
    "            td_error = td_target - self.Q1[state][action]\n",
    "            self.Q1[state][action] += self.alpha * td_error\n",
    "        else:\n",
    "            best_next_action = np.argmax(self.Q2[next_state])\n",
    "            td_target = reward + (0 if done else self.gamma * self.Q1[next_state][best_next_action])\n",
    "            td_error = td_target - self.Q2[state][action]\n",
    "            self.Q2[state][action] += self.alpha * td_error\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decays the exploration rate epsilon.\"\"\"\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad0ca608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   \n",
    "epsilon_decay = 0.9995\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 10000\n",
    "video_dir = \"videos_frozenlake_double_q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36bbb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DoubleQLearningAgent(env, alpha, gamma, epsilon, epsilon_decay, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c4c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Avg Reward: 0.118, Epsilon: 0.606\n",
      "Episode: 2000, Avg Reward: 0.436, Epsilon: 0.368\n",
      "Episode: 3000, Avg Reward: 0.686, Epsilon: 0.223\n",
      "Episode: 4000, Avg Reward: 0.828, Epsilon: 0.135\n",
      "Episode: 5000, Avg Reward: 0.895, Epsilon: 0.082\n",
      "Episode: 6000, Avg Reward: 0.922, Epsilon: 0.050\n",
      "Episode: 7000, Avg Reward: 0.956, Epsilon: 0.030\n",
      "Episode: 8000, Avg Reward: 0.968, Epsilon: 0.018\n",
      "Episode: 9000, Avg Reward: 0.978, Epsilon: 0.011\n",
      "Episode: 10000, Avg Reward: 0.991, Epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Running the Double Q-Learning agent in the FrozenLake environment.\"\"\"\n",
    "rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    agent.decay_epsilon()\n",
    "    rewards.append(total_reward)\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        avg_reward = np.mean(rewards[-1000:])\n",
    "        print(f\"Episode: {episode+1}, Avg Reward: {avg_reward:.3f}, Epsilon: {agent.epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_record(Q_table, env_name, video_dir):\n",
    "    \"\"\"This function tests the learned Q-table and records videos of the agent's performance.\"\"\"\n",
    "\n",
    "    # Create environment with video recording\n",
    "    env = gym.make(env_name,is_slippery=False, render_mode=\"rgb_array\")  # For recording\n",
    "    env = gym.wrappers.RecordVideo(env, video_dir, episode_trigger=lambda e: True)\n",
    "\n",
    "    #n_actions = env.action_space.n\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(5):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Greedy policy (no exploration)\n",
    "            action = np.argmax(Q_table[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Videos saved in: {video_dir}\")\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39f823b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-0.mp4\n",
      "Episode 1: Total Reward = 1.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-1.mp4\n",
      "Episode 2: Total Reward = 1.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3: Total Reward = 1.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-3.mp4\n",
      "Episode 4: Total Reward = 1.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_double_q\\rl-video-episode-4.mp4\n",
      "Episode 5: Total Reward = 1.0\n",
      "Videos saved in: videos_frozenlake_double_q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_record(agent.Q1 + agent.Q2, \"FrozenLake-v1\", video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caadcb3a",
   "metadata": {},
   "source": [
    "## Sarsa on Cliffwalking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a463283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5177fc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "env.observation_space.n, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98e291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, epsilon_decay, min_epsilon):\n",
    "        self.env = env\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay      \n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"This function selects an action using the epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample() # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])  # Exploit\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"This function performs the SARSA update.\"\"\"\n",
    "        td_target = reward + (0 if done else self.gamma * self.Q[next_state][next_action])\n",
    "        self.Q[state][action] += self.alpha * (td_target - self.Q[state][action])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decays the exploration rate epsilon.\"\"\"\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon - self.epsilon_decay)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "758372cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 2500\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = (epsilon - epsilon_min) / 1600\n",
    "video_dir = \"videos_cliffwalking_sarsa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b243ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SarsaAgent(env, alpha, gamma, epsilon, epsilon_decay, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1734c001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Avg Reward: -48811.040, Epsilon: 0.972\n",
      "Episode: 100, Avg Reward: -34092.120, Epsilon: 0.944\n",
      "Episode: 150, Avg Reward: -15767.280, Epsilon: 0.916\n",
      "Episode: 200, Avg Reward: -9985.550, Epsilon: 0.887\n",
      "Episode: 250, Avg Reward: -5761.890, Epsilon: 0.859\n",
      "Episode: 300, Avg Reward: -2869.040, Epsilon: 0.831\n",
      "Episode: 350, Avg Reward: -2016.030, Epsilon: 0.803\n",
      "Episode: 400, Avg Reward: -1541.570, Epsilon: 0.775\n",
      "Episode: 450, Avg Reward: -1028.120, Epsilon: 0.747\n",
      "Episode: 500, Avg Reward: -801.970, Epsilon: 0.719\n",
      "Episode: 550, Avg Reward: -536.840, Epsilon: 0.691\n",
      "Episode: 600, Avg Reward: -398.750, Epsilon: 0.662\n",
      "Episode: 650, Avg Reward: -287.820, Epsilon: 0.634\n",
      "Episode: 700, Avg Reward: -284.010, Epsilon: 0.606\n",
      "Episode: 750, Avg Reward: -266.080, Epsilon: 0.578\n",
      "Episode: 800, Avg Reward: -181.770, Epsilon: 0.550\n",
      "Episode: 850, Avg Reward: -153.310, Epsilon: 0.522\n",
      "Episode: 900, Avg Reward: -116.210, Epsilon: 0.494\n",
      "Episode: 950, Avg Reward: -132.350, Epsilon: 0.466\n",
      "Episode: 1000, Avg Reward: -146.280, Epsilon: 0.437\n",
      "Episode: 1050, Avg Reward: -113.260, Epsilon: 0.409\n",
      "Episode: 1100, Avg Reward: -89.710, Epsilon: 0.381\n",
      "Episode: 1150, Avg Reward: -76.050, Epsilon: 0.353\n",
      "Episode: 1200, Avg Reward: -85.010, Epsilon: 0.325\n",
      "Episode: 1250, Avg Reward: -85.820, Epsilon: 0.297\n",
      "Episode: 1300, Avg Reward: -64.580, Epsilon: 0.269\n",
      "Episode: 1350, Avg Reward: -64.610, Epsilon: 0.241\n",
      "Episode: 1400, Avg Reward: -76.260, Epsilon: 0.212\n",
      "Episode: 1450, Avg Reward: -68.030, Epsilon: 0.184\n",
      "Episode: 1500, Avg Reward: -53.030, Epsilon: 0.156\n",
      "Episode: 1550, Avg Reward: -41.710, Epsilon: 0.128\n",
      "Episode: 1600, Avg Reward: -46.730, Epsilon: 0.100\n",
      "Episode: 1650, Avg Reward: -43.380, Epsilon: 0.100\n",
      "Episode: 1700, Avg Reward: -33.820, Epsilon: 0.100\n",
      "Episode: 1750, Avg Reward: -31.250, Epsilon: 0.100\n",
      "Episode: 1800, Avg Reward: -36.290, Epsilon: 0.100\n",
      "Episode: 1850, Avg Reward: -39.460, Epsilon: 0.100\n",
      "Episode: 1900, Avg Reward: -31.130, Epsilon: 0.100\n",
      "Episode: 1950, Avg Reward: -39.910, Epsilon: 0.100\n",
      "Episode: 2000, Avg Reward: -40.360, Epsilon: 0.100\n",
      "Episode: 2050, Avg Reward: -29.050, Epsilon: 0.100\n",
      "Episode: 2100, Avg Reward: -25.120, Epsilon: 0.100\n",
      "Episode: 2150, Avg Reward: -38.220, Epsilon: 0.100\n",
      "Episode: 2200, Avg Reward: -39.100, Epsilon: 0.100\n",
      "Episode: 2250, Avg Reward: -24.100, Epsilon: 0.100\n",
      "Episode: 2300, Avg Reward: -31.600, Epsilon: 0.100\n",
      "Episode: 2350, Avg Reward: -66.830, Epsilon: 0.100\n",
      "Episode: 2400, Avg Reward: -67.550, Epsilon: 0.100\n",
      "Episode: 2450, Avg Reward: -35.380, Epsilon: 0.100\n",
      "Episode: 2500, Avg Reward: -26.420, Epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training the SARSA agent in the CliffWalking environment.\"\"\"\n",
    "rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    action = agent.choose_action(state)\n",
    "    while not done:\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_action = agent.choose_action(next_state)\n",
    "        agent.update(state, action, reward, next_state, next_action, done)\n",
    "        state = next_state  \n",
    "        action = next_action\n",
    "        total_reward += reward\n",
    "    agent.decay_epsilon()\n",
    "    rewards.append(total_reward)\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        avg_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Episode: {episode+1}, Avg Reward: {avg_reward:.3f}, Epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca082a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_record(Q_table, env_name, video_dir):\n",
    "    \"\"\"This function tests the learned Q-table(of Sarsa) and records videos of the agent's performance for 5 episodes.\"\"\"\n",
    "\n",
    "    # Create environment with video recording\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")  # For recording\n",
    "    env = gym.wrappers.RecordVideo(env, video_dir, episode_trigger=lambda e: True)\n",
    "\n",
    "    #n_actions = env.action_space.n\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(5):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Greedy policy (no exploration)\n",
    "            action = np.argmax(Q_table[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Videos saved in: {video_dir}\")\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bd8143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-0.mp4\n",
      "Episode 1: Total Reward = -17\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-1.mp4\n",
      "Episode 2: Total Reward = -17\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3: Total Reward = -17\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-3.mp4\n",
      "Episode 4: Total Reward = -17\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cliffwalking_sarsa\\rl-video-episode-4.mp4\n",
      "Episode 5: Total Reward = -17\n",
      "Videos saved in: videos_cliffwalking_sarsa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-17, -17, -17, -17, -17]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_record(agent.Q, \"CliffWalking-v0\", video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049f2bb",
   "metadata": {},
   "source": [
    "## Reinforce on CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587fbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fb079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f900a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "state_size, n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ed4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb1eec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (3): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = PolicyNet(state_size, 128, n_actions).to(device)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27276ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33b48a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"The function computes the returns and returns as a torch tensor\"\"\"\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return torch.tensor(returns, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e711a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "video_dir = \"videos_cartpole_reinforce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: 500.0\n",
      "Episode 40: 500.0\n",
      "Episode 60: 500.0\n",
      "Episode 80: 500.0\n",
      "Episode 100: 500.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    log_probs, rewards = [], []\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        probs = policy(state_tensor) # Computing the probabilities of disparate actions\n",
    "        dist = torch.distributions.Categorical(probs) # Create the Categorical distribution object\n",
    "        action = dist.sample() # Sample from the output distribution using the probabilities\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        rewards.append(reward)\n",
    "        total_reward += reward\n",
    "    \n",
    "    returns = compute_returns(rewards, gamma)\n",
    "\n",
    "    loss = 0\n",
    "    for t, (log_prob, Gt) in enumerate(zip(log_probs, returns)):\n",
    "        loss += -log_prob * ((gamma ** t) * Gt)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (episode + 1) % 20 == 0:\n",
    "        print(f\"Episode {episode + 1}: {total_reward}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ef1b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'Reinforce_Cartpole_net/reinforce_agent.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_record(env_name, video_dir):   \n",
    "    \"\"\"This function tests and records 5 episodes using the trained agent\"\"\" \n",
    "    # Create environment with video recording\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")  \n",
    "    env = gym.wrappers.RecordVideo(env, video_dir, episode_trigger=lambda e: True)\n",
    "    # Load the model's state_dict\n",
    "    loaded_state_dict = torch.load(\"Reinforce_Cartpole_net/reinforce_agent.pth\")\n",
    "    policy = PolicyNet(state_size, 128, n_actions).to(device) \n",
    "    policy.load_state_dict(loaded_state_dict)\n",
    "    policy.eval()\n",
    "    for episode in range(5):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            probs = policy(state_tensor)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {episode + 1} reward: {total_reward}\")\n",
    "    env.close()\n",
    "    print(f\"Videos saved in: {video_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30c3ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-0.mp4\n",
      "Episode 1 reward: 500.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-1.mp4\n",
      "Episode 2 reward: 82.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-2.mp4\n",
      "Episode 3 reward: 500.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-3.mp4\n",
      "Episode 4 reward: 500.0\n",
      "Moviepy - Building video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_cartpole_reinforce\\rl-video-episode-4.mp4\n",
      "Episode 5 reward: 500.0\n",
      "Videos saved in: videos_cartpole_reinforce\n"
     ]
    }
   ],
   "source": [
    "test_and_record(\"CartPole-v1\", video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a8021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
