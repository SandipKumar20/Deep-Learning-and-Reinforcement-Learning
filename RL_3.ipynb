{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de45611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24: episode 1 done, reward=23.00, epsilon=1.00\n",
      "39: episode 2 done, reward=15.00, epsilon=0.93\n",
      "64: episode 3 done, reward=25.00, epsilon=0.72\n",
      "74: episode 4 done, reward=10.00, epsilon=0.66\n",
      "93: episode 5 done, reward=19.00, epsilon=0.54\n",
      "112: episode 6 done, reward=19.00, epsilon=0.45\n",
      "134: episode 7 done, reward=22.00, epsilon=0.36\n",
      "144: episode 8 done, reward=10.00, epsilon=0.32\n",
      "157: episode 9 done, reward=13.00, epsilon=0.28\n",
      "171: episode 10 done, reward=14.00, epsilon=0.25\n",
      "181: episode 11 done, reward=10.00, epsilon=0.22\n",
      "194: episode 12 done, reward=13.00, epsilon=0.20\n",
      "203: episode 13 done, reward=9.00, epsilon=0.18\n",
      "213: episode 14 done, reward=10.00, epsilon=0.16\n",
      "224: episode 15 done, reward=11.00, epsilon=0.15\n",
      "234: episode 16 done, reward=10.00, epsilon=0.13\n",
      "243: episode 17 done, reward=9.00, epsilon=0.12\n",
      "251: episode 18 done, reward=8.00, epsilon=0.11\n",
      "260: episode 19 done, reward=9.00, epsilon=0.10\n",
      "270: episode 20 done, reward=10.00, epsilon=0.09\n",
      "281: episode 21 done, reward=11.00, epsilon=0.08\n",
      "291: episode 22 done, reward=10.00, epsilon=0.07\n",
      "302: episode 23 done, reward=11.00, epsilon=0.07\n",
      "312: episode 24 done, reward=10.00, epsilon=0.06\n",
      "322: episode 25 done, reward=10.00, epsilon=0.05\n",
      "333: episode 26 done, reward=11.00, epsilon=0.05\n",
      "344: episode 27 done, reward=11.00, epsilon=0.04\n",
      "353: episode 28 done, reward=9.00, epsilon=0.04\n",
      "362: episode 29 done, reward=9.00, epsilon=0.04\n",
      "371: episode 30 done, reward=9.00, epsilon=0.03\n",
      "380: episode 31 done, reward=9.00, epsilon=0.03\n",
      "393: episode 32 done, reward=13.00, epsilon=0.03\n",
      "403: episode 33 done, reward=10.00, epsilon=0.02\n",
      "414: episode 34 done, reward=11.00, epsilon=0.02\n",
      "426: episode 35 done, reward=12.00, epsilon=0.02\n",
      "435: episode 36 done, reward=9.00, epsilon=0.02\n",
      "445: episode 37 done, reward=10.00, epsilon=0.02\n",
      "455: episode 38 done, reward=10.00, epsilon=0.01\n",
      "464: episode 39 done, reward=9.00, epsilon=0.01\n",
      "474: episode 40 done, reward=10.00, epsilon=0.01\n",
      "485: episode 41 done, reward=11.00, epsilon=0.01\n",
      "494: episode 42 done, reward=9.00, epsilon=0.01\n",
      "504: episode 43 done, reward=10.00, epsilon=0.01\n",
      "514: episode 44 done, reward=10.00, epsilon=0.01\n",
      "524: episode 45 done, reward=10.00, epsilon=0.01\n",
      "534: episode 46 done, reward=10.00, epsilon=0.01\n",
      "543: episode 47 done, reward=9.00, epsilon=0.01\n",
      "552: episode 48 done, reward=9.00, epsilon=0.01\n",
      "562: episode 49 done, reward=10.00, epsilon=0.00\n",
      "575: episode 50 done, reward=13.00, epsilon=0.00\n",
      "586: episode 51 done, reward=11.00, epsilon=0.00\n",
      "597: episode 52 done, reward=11.00, epsilon=0.00\n",
      "609: episode 53 done, reward=12.00, epsilon=0.00\n",
      "617: episode 54 done, reward=8.00, epsilon=0.00\n",
      "628: episode 55 done, reward=11.00, epsilon=0.00\n",
      "642: episode 56 done, reward=14.00, epsilon=0.00\n",
      "653: episode 57 done, reward=11.00, epsilon=0.00\n",
      "664: episode 58 done, reward=11.00, epsilon=0.00\n",
      "675: episode 59 done, reward=11.00, epsilon=0.00\n",
      "685: episode 60 done, reward=10.00, epsilon=0.00\n",
      "700: episode 61 done, reward=15.00, epsilon=0.00\n",
      "712: episode 62 done, reward=12.00, epsilon=0.00\n",
      "722: episode 63 done, reward=10.00, epsilon=0.00\n",
      "732: episode 64 done, reward=10.00, epsilon=0.00\n",
      "743: episode 65 done, reward=11.00, epsilon=0.00\n",
      "753: episode 66 done, reward=10.00, epsilon=0.00\n",
      "764: episode 67 done, reward=11.00, epsilon=0.00\n",
      "775: episode 68 done, reward=11.00, epsilon=0.00\n",
      "786: episode 69 done, reward=11.00, epsilon=0.00\n",
      "799: episode 70 done, reward=13.00, epsilon=0.00\n",
      "811: episode 71 done, reward=12.00, epsilon=0.00\n",
      "821: episode 72 done, reward=10.00, epsilon=0.00\n",
      "833: episode 73 done, reward=12.00, epsilon=0.00\n",
      "846: episode 74 done, reward=13.00, epsilon=0.00\n",
      "857: episode 75 done, reward=11.00, epsilon=0.00\n",
      "868: episode 76 done, reward=11.00, epsilon=0.00\n",
      "880: episode 77 done, reward=12.00, epsilon=0.00\n",
      "893: episode 78 done, reward=13.00, epsilon=0.00\n",
      "906: episode 79 done, reward=13.00, epsilon=0.00\n",
      "923: episode 80 done, reward=17.00, epsilon=0.00\n",
      "936: episode 81 done, reward=13.00, epsilon=0.00\n",
      "947: episode 82 done, reward=11.00, epsilon=0.00\n",
      "957: episode 83 done, reward=10.00, epsilon=0.00\n",
      "971: episode 84 done, reward=14.00, epsilon=0.00\n",
      "984: episode 85 done, reward=13.00, epsilon=0.00\n",
      "997: episode 86 done, reward=13.00, epsilon=0.00\n",
      "1019: episode 87 done, reward=22.00, epsilon=0.00\n",
      "1032: episode 88 done, reward=13.00, epsilon=0.00\n",
      "1045: episode 89 done, reward=13.00, epsilon=0.00\n",
      "1058: episode 90 done, reward=13.00, epsilon=0.00\n",
      "1072: episode 91 done, reward=14.00, epsilon=0.00\n",
      "1088: episode 92 done, reward=16.00, epsilon=0.00\n",
      "1103: episode 93 done, reward=15.00, epsilon=0.00\n",
      "1119: episode 94 done, reward=16.00, epsilon=0.00\n",
      "1135: episode 95 done, reward=16.00, epsilon=0.00\n",
      "1154: episode 96 done, reward=19.00, epsilon=0.00\n",
      "1170: episode 97 done, reward=16.00, epsilon=0.00\n",
      "1188: episode 98 done, reward=18.00, epsilon=0.00\n",
      "1205: episode 99 done, reward=17.00, epsilon=0.00\n",
      "1217: episode 100 done, reward=12.00, epsilon=0.00\n",
      "1229: episode 101 done, reward=12.00, epsilon=0.00\n",
      "1243: episode 102 done, reward=14.00, epsilon=0.00\n",
      "1287: episode 103 done, reward=44.00, epsilon=0.00\n",
      "1314: episode 104 done, reward=27.00, epsilon=0.00\n",
      "1333: episode 105 done, reward=19.00, epsilon=0.00\n",
      "1357: episode 106 done, reward=24.00, epsilon=0.00\n",
      "1375: episode 107 done, reward=18.00, epsilon=0.00\n",
      "1400: episode 108 done, reward=25.00, epsilon=0.00\n",
      "1432: episode 109 done, reward=32.00, epsilon=0.00\n",
      "1453: episode 110 done, reward=21.00, epsilon=0.00\n",
      "1470: episode 111 done, reward=17.00, epsilon=0.00\n",
      "1495: episode 112 done, reward=25.00, epsilon=0.00\n",
      "1517: episode 113 done, reward=22.00, epsilon=0.00\n",
      "1562: episode 114 done, reward=45.00, epsilon=0.00\n",
      "1598: episode 115 done, reward=36.00, epsilon=0.00\n",
      "1616: episode 116 done, reward=18.00, epsilon=0.00\n",
      "1659: episode 117 done, reward=43.00, epsilon=0.00\n",
      "1716: episode 118 done, reward=57.00, epsilon=0.00\n",
      "1733: episode 119 done, reward=17.00, epsilon=0.00\n",
      "1780: episode 120 done, reward=47.00, epsilon=0.00\n",
      "1818: episode 121 done, reward=38.00, epsilon=0.00\n",
      "1873: episode 122 done, reward=55.00, epsilon=0.00\n",
      "1911: episode 123 done, reward=38.00, epsilon=0.00\n",
      "1989: episode 124 done, reward=78.00, epsilon=0.00\n",
      "2040: episode 125 done, reward=51.00, epsilon=0.00\n",
      "2091: episode 126 done, reward=51.00, epsilon=0.00\n",
      "2140: episode 127 done, reward=49.00, epsilon=0.00\n",
      "2162: episode 128 done, reward=22.00, epsilon=0.00\n",
      "2192: episode 129 done, reward=30.00, epsilon=0.00\n",
      "2222: episode 130 done, reward=30.00, epsilon=0.00\n",
      "2254: episode 131 done, reward=32.00, epsilon=0.00\n",
      "2291: episode 132 done, reward=37.00, epsilon=0.00\n",
      "2335: episode 133 done, reward=44.00, epsilon=0.00\n",
      "2376: episode 134 done, reward=41.00, epsilon=0.00\n",
      "2430: episode 135 done, reward=54.00, epsilon=0.00\n",
      "2500: episode 136 done, reward=70.00, epsilon=0.00\n",
      "2536: episode 137 done, reward=36.00, epsilon=0.00\n",
      "2561: episode 138 done, reward=25.00, epsilon=0.00\n",
      "2609: episode 139 done, reward=48.00, epsilon=0.00\n",
      "2670: episode 140 done, reward=61.00, epsilon=0.00\n",
      "2712: episode 141 done, reward=42.00, epsilon=0.00\n",
      "2740: episode 142 done, reward=28.00, epsilon=0.00\n",
      "2771: episode 143 done, reward=31.00, epsilon=0.00\n",
      "2813: episode 144 done, reward=42.00, epsilon=0.00\n",
      "2867: episode 145 done, reward=54.00, epsilon=0.00\n",
      "2903: episode 146 done, reward=36.00, epsilon=0.00\n",
      "2955: episode 147 done, reward=52.00, epsilon=0.00\n",
      "2985: episode 148 done, reward=30.00, epsilon=0.00\n",
      "3023: episode 149 done, reward=38.00, epsilon=0.00\n",
      "3057: episode 150 done, reward=34.00, epsilon=0.00\n",
      "3088: episode 151 done, reward=31.00, epsilon=0.00\n",
      "3113: episode 152 done, reward=25.00, epsilon=0.00\n",
      "3177: episode 153 done, reward=64.00, epsilon=0.00\n",
      "3207: episode 154 done, reward=30.00, epsilon=0.00\n",
      "3255: episode 155 done, reward=48.00, epsilon=0.00\n",
      "3322: episode 156 done, reward=67.00, epsilon=0.00\n",
      "3441: episode 157 done, reward=119.00, epsilon=0.00\n",
      "3499: episode 158 done, reward=58.00, epsilon=0.00\n",
      "3583: episode 159 done, reward=84.00, epsilon=0.00\n",
      "3629: episode 160 done, reward=46.00, epsilon=0.00\n",
      "3673: episode 161 done, reward=44.00, epsilon=0.00\n",
      "3761: episode 162 done, reward=88.00, epsilon=0.00\n",
      "3812: episode 163 done, reward=51.00, epsilon=0.00\n",
      "3876: episode 164 done, reward=64.00, epsilon=0.00\n",
      "3968: episode 165 done, reward=92.00, epsilon=0.00\n",
      "4019: episode 166 done, reward=51.00, epsilon=0.00\n",
      "4085: episode 167 done, reward=66.00, epsilon=0.00\n",
      "4177: episode 168 done, reward=92.00, epsilon=0.00\n",
      "4283: episode 169 done, reward=106.00, epsilon=0.00\n",
      "4426: episode 170 done, reward=143.00, epsilon=0.00\n",
      "4589: episode 171 done, reward=163.00, epsilon=0.00\n",
      "Whee!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from ptan.experience import ExperienceFirstLast, ExperienceSourceFirstLast\n",
    "import ptan\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import typing as tt\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "TGT_NET_SYNC = 10\n",
    "GAMMA = 0.9\n",
    "REPLAY_SIZE = 1000\n",
    "LR = 1e-3\n",
    "EPS_DECAY = 0.99\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def unpack_batch(batch: tt.List[ExperienceFirstLast], net: Net, gamma: float):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.as_tensor(np.stack(states))\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards)\n",
    "    last_states_v = torch.as_tensor(np.stack(last_states))\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v * gamma + rewards_v\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.ArgmaxActionSelector()\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1, selector=selector)\n",
    "    agent = ptan.agent.DQNAgent(net, selector)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n",
    "    optimizer = optim.Adam(net.parameters(), LR)\n",
    "\n",
    "    step = 0\n",
    "    episode = 0\n",
    "    solved = False\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "        buffer.populate(1)\n",
    "\n",
    "        for reward, steps in exp_source.pop_rewards_steps():\n",
    "            episode += 1\n",
    "            print(f\"{step}: episode {episode} done, reward={reward:.2f}, \"\n",
    "                  f\"epsilon={selector.epsilon:.2f}\")\n",
    "            solved = reward > 150\n",
    "        if solved:\n",
    "            print(\"Whee!\")\n",
    "            break\n",
    "        if len(buffer) < 2*BATCH_SIZE:\n",
    "            continue\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        states_v, actions_v, tgt_q_v = unpack_batch(batch, tgt_net.target_model, GAMMA)\n",
    "        optimizer.zero_grad()\n",
    "        q_v = net(states_v)\n",
    "        q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        loss_v = F.mse_loss(q_v, tgt_q_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        selector.epsilon *= EPS_DECAY\n",
    "\n",
    "        if step % TGT_NET_SYNC == 0:\n",
    "            tgt_net.sync()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
