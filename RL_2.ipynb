{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615ffa20",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1fc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 100\n",
    "\n",
    "State = int\n",
    "Action = int\n",
    "RewardKey = tt.Tuple[State, Action, State]\n",
    "TransitKey = tt.Tuple[State, Action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db2615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.rewards: tt.Dict[RewardKey, float] = defaultdict(float)\n",
    "        self.transits: tt.Dict[TransitKey, Counter] = defaultdict(Counter)\n",
    "        self.values: tt.Dict[State, float] = defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, n: int):\n",
    "        for _ in range(n):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, is_trunc, _ = self.env.step(action)\n",
    "            rw_key = (self.state, action, new_state)\n",
    "            self.rewards[rw_key] = float(reward)\n",
    "            tr_key = (self.state, action)\n",
    "            self.transits[tr_key][new_state] += 1\n",
    "            if is_done or is_trunc:\n",
    "                self.state, _ = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "    def calc_action_value(self, state: State, action: Action) -> float:\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            rw_key = (state, action, tgt_state)\n",
    "            reward = self.rewards[rw_key]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state: State) -> Action:\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, is_trunc, _ = env.step(action)\n",
    "            rw_key = (state, action, new_state)\n",
    "            self.rewards[rw_key] = float(reward)\n",
    "            tr_key = (state, action)\n",
    "            self.transits[tr_key][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done or is_trunc:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            self.values[state] = max(state_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a99f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: Best reward updated: 0.000 -> 0.010\n",
      "5: Best reward updated: 0.010 -> 0.080\n",
      "6: Best reward updated: 0.080 -> 0.120\n",
      "10: Best reward updated: 0.120 -> 0.300\n",
      "12: Best reward updated: 0.300 -> 0.430\n",
      "13: Best reward updated: 0.430 -> 0.440\n",
      "17: Best reward updated: 0.440 -> 0.490\n",
      "18: Best reward updated: 0.490 -> 0.640\n",
      "19: Best reward updated: 0.640 -> 0.740\n",
      "20: Best reward updated: 0.740 -> 0.770\n",
      "21: Best reward updated: 0.770 -> 0.800\n",
      "67: Best reward updated: 0.800 -> 0.810\n",
      "Solved in 67 iterations!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(f\"{iter_no}: Best reward updated: {best_reward:.3f} -> {reward:.3f}\")\n",
    "            best_reward = reward\n",
    "        if reward > 0.8:\n",
    "            print(f\"Solved in {iter_no} iterations!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efea1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing & Recording trained agent ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1: reward=1.0\n",
      "Test Episode 2: reward=1.0\n",
      "Test Episode 3: reward=0.0\n",
      "Test Episode 4: reward=1.0\n",
      "Test Episode 5: reward=1.0\n",
      "All 5 test episodes recorded in ./videos folder\n"
     ]
    }
   ],
   "source": [
    "# -------- Testing + Recording Loop --------\n",
    "num_episodes = 5\n",
    "print(\"\\n=== Testing & Recording trained agent ===\")\n",
    "video_env = RecordVideo(\n",
    "        gym.make(ENV_NAME, render_mode=\"rgb_array\"),\n",
    "        video_folder=\"./videos\",\n",
    "        episode_trigger=lambda ep_id: True  # record every test episode\n",
    "    )\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = video_env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        new_state, reward, is_done, is_trunc, _ = video_env.step(action)\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "        if is_done or is_trunc:\n",
    "            break\n",
    "    print(f\"Test Episode {ep+1}: reward={total_reward}\")\n",
    "\n",
    "video_env.close()\n",
    "print(f\"All {num_episodes} test episodes recorded in ./videos folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22df21ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " defaultdict(collections.Counter,\n",
       "             {(0, np.int64(2)): Counter({4: 1935, 0: 1925, 1: 1917}),\n",
       "              (0, np.int64(3)): Counter({0: 492, 1: 248}),\n",
       "              (1, np.int64(1)): Counter({2: 112, 0: 99, 5: 71}),\n",
       "              (1, np.int64(3)): Counter({1: 1471, 0: 1402, 2: 1322}),\n",
       "              (0, np.int64(1)): Counter({4: 368, 0: 347, 1: 346}),\n",
       "              (1, np.int64(0)): Counter({5: 99, 1: 98, 0: 98}),\n",
       "              (1, np.int64(2)): Counter({2: 100, 1: 97, 5: 91}),\n",
       "              (0, np.int64(0)): Counter({0: 46137, 4: 23157}),\n",
       "              (2, np.int64(2)): Counter({2: 1500, 3: 1499, 6: 1428}),\n",
       "              (2, np.int64(1)): Counter({1: 47, 3: 46, 6: 45}),\n",
       "              (3, np.int64(3)): Counter({3: 3028, 2: 1474}),\n",
       "              (3, np.int64(0)): Counter({3: 30, 7: 22, 2: 19}),\n",
       "              (2, np.int64(3)): Counter({3: 51, 2: 47, 1: 43}),\n",
       "              (2, np.int64(0)): Counter({2: 843, 6: 836, 1: 795}),\n",
       "              (6, np.int64(2)): Counter({2: 1717, 7: 1669, 10: 1652}),\n",
       "              (3, np.int64(2)): Counter({3: 52, 7: 21}),\n",
       "              (4, np.int64(2)): Counter({8: 143, 0: 129, 5: 121}),\n",
       "              (8, np.int64(3)): Counter({8: 12780, 4: 12639, 9: 12480}),\n",
       "              (8, np.int64(1)): Counter({9: 115, 8: 99, 12: 95}),\n",
       "              (9, np.int64(2)): Counter({5: 196, 13: 188, 10: 164}),\n",
       "              (10, np.int64(3)): Counter({6: 9, 11: 7, 9: 4}),\n",
       "              (6, np.int64(1)): Counter({5: 21, 7: 17, 10: 14}),\n",
       "              (10, np.int64(0)): Counter({9: 2856, 6: 2855, 14: 2759}),\n",
       "              (14, np.int64(0)): Counter({14: 8, 13: 7, 10: 6}),\n",
       "              (14, np.int64(1)): Counter({15: 3979, 13: 3950, 14: 3827}),\n",
       "              (4, np.int64(1)): Counter({5: 119, 4: 99, 8: 91}),\n",
       "              (4, np.int64(3)): Counter({5: 99, 4: 94, 0: 86}),\n",
       "              (8, np.int64(0)): Counter({4: 302, 8: 240, 12: 237}),\n",
       "              (8, np.int64(2)): Counter({9: 37, 4: 36, 12: 28}),\n",
       "              (9, np.int64(3)): Counter({10: 12, 5: 11, 8: 11}),\n",
       "              (4, np.int64(0)): Counter({4: 19026, 8: 18897, 0: 18694}),\n",
       "              (3, np.int64(1)): Counter({7: 28, 2: 27, 3: 14}),\n",
       "              (5, 0): Counter(),\n",
       "              (5, 1): Counter(),\n",
       "              (5, 2): Counter(),\n",
       "              (5, 3): Counter(),\n",
       "              (6, 0): Counter({5: 19, 10: 16, 2: 15}),\n",
       "              (6, 3): Counter({7: 14, 2: 13, 5: 11}),\n",
       "              (7, 0): Counter(),\n",
       "              (7, 1): Counter(),\n",
       "              (7, 2): Counter(),\n",
       "              (7, 3): Counter(),\n",
       "              (9, 0): Counter({8: 17, 5: 9, 13: 9}),\n",
       "              (9, 1): Counter({8: 6880, 13: 6800, 10: 6690}),\n",
       "              (10, 1): Counter({9: 21, 14: 18, 11: 12}),\n",
       "              (10, 2): Counter({6: 10, 11: 8, 14: 6}),\n",
       "              (11, 0): Counter(),\n",
       "              (11, 1): Counter(),\n",
       "              (11, 2): Counter(),\n",
       "              (11, 3): Counter(),\n",
       "              (12, 0): Counter(),\n",
       "              (12, 1): Counter(),\n",
       "              (12, 2): Counter(),\n",
       "              (12, 3): Counter(),\n",
       "              (13, 0): Counter({12: 121, 9: 117, 13: 115}),\n",
       "              (13, 1): Counter({13: 9, 12: 4, 14: 4}),\n",
       "              (13, 2): Counter({13: 5379, 9: 5326, 14: 5194}),\n",
       "              (13, 3): Counter({9: 64, 12: 49, 14: 47}),\n",
       "              (14, 2): Counter({14: 26, 10: 18, 15: 18}),\n",
       "              (14, 3): Counter({13: 6, 10: 6, 15: 4}),\n",
       "              (15, 0): Counter(),\n",
       "              (15, 1): Counter(),\n",
       "              (15, 2): Counter(),\n",
       "              (15, 3): Counter()}),\n",
       " defaultdict(float,\n",
       "             {(0, np.int64(2), 0): 0.0,\n",
       "              (0, np.int64(3), 1): 0.0,\n",
       "              (1, np.int64(1), 5): 0.0,\n",
       "              (0, np.int64(2), 1): 0.0,\n",
       "              (1, np.int64(3), 0): 0.0,\n",
       "              (0, np.int64(1), 1): 0.0,\n",
       "              (1, np.int64(0), 1): 0.0,\n",
       "              (1, np.int64(0), 0): 0.0,\n",
       "              (0, np.int64(3), 0): 0.0,\n",
       "              (0, np.int64(1), 0): 0.0,\n",
       "              (1, np.int64(2), 5): 0.0,\n",
       "              (1, np.int64(1), 0): 0.0,\n",
       "              (0, np.int64(0), 0): 0.0,\n",
       "              (1, np.int64(2), 2): 0.0,\n",
       "              (2, np.int64(2), 2): 0.0,\n",
       "              (2, np.int64(1), 3): 0.0,\n",
       "              (3, np.int64(3), 3): 0.0,\n",
       "              (3, np.int64(0), 2): 0.0,\n",
       "              (2, np.int64(3), 2): 0.0,\n",
       "              (2, np.int64(0), 6): 0.0,\n",
       "              (6, np.int64(2), 2): 0.0,\n",
       "              (3, np.int64(2), 3): 0.0,\n",
       "              (3, np.int64(3), 2): 0.0,\n",
       "              (3, np.int64(0), 7): 0.0,\n",
       "              (1, np.int64(0), 5): 0.0,\n",
       "              (0, np.int64(2), 4): 0.0,\n",
       "              (4, np.int64(2), 8): 0.0,\n",
       "              (8, np.int64(3), 8): 0.0,\n",
       "              (8, np.int64(1), 9): 0.0,\n",
       "              (9, np.int64(2), 10): 0.0,\n",
       "              (10, np.int64(3), 6): 0.0,\n",
       "              (6, np.int64(1), 10): 0.0,\n",
       "              (10, np.int64(0), 14): 0.0,\n",
       "              (14, np.int64(0), 14): 0.0,\n",
       "              (14, np.int64(1), 14): 0.0,\n",
       "              (14, np.int64(1), 15): 1.0,\n",
       "              (0, np.int64(0), 4): 0.0,\n",
       "              (4, np.int64(1), 4): 0.0,\n",
       "              (4, np.int64(3), 4): 0.0,\n",
       "              (4, np.int64(2), 0): 0.0,\n",
       "              (0, np.int64(1), 4): 0.0,\n",
       "              (4, np.int64(1), 8): 0.0,\n",
       "              (8, np.int64(0), 8): 0.0,\n",
       "              (8, np.int64(2), 9): 0.0,\n",
       "              (9, np.int64(3), 5): 0.0,\n",
       "              (4, np.int64(1), 5): 0.0,\n",
       "              (4, np.int64(0), 0): 0.0,\n",
       "              (4, np.int64(3), 5): 0.0,\n",
       "              (4, np.int64(3), 0): 0.0,\n",
       "              (1, np.int64(2), 1): 0.0,\n",
       "              (1, np.int64(3), 1): 0.0,\n",
       "              (4, np.int64(2), 5): 0.0,\n",
       "              (1, np.int64(1), 2): 0.0,\n",
       "              (3, np.int64(1), 7): 0.0,\n",
       "              (1, np.int64(3), 2): 0.0,\n",
       "              (3, np.int64(0), 3): 0.0,\n",
       "              (4, 0, 4): 0.0,\n",
       "              (4, 0, 8): 0.0,\n",
       "              (8, 0, 12): 0.0,\n",
       "              (8, 0, 4): 0.0,\n",
       "              (3, np.int64(2), 7): 0.0,\n",
       "              (2, np.int64(0), 2): 0.0,\n",
       "              (6, np.int64(3), 2): 0.0,\n",
       "              (2, np.int64(0), 1): 0.0,\n",
       "              (8, np.int64(2), 12): 0.0,\n",
       "              (2, np.int64(3), 3): 0.0,\n",
       "              (3, np.int64(1), 2): 0.0,\n",
       "              (6, np.int64(1), 5): 0.0,\n",
       "              (8, np.int64(3), 4): 0.0,\n",
       "              (2, np.int64(2), 6): 0.0,\n",
       "              (2, np.int64(3), 1): 0.0,\n",
       "              (2, np.int64(1), 1): 0.0,\n",
       "              (9, np.int64(3), 8): 0.0,\n",
       "              (8, np.int64(1), 8): 0.0,\n",
       "              (9, np.int64(0), 5): 0.0,\n",
       "              (2, np.int64(1), 6): 0.0,\n",
       "              (6, np.int64(0), 5): 0.0,\n",
       "              (2, np.int64(2), 3): 0.0,\n",
       "              (9, 2, 5): 0.0,\n",
       "              (10, 0, 6): 0.0,\n",
       "              (10, 0, 9): 0.0,\n",
       "              (9, 2, 13): 0.0,\n",
       "              (13, 0, 13): 0.0,\n",
       "              (13, 0, 9): 0.0,\n",
       "              (14, 1, 13): 0.0,\n",
       "              (14, 0, 10): 0.0,\n",
       "              (8, 1, 12): 0.0,\n",
       "              (13, 0, 12): 0.0,\n",
       "              (14, 0, 13): 0.0,\n",
       "              (6, 1, 7): 0.0,\n",
       "              (8, np.int64(3), 9): 0.0,\n",
       "              (6, 2, 7): 0.0,\n",
       "              (6, 3, 5): 0.0,\n",
       "              (13, np.int64(3), 12): 0.0,\n",
       "              (8, np.int64(2), 4): 0.0,\n",
       "              (6, 2, 10): 0.0,\n",
       "              (6, 3, 7): 0.0,\n",
       "              (9, np.int64(3), 10): 0.0,\n",
       "              (10, np.int64(1), 14): 0.0,\n",
       "              (14, np.int64(2), 14): 0.0,\n",
       "              (6, np.int64(0), 10): 0.0,\n",
       "              (9, np.int64(0), 8): 0.0,\n",
       "              (10, 1, 9): 0.0,\n",
       "              (14, np.int64(2), 10): 0.0,\n",
       "              (10, 1, 11): 0.0,\n",
       "              (10, np.int64(2), 6): 0.0,\n",
       "              (14, np.int64(2), 15): 1.0,\n",
       "              (9, np.int64(1), 13): 0.0,\n",
       "              (13, np.int64(2), 13): 0.0,\n",
       "              (13, np.int64(3), 14): 0.0,\n",
       "              (14, np.int64(3), 13): 0.0,\n",
       "              (13, np.int64(2), 9): 0.0,\n",
       "              (13, 3, 9): 0.0,\n",
       "              (9, 1, 10): 0.0,\n",
       "              (9, 1, 8): 0.0,\n",
       "              (6, np.int64(0), 2): 0.0,\n",
       "              (10, np.int64(2), 11): 0.0,\n",
       "              (13, np.int64(2), 14): 0.0,\n",
       "              (3, np.int64(1), 3): 0.0,\n",
       "              (10, np.int64(3), 11): 0.0,\n",
       "              (14, np.int64(3), 10): 0.0,\n",
       "              (10, np.int64(3), 9): 0.0,\n",
       "              (9, np.int64(0), 13): 0.0,\n",
       "              (13, np.int64(1), 13): 0.0,\n",
       "              (10, np.int64(2), 14): 0.0,\n",
       "              (14, np.int64(3), 15): 1.0,\n",
       "              (13, np.int64(1), 12): 0.0,\n",
       "              (13, np.int64(1), 14): 0.0}),\n",
       " defaultdict(float,\n",
       "             {0: 0.06728044593226164,\n",
       "              4: 0.08949566650564644,\n",
       "              1: 0.05959836436877796,\n",
       "              5: 0.0,\n",
       "              2: 0.0724044985667855,\n",
       "              6: 0.10815966030443007,\n",
       "              3: 0.05412500111151787,\n",
       "              7: 0.0,\n",
       "              8: 0.1413314436193564,\n",
       "              10: 0.29257570289608853,\n",
       "              9: 0.24156190537862898,\n",
       "              14: 0.6379993669594402,\n",
       "              11: 0.0,\n",
       "              12: 0.0,\n",
       "              13: 0.3742579049502607,\n",
       "              15: 0.0}))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.select_action(1), agent.transits, agent.rewards, agent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transition probabilities and rewards\n",
    "env = gym.make(ENV_NAME)\n",
    "env.unwrapped.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36a6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1de1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a1d939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(16), np.int64(4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00717e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(num_states, num_actions, gamma=0.9):\n",
    "    \"\"\"Function to perform value iteration algorithm.\"\"\"\n",
    "    policy = np.zeros(num_states)\n",
    "    V = np.zeros(num_states)\n",
    "    V_old = -1 * np.ones(num_states)\n",
    "    while np.max(np.abs(V - V_old)) > 1e-3:\n",
    "        V_old = V.copy()\n",
    "        for s in range(num_states):\n",
    "            q = np.zeros(num_actions)\n",
    "            for a in range(num_actions):\n",
    "                for prob, next_s, reward, is_done in env.unwrapped.P[s][a]:\n",
    "                    q[a] += prob * (reward + gamma * V_old[next_s]) # Computing the Q Value using the bellman equation of optimality\n",
    "            V[s] = np.max(q) # Comouting the value(estimate) of a state\n",
    "            policy[s] = np.argmax(q)\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232e4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "value, policy = value_iteration(num_states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e6eae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.06253203, 0.05606969, 0.07051342, 0.05159888],\n",
       "        [0.08603505, 0.        , 0.11007757, 0.        ],\n",
       "        [0.14066756, 0.24422494, 0.29731176, 0.        ],\n",
       "        [0.        , 0.3775274 , 0.6377414 , 0.        ]]),\n",
       " array([[0., 3., 0., 3.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [3., 1., 0., 0.],\n",
       "        [0., 2., 1., 0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.reshape(4, 4), policy.reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e59bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing & Recording trained agent ===\n",
      "Test Episode 1: reward=1.0\n",
      "Test Episode 2: reward=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_viter folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 3: reward=1.0\n",
      "Test Episode 4: reward=0.0\n",
      "Test Episode 5: reward=1.0\n",
      "All 5 test episodes recorded in ./videos_frozenlake_viter folder\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "env_name = \"FrozenLake-v1\"\n",
    "print(\"\\n=== Testing & Recording trained agent ===\")\n",
    "video_env = RecordVideo(\n",
    "        gym.make(env_name, render_mode=\"rgb_array\"),\n",
    "        video_folder=\"./videos_frozenlake_viter\",\n",
    "        episode_trigger=lambda ep_id: True  # record every test episode\n",
    "    )\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = video_env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = int(policy[state])\n",
    "        new_state, reward, is_done, is_trunc, _ = video_env.step(action)\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "        if is_done or is_trunc:\n",
    "            break\n",
    "    print(f\"Test Episode {ep+1}: reward={total_reward}\")\n",
    "\n",
    "video_env.close()\n",
    "print(f\"All {num_episodes} test episodes recorded in ./videos_frozenlake_viter folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fe930",
   "metadata": {},
   "source": [
    "## Value Iteration on the gymnasium Taxi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b285fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7985e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678dd3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(500), np.int64(6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f114d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(num_states, num_actions, gamma=0.9):\n",
    "    \"\"\"Function to perform value iteration algorithm.\"\"\"\n",
    "    policy = np.zeros(num_states)\n",
    "    V = np.zeros(num_states)\n",
    "    V_old = -1 * np.ones(num_states)\n",
    "    while np.max(np.abs(V - V_old)) > 1e-3:\n",
    "        V_old = V.copy()\n",
    "        for s in range(num_states):\n",
    "            q = np.zeros(num_actions)\n",
    "            for a in range(num_actions):\n",
    "                for prob, next_s, reward, is_done in env.unwrapped.P[s][a]:\n",
    "                    q[a] += prob * (reward + gamma * V_old[next_s]) # Computing the Q Value using the bellman equation of optimality\n",
    "            V[s] = np.max(q) # Comouting the value(estimate) of a state\n",
    "            policy[s] = np.argmax(q)\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9515fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "value, policy = value_iteration(num_states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a384a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 89.46916234,  32.81563744,  55.26016234,  37.57393009,\n",
       "          8.42815264,  32.81563744,   8.42815264,  15.28085117,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         12.75186641,  18.08978465,  12.75186641,  37.57393009,\n",
       "        100.52229109,  37.57393009,  62.51229109,  42.85987234,\n",
       "         79.52229109,  28.53411868,  48.73419109,  32.81563744,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         89.46916234,  42.85987234,  55.26016234,  48.73419109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         24.67980717,  70.56916234,  24.67980717,  37.57393009,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         48.73419109,  79.52229109,  48.73419109,  55.26016234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         28.53411868,  79.52229109,  28.53411868,  42.85987234,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         42.85987234,  89.46916234,  42.85987234,  62.51229109,\n",
       "         32.81563744,   8.42815264,  18.08978465,  10.47672475,\n",
       "         32.81563744,  89.46916234,  32.81563744,  48.73419109,\n",
       "         18.08978465,   8.42815264,  32.81563744,  10.47672475,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         37.57393009, 100.52229109,  37.57393009,  55.26016234,\n",
       "         79.52229109,  28.53411868,  48.73419109,  32.81563744,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         37.57393009,  21.21187144,  62.51229109,  24.67980717,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         89.46916234,  42.85987234,  70.56916234,  48.73419109,\n",
       "         70.56916234,  24.67980717,  42.85987234,  28.53411868,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         79.52229109,  48.73419109,  62.51229109,  55.26016234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         21.21187144,  62.51229109,  21.21187144,  32.81563744,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         55.26016234,  70.56916234,  55.26016234,  62.51229109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         24.67980717,  70.56916234,  24.67980717,  37.57393009,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         24.67980717,  32.81563744,  24.67980717,  62.51229109,\n",
       "         48.73419109,  79.52229109,  48.73419109,  70.56916234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         28.53411868,  79.52229109,  28.53411868,  42.85987234,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         42.85987234,  89.46916234,  42.85987234,  62.51229109,\n",
       "         70.56916234,  24.67980717,  42.85987234,  28.53411868,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         42.85987234,  24.67980717,  70.56916234,  28.53411868,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         79.52229109,  48.73419109,  79.52229109,  55.26016234,\n",
       "         62.51229109,  21.21187144,  37.57393009,  24.67980717,\n",
       "         15.28085117,  48.73419109,  15.28085117,  24.67980717,\n",
       "         37.57393009,  21.21187144,  62.51229109,  24.67980717,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         70.56916234,  55.26016234,  70.56916234,  62.51229109,\n",
       "         55.26016234,  18.08978465,  32.81563744,  21.21187144,\n",
       "         18.08978465,  55.26016234,  18.08978465,  28.53411868,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         24.67980717,  32.81563744,  24.67980717,  62.51229109,\n",
       "         62.51229109,  62.51229109,  62.51229109,  70.56916234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         21.21187144,  62.51229109,  21.21187144,  32.81563744,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         28.53411868,  37.57393009,  28.53411868,  70.56916234,\n",
       "         55.26016234,  70.56916234,  55.26016234,  79.52229109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         24.67980717,  70.56916234,  24.67980717,  37.57393009,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         24.67980717,  32.81563744,  24.67980717,  62.51229109,\n",
       "         48.73419109,  79.52229109,  48.73419109,  70.56916234,\n",
       "         62.51229109,  21.21187144,  37.57393009,  24.67980717,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         48.73419109,  28.53411868,  79.52229109,  32.81563744,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         70.56916234,  42.85987234,  89.46916234,  48.73419109,\n",
       "         55.26016234,  18.08978465,  32.81563744,  21.21187144,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         62.51229109,  48.73419109,  62.51229109,  55.26016234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         15.28085117,  48.73419109,  15.28085117,  24.67980717,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         55.26016234,  55.26016234,  55.26016234,  62.51229109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         18.08978465,  55.26016234,  18.08978465,  28.53411868,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         32.81563744,  42.85987234,  32.81563744,  79.52229109,\n",
       "         48.73419109,  62.51229109,  48.73419109,  89.46916234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         21.21187144,  62.51229109,  21.21187144,  32.81563744,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         28.53411868,  37.57393009,  28.53411868,  70.56916234,\n",
       "         42.85987234,  70.56916234,  42.85987234,  79.52229109,\n",
       "         55.26016234,  18.08978465,  32.81563744,  21.21187144,\n",
       "          8.42815264,  32.81563744,   8.42815264,  15.28085117,\n",
       "         55.26016234,  32.81563744,  89.46916234,  37.57393009,\n",
       "         12.75186641,  18.08978465,  12.75186641,  37.57393009,\n",
       "         62.51229109,  37.57393009, 100.52229109,  42.85987234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         55.26016234,  42.85987234,  55.26016234,  48.73419109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         48.73419109,  48.73419109,  48.73419109,  55.26016234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         15.28085117,  48.73419109,  15.28085117,  24.67980717,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         37.57393009,  48.73419109,  37.57393009,  89.46916234,\n",
       "         42.85987234,  55.26016234,  42.85987234, 100.52229109,\n",
       "         32.81563744,   8.42815264,  18.08978465,  10.47672475,\n",
       "         18.08978465,  55.26016234,  18.08978465,  28.53411868,\n",
       "         18.08978465,   8.42815264,  32.81563744,  10.47672475,\n",
       "         32.81563744,  42.85987234,  32.81563744,  79.52229109,\n",
       "         37.57393009,  62.51229109,  37.57393009,  89.46916234]),\n",
       " array([4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 5.,\n",
       "        0., 0., 0., 3., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 4.,\n",
       "        4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 5., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        2., 2., 2., 2., 0., 0., 0., 0., 2., 2., 2., 2., 1., 2., 0., 2., 1.,\n",
       "        1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3., 2., 2., 2., 2., 1., 2.,\n",
       "        3., 2., 3., 3., 3., 3., 1., 1., 1., 1., 3., 3., 3., 3., 2., 2., 2.,\n",
       "        2., 3., 1., 3., 2., 3., 3., 3., 3., 1., 1., 1., 1., 3., 3., 3., 3.,\n",
       "        0., 0., 0., 0., 3., 1., 3., 0., 3., 3., 3., 3., 1., 1., 1., 1., 3.,\n",
       "        3., 3., 3., 0., 0., 0., 0., 3., 1., 3., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        4., 4., 4., 4., 1., 1., 1., 1., 1., 1., 5., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 4., 4., 4., 4.,\n",
       "        1., 1., 1., 5., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3.,\n",
       "        3., 3., 3., 1., 1., 1., 3.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a9a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing & Recording trained agent ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_taxi_viter folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1: reward=11\n",
      "Test Episode 2: reward=4\n",
      "Test Episode 3: reward=7\n",
      "Test Episode 4: reward=7\n",
      "Test Episode 5: reward=12\n",
      "All 5 test episodes recorded in ./videos_taxi_viter folder\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "env_name = \"Taxi-v3\"\n",
    "print(\"\\n=== Testing & Recording trained agent ===\")\n",
    "video_env = RecordVideo(\n",
    "        gym.make(env_name, render_mode=\"rgb_array\"),\n",
    "        video_folder=\"./videos_taxi_viter\",\n",
    "        episode_trigger=lambda ep_id: True  # record every test episode\n",
    "    )\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = video_env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = int(policy[state])\n",
    "        new_state, reward, is_done, is_trunc, _ = video_env.step(action)\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "        if is_done or is_trunc:\n",
    "            break\n",
    "    print(f\"Test Episode {ep+1}: reward={total_reward}\")\n",
    "\n",
    "video_env.close()\n",
    "print(f\"All {num_episodes} test episodes recorded in ./videos_taxi_viter folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b2984",
   "metadata": {},
   "source": [
    "## Tabular Q Learning on the FrozenLake Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5198a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaaae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "EPSILON = 0.1\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc372d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = int\n",
    "Action = int\n",
    "ValuesKey = tt.Tuple[State, Action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.values: tt.Dict[ValuesKey, float] = defaultdict(float) \n",
    "\n",
    "    def sample_env(self) -> tt.Tuple[State, Action, float, State]:\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, is_trunc, _ = self.env.step(action)\n",
    "        if is_done or is_trunc:\n",
    "            self.state, _ = self.env.reset()\n",
    "        else:\n",
    "            self.state = new_state\n",
    "        return old_state, action, reward, new_state\n",
    "    \n",
    "    def best_value_and_action(self, state: State) -> tt.Tuple[float, Action]:\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        if best_action is None:\n",
    "            best_action = self.env.action_space.sample()\n",
    "            best_value = self.values[(state, best_action)]\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, state: State, action: Action, reward: float, next_state: State):\n",
    "        best_value, _ = self.best_value_and_action(next_state)\n",
    "        # Q learning update\n",
    "        self.values[(state, action)] = (1 - ALPHA) * self.values[(state, action)] + ALPHA * (reward + GAMMA * best_value)\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, is_trunc, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done or is_trunc:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbaf455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2216: Best reward updated: 0.000 -> 0.050\n",
      "2219: Best reward updated: 0.050 -> 0.100\n",
      "2249: Best reward updated: 0.100 -> 0.200\n",
      "2466: Best reward updated: 0.200 -> 0.250\n",
      "2499: Best reward updated: 0.250 -> 0.400\n",
      "4732: Best reward updated: 0.400 -> 0.450\n",
      "4741: Best reward updated: 0.450 -> 0.500\n",
      "4742: Best reward updated: 0.500 -> 0.700\n",
      "4760: Best reward updated: 0.700 -> 0.850\n",
      "Solved in 4760 iterations!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        state, action, reward, next_state = agent.sample_env()\n",
    "        agent.value_update(state, action, reward, next_state)\n",
    "\n",
    "        test_reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            test_reward += agent.play_episode(test_env)\n",
    "        test_reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", test_reward, iter_no)\n",
    "        if test_reward > best_reward:\n",
    "            print(f\"{iter_no}: Best reward updated: {best_reward:.3f} -> {test_reward:.3f}\")\n",
    "            best_reward = test_reward\n",
    "        if test_reward > 0.8:\n",
    "            print(f\"Solved in {iter_no} iterations!\")\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a89ccc",
   "metadata": {},
   "source": [
    "## Sarsa Algorithm on FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f50e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ad5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b1835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = int\n",
    "Action = int\n",
    "ValuesKey = tt.Tuple[State, Action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80995262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
