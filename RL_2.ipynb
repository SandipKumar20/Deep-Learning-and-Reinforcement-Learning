{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615ffa20",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1fc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 100\n",
    "\n",
    "State = int\n",
    "Action = int\n",
    "RewardKey = tt.Tuple[State, Action, State]\n",
    "TransitKey = tt.Tuple[State, Action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db2615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.rewards: tt.Dict[RewardKey, float] = defaultdict(float)\n",
    "        self.transits: tt.Dict[TransitKey, Counter] = defaultdict(Counter)\n",
    "        self.values: tt.Dict[State, float] = defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, n: int):\n",
    "        for _ in range(n):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, is_trunc, _ = self.env.step(action)\n",
    "            rw_key = (self.state, action, new_state)\n",
    "            self.rewards[rw_key] = float(reward)\n",
    "            tr_key = (self.state, action)\n",
    "            self.transits[tr_key][new_state] += 1\n",
    "            if is_done or is_trunc:\n",
    "                self.state, _ = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "    def calc_action_value(self, state: State, action: Action) -> float:\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            rw_key = (state, action, tgt_state)\n",
    "            reward = self.rewards[rw_key]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state: State) -> Action:\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, is_trunc, _ = env.step(action)\n",
    "            rw_key = (state, action, new_state)\n",
    "            self.rewards[rw_key] = float(reward)\n",
    "            tr_key = (state, action)\n",
    "            self.transits[tr_key][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done or is_trunc:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            self.values[state] = max(state_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a99f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: Best reward updated: 0.000 -> 0.010\n",
      "5: Best reward updated: 0.010 -> 0.080\n",
      "6: Best reward updated: 0.080 -> 0.120\n",
      "10: Best reward updated: 0.120 -> 0.300\n",
      "12: Best reward updated: 0.300 -> 0.430\n",
      "13: Best reward updated: 0.430 -> 0.440\n",
      "17: Best reward updated: 0.440 -> 0.490\n",
      "18: Best reward updated: 0.490 -> 0.640\n",
      "19: Best reward updated: 0.640 -> 0.740\n",
      "20: Best reward updated: 0.740 -> 0.770\n",
      "21: Best reward updated: 0.770 -> 0.800\n",
      "67: Best reward updated: 0.800 -> 0.810\n",
      "Solved in 67 iterations!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(f\"{iter_no}: Best reward updated: {best_reward:.3f} -> {reward:.3f}\")\n",
    "            best_reward = reward\n",
    "        if reward > 0.8:\n",
    "            print(f\"Solved in {iter_no} iterations!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efea1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing & Recording trained agent ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1: reward=1.0\n",
      "Test Episode 2: reward=1.0\n",
      "Test Episode 3: reward=0.0\n",
      "Test Episode 4: reward=1.0\n",
      "Test Episode 5: reward=1.0\n",
      "All 5 test episodes recorded in ./videos folder\n"
     ]
    }
   ],
   "source": [
    "# -------- Testing + Recording Loop --------\n",
    "num_episodes = 5\n",
    "print(\"\\n=== Testing & Recording trained agent ===\")\n",
    "video_env = RecordVideo(\n",
    "        gym.make(ENV_NAME, render_mode=\"rgb_array\"),\n",
    "        video_folder=\"./videos\",\n",
    "        episode_trigger=lambda ep_id: True  # record every test episode\n",
    "    )\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = video_env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        new_state, reward, is_done, is_trunc, _ = video_env.step(action)\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "        if is_done or is_trunc:\n",
    "            break\n",
    "    print(f\"Test Episode {ep+1}: reward={total_reward}\")\n",
    "\n",
    "video_env.close()\n",
    "print(f\"All {num_episodes} test episodes recorded in ./videos folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22df21ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " defaultdict(collections.Counter,\n",
       "             {(0, np.int64(2)): Counter({4: 1935, 0: 1925, 1: 1917}),\n",
       "              (0, np.int64(3)): Counter({0: 492, 1: 248}),\n",
       "              (1, np.int64(1)): Counter({2: 112, 0: 99, 5: 71}),\n",
       "              (1, np.int64(3)): Counter({1: 1471, 0: 1402, 2: 1322}),\n",
       "              (0, np.int64(1)): Counter({4: 368, 0: 347, 1: 346}),\n",
       "              (1, np.int64(0)): Counter({5: 99, 1: 98, 0: 98}),\n",
       "              (1, np.int64(2)): Counter({2: 100, 1: 97, 5: 91}),\n",
       "              (0, np.int64(0)): Counter({0: 46137, 4: 23157}),\n",
       "              (2, np.int64(2)): Counter({2: 1500, 3: 1499, 6: 1428}),\n",
       "              (2, np.int64(1)): Counter({1: 47, 3: 46, 6: 45}),\n",
       "              (3, np.int64(3)): Counter({3: 3028, 2: 1474}),\n",
       "              (3, np.int64(0)): Counter({3: 30, 7: 22, 2: 19}),\n",
       "              (2, np.int64(3)): Counter({3: 51, 2: 47, 1: 43}),\n",
       "              (2, np.int64(0)): Counter({2: 843, 6: 836, 1: 795}),\n",
       "              (6, np.int64(2)): Counter({2: 1717, 7: 1669, 10: 1652}),\n",
       "              (3, np.int64(2)): Counter({3: 52, 7: 21}),\n",
       "              (4, np.int64(2)): Counter({8: 143, 0: 129, 5: 121}),\n",
       "              (8, np.int64(3)): Counter({8: 12780, 4: 12639, 9: 12480}),\n",
       "              (8, np.int64(1)): Counter({9: 115, 8: 99, 12: 95}),\n",
       "              (9, np.int64(2)): Counter({5: 196, 13: 188, 10: 164}),\n",
       "              (10, np.int64(3)): Counter({6: 9, 11: 7, 9: 4}),\n",
       "              (6, np.int64(1)): Counter({5: 21, 7: 17, 10: 14}),\n",
       "              (10, np.int64(0)): Counter({9: 2856, 6: 2855, 14: 2759}),\n",
       "              (14, np.int64(0)): Counter({14: 8, 13: 7, 10: 6}),\n",
       "              (14, np.int64(1)): Counter({15: 3979, 13: 3950, 14: 3827}),\n",
       "              (4, np.int64(1)): Counter({5: 119, 4: 99, 8: 91}),\n",
       "              (4, np.int64(3)): Counter({5: 99, 4: 94, 0: 86}),\n",
       "              (8, np.int64(0)): Counter({4: 302, 8: 240, 12: 237}),\n",
       "              (8, np.int64(2)): Counter({9: 37, 4: 36, 12: 28}),\n",
       "              (9, np.int64(3)): Counter({10: 12, 5: 11, 8: 11}),\n",
       "              (4, np.int64(0)): Counter({4: 19026, 8: 18897, 0: 18694}),\n",
       "              (3, np.int64(1)): Counter({7: 28, 2: 27, 3: 14}),\n",
       "              (5, 0): Counter(),\n",
       "              (5, 1): Counter(),\n",
       "              (5, 2): Counter(),\n",
       "              (5, 3): Counter(),\n",
       "              (6, 0): Counter({5: 19, 10: 16, 2: 15}),\n",
       "              (6, 3): Counter({7: 14, 2: 13, 5: 11}),\n",
       "              (7, 0): Counter(),\n",
       "              (7, 1): Counter(),\n",
       "              (7, 2): Counter(),\n",
       "              (7, 3): Counter(),\n",
       "              (9, 0): Counter({8: 17, 5: 9, 13: 9}),\n",
       "              (9, 1): Counter({8: 6880, 13: 6800, 10: 6690}),\n",
       "              (10, 1): Counter({9: 21, 14: 18, 11: 12}),\n",
       "              (10, 2): Counter({6: 10, 11: 8, 14: 6}),\n",
       "              (11, 0): Counter(),\n",
       "              (11, 1): Counter(),\n",
       "              (11, 2): Counter(),\n",
       "              (11, 3): Counter(),\n",
       "              (12, 0): Counter(),\n",
       "              (12, 1): Counter(),\n",
       "              (12, 2): Counter(),\n",
       "              (12, 3): Counter(),\n",
       "              (13, 0): Counter({12: 121, 9: 117, 13: 115}),\n",
       "              (13, 1): Counter({13: 9, 12: 4, 14: 4}),\n",
       "              (13, 2): Counter({13: 5379, 9: 5326, 14: 5194}),\n",
       "              (13, 3): Counter({9: 64, 12: 49, 14: 47}),\n",
       "              (14, 2): Counter({14: 26, 10: 18, 15: 18}),\n",
       "              (14, 3): Counter({13: 6, 10: 6, 15: 4}),\n",
       "              (15, 0): Counter(),\n",
       "              (15, 1): Counter(),\n",
       "              (15, 2): Counter(),\n",
       "              (15, 3): Counter()}),\n",
       " defaultdict(float,\n",
       "             {(0, np.int64(2), 0): 0.0,\n",
       "              (0, np.int64(3), 1): 0.0,\n",
       "              (1, np.int64(1), 5): 0.0,\n",
       "              (0, np.int64(2), 1): 0.0,\n",
       "              (1, np.int64(3), 0): 0.0,\n",
       "              (0, np.int64(1), 1): 0.0,\n",
       "              (1, np.int64(0), 1): 0.0,\n",
       "              (1, np.int64(0), 0): 0.0,\n",
       "              (0, np.int64(3), 0): 0.0,\n",
       "              (0, np.int64(1), 0): 0.0,\n",
       "              (1, np.int64(2), 5): 0.0,\n",
       "              (1, np.int64(1), 0): 0.0,\n",
       "              (0, np.int64(0), 0): 0.0,\n",
       "              (1, np.int64(2), 2): 0.0,\n",
       "              (2, np.int64(2), 2): 0.0,\n",
       "              (2, np.int64(1), 3): 0.0,\n",
       "              (3, np.int64(3), 3): 0.0,\n",
       "              (3, np.int64(0), 2): 0.0,\n",
       "              (2, np.int64(3), 2): 0.0,\n",
       "              (2, np.int64(0), 6): 0.0,\n",
       "              (6, np.int64(2), 2): 0.0,\n",
       "              (3, np.int64(2), 3): 0.0,\n",
       "              (3, np.int64(3), 2): 0.0,\n",
       "              (3, np.int64(0), 7): 0.0,\n",
       "              (1, np.int64(0), 5): 0.0,\n",
       "              (0, np.int64(2), 4): 0.0,\n",
       "              (4, np.int64(2), 8): 0.0,\n",
       "              (8, np.int64(3), 8): 0.0,\n",
       "              (8, np.int64(1), 9): 0.0,\n",
       "              (9, np.int64(2), 10): 0.0,\n",
       "              (10, np.int64(3), 6): 0.0,\n",
       "              (6, np.int64(1), 10): 0.0,\n",
       "              (10, np.int64(0), 14): 0.0,\n",
       "              (14, np.int64(0), 14): 0.0,\n",
       "              (14, np.int64(1), 14): 0.0,\n",
       "              (14, np.int64(1), 15): 1.0,\n",
       "              (0, np.int64(0), 4): 0.0,\n",
       "              (4, np.int64(1), 4): 0.0,\n",
       "              (4, np.int64(3), 4): 0.0,\n",
       "              (4, np.int64(2), 0): 0.0,\n",
       "              (0, np.int64(1), 4): 0.0,\n",
       "              (4, np.int64(1), 8): 0.0,\n",
       "              (8, np.int64(0), 8): 0.0,\n",
       "              (8, np.int64(2), 9): 0.0,\n",
       "              (9, np.int64(3), 5): 0.0,\n",
       "              (4, np.int64(1), 5): 0.0,\n",
       "              (4, np.int64(0), 0): 0.0,\n",
       "              (4, np.int64(3), 5): 0.0,\n",
       "              (4, np.int64(3), 0): 0.0,\n",
       "              (1, np.int64(2), 1): 0.0,\n",
       "              (1, np.int64(3), 1): 0.0,\n",
       "              (4, np.int64(2), 5): 0.0,\n",
       "              (1, np.int64(1), 2): 0.0,\n",
       "              (3, np.int64(1), 7): 0.0,\n",
       "              (1, np.int64(3), 2): 0.0,\n",
       "              (3, np.int64(0), 3): 0.0,\n",
       "              (4, 0, 4): 0.0,\n",
       "              (4, 0, 8): 0.0,\n",
       "              (8, 0, 12): 0.0,\n",
       "              (8, 0, 4): 0.0,\n",
       "              (3, np.int64(2), 7): 0.0,\n",
       "              (2, np.int64(0), 2): 0.0,\n",
       "              (6, np.int64(3), 2): 0.0,\n",
       "              (2, np.int64(0), 1): 0.0,\n",
       "              (8, np.int64(2), 12): 0.0,\n",
       "              (2, np.int64(3), 3): 0.0,\n",
       "              (3, np.int64(1), 2): 0.0,\n",
       "              (6, np.int64(1), 5): 0.0,\n",
       "              (8, np.int64(3), 4): 0.0,\n",
       "              (2, np.int64(2), 6): 0.0,\n",
       "              (2, np.int64(3), 1): 0.0,\n",
       "              (2, np.int64(1), 1): 0.0,\n",
       "              (9, np.int64(3), 8): 0.0,\n",
       "              (8, np.int64(1), 8): 0.0,\n",
       "              (9, np.int64(0), 5): 0.0,\n",
       "              (2, np.int64(1), 6): 0.0,\n",
       "              (6, np.int64(0), 5): 0.0,\n",
       "              (2, np.int64(2), 3): 0.0,\n",
       "              (9, 2, 5): 0.0,\n",
       "              (10, 0, 6): 0.0,\n",
       "              (10, 0, 9): 0.0,\n",
       "              (9, 2, 13): 0.0,\n",
       "              (13, 0, 13): 0.0,\n",
       "              (13, 0, 9): 0.0,\n",
       "              (14, 1, 13): 0.0,\n",
       "              (14, 0, 10): 0.0,\n",
       "              (8, 1, 12): 0.0,\n",
       "              (13, 0, 12): 0.0,\n",
       "              (14, 0, 13): 0.0,\n",
       "              (6, 1, 7): 0.0,\n",
       "              (8, np.int64(3), 9): 0.0,\n",
       "              (6, 2, 7): 0.0,\n",
       "              (6, 3, 5): 0.0,\n",
       "              (13, np.int64(3), 12): 0.0,\n",
       "              (8, np.int64(2), 4): 0.0,\n",
       "              (6, 2, 10): 0.0,\n",
       "              (6, 3, 7): 0.0,\n",
       "              (9, np.int64(3), 10): 0.0,\n",
       "              (10, np.int64(1), 14): 0.0,\n",
       "              (14, np.int64(2), 14): 0.0,\n",
       "              (6, np.int64(0), 10): 0.0,\n",
       "              (9, np.int64(0), 8): 0.0,\n",
       "              (10, 1, 9): 0.0,\n",
       "              (14, np.int64(2), 10): 0.0,\n",
       "              (10, 1, 11): 0.0,\n",
       "              (10, np.int64(2), 6): 0.0,\n",
       "              (14, np.int64(2), 15): 1.0,\n",
       "              (9, np.int64(1), 13): 0.0,\n",
       "              (13, np.int64(2), 13): 0.0,\n",
       "              (13, np.int64(3), 14): 0.0,\n",
       "              (14, np.int64(3), 13): 0.0,\n",
       "              (13, np.int64(2), 9): 0.0,\n",
       "              (13, 3, 9): 0.0,\n",
       "              (9, 1, 10): 0.0,\n",
       "              (9, 1, 8): 0.0,\n",
       "              (6, np.int64(0), 2): 0.0,\n",
       "              (10, np.int64(2), 11): 0.0,\n",
       "              (13, np.int64(2), 14): 0.0,\n",
       "              (3, np.int64(1), 3): 0.0,\n",
       "              (10, np.int64(3), 11): 0.0,\n",
       "              (14, np.int64(3), 10): 0.0,\n",
       "              (10, np.int64(3), 9): 0.0,\n",
       "              (9, np.int64(0), 13): 0.0,\n",
       "              (13, np.int64(1), 13): 0.0,\n",
       "              (10, np.int64(2), 14): 0.0,\n",
       "              (14, np.int64(3), 15): 1.0,\n",
       "              (13, np.int64(1), 12): 0.0,\n",
       "              (13, np.int64(1), 14): 0.0}),\n",
       " defaultdict(float,\n",
       "             {0: 0.06728044593226164,\n",
       "              4: 0.08949566650564644,\n",
       "              1: 0.05959836436877796,\n",
       "              5: 0.0,\n",
       "              2: 0.0724044985667855,\n",
       "              6: 0.10815966030443007,\n",
       "              3: 0.05412500111151787,\n",
       "              7: 0.0,\n",
       "              8: 0.1413314436193564,\n",
       "              10: 0.29257570289608853,\n",
       "              9: 0.24156190537862898,\n",
       "              14: 0.6379993669594402,\n",
       "              11: 0.0,\n",
       "              12: 0.0,\n",
       "              13: 0.3742579049502607,\n",
       "              15: 0.0}))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.select_action(1), agent.transits, agent.rewards, agent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transition probabilities and rewards\n",
    "env = gym.make(ENV_NAME)\n",
    "env.unwrapped.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36a6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1de1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a1d939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(16), np.int64(4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00717e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(num_states, num_actions, gamma=0.9):\n",
    "    \"\"\"Function to perform value iteration algorithm.\"\"\"\n",
    "    policy = np.zeros(num_states)\n",
    "    V = np.zeros(num_states)\n",
    "    V_old = -1 * np.ones(num_states)\n",
    "    while np.max(np.abs(V - V_old)) > 1e-3:\n",
    "        V_old = V.copy()\n",
    "        for s in range(num_states):\n",
    "            q = np.zeros(num_actions)\n",
    "            for a in range(num_actions):\n",
    "                for prob, next_s, reward, is_done in env.unwrapped.P[s][a]:\n",
    "                    q[a] += prob * (reward + gamma * V_old[next_s]) # Computing the Q Value using the bellman equation of optimality\n",
    "            V[s] = np.max(q) # Comouting the value(estimate) of a state\n",
    "            policy[s] = np.argmax(q)\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232e4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "value, policy = value_iteration(num_states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e6eae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.06253203, 0.05606969, 0.07051342, 0.05159888],\n",
       "        [0.08603505, 0.        , 0.11007757, 0.        ],\n",
       "        [0.14066756, 0.24422494, 0.29731176, 0.        ],\n",
       "        [0.        , 0.3775274 , 0.6377414 , 0.        ]]),\n",
       " array([[0., 3., 0., 3.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [3., 1., 0., 0.],\n",
       "        [0., 2., 1., 0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.reshape(4, 4), policy.reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e59bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing & Recording trained agent ===\n",
      "Test Episode 1: reward=1.0\n",
      "Test Episode 2: reward=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_viter folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 3: reward=1.0\n",
      "Test Episode 4: reward=0.0\n",
      "Test Episode 5: reward=1.0\n",
      "All 5 test episodes recorded in ./videos_frozenlake_viter folder\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "env_name = \"FrozenLake-v1\"\n",
    "print(\"\\n=== Testing & Recording trained agent ===\")\n",
    "video_env = RecordVideo(\n",
    "        gym.make(env_name, render_mode=\"rgb_array\"),\n",
    "        video_folder=\"./videos_frozenlake_viter\",\n",
    "        episode_trigger=lambda ep_id: True  # record every test episode\n",
    "    )\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = video_env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = int(policy[state])\n",
    "        new_state, reward, is_done, is_trunc, _ = video_env.step(action)\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "        if is_done or is_trunc:\n",
    "            break\n",
    "    print(f\"Test Episode {ep+1}: reward={total_reward}\")\n",
    "\n",
    "video_env.close()\n",
    "print(f\"All {num_episodes} test episodes recorded in ./videos_frozenlake_viter folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fe930",
   "metadata": {},
   "source": [
    "## Value Iteration on the gymnasium Taxi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b285fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7985e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678dd3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(500), np.int64(6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(num_states, num_actions, gamma=0.9):\n",
    "    \"\"\"Function to perform value iteration algorithm.\"\"\"\n",
    "    policy = np.zeros(num_states)\n",
    "    V = np.zeros(num_states)\n",
    "    V_old = -1 * np.ones(num_states)\n",
    "    while np.max(np.abs(V - V_old)) > 1e-3:\n",
    "        V_old = V.copy()\n",
    "        for s in range(num_states):\n",
    "            q = np.zeros(num_actions)\n",
    "            for a in range(num_actions):\n",
    "                for prob, next_s, reward, is_done in env.unwrapped.P[s][a]:\n",
    "                    q[a] += prob * (reward + gamma * V_old[next_s]) # Computing the Q Value using the bellman equation of optimality\n",
    "            V[s] = np.max(q) # Computing the value(estimate) of a state\n",
    "            policy[s] = np.argmax(q)\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9515fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "value, policy = value_iteration(num_states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a384a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 89.46916234,  32.81563744,  55.26016234,  37.57393009,\n",
       "          8.42815264,  32.81563744,   8.42815264,  15.28085117,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         12.75186641,  18.08978465,  12.75186641,  37.57393009,\n",
       "        100.52229109,  37.57393009,  62.51229109,  42.85987234,\n",
       "         79.52229109,  28.53411868,  48.73419109,  32.81563744,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         89.46916234,  42.85987234,  55.26016234,  48.73419109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         24.67980717,  70.56916234,  24.67980717,  37.57393009,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         48.73419109,  79.52229109,  48.73419109,  55.26016234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         28.53411868,  79.52229109,  28.53411868,  42.85987234,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         42.85987234,  89.46916234,  42.85987234,  62.51229109,\n",
       "         32.81563744,   8.42815264,  18.08978465,  10.47672475,\n",
       "         32.81563744,  89.46916234,  32.81563744,  48.73419109,\n",
       "         18.08978465,   8.42815264,  32.81563744,  10.47672475,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         37.57393009, 100.52229109,  37.57393009,  55.26016234,\n",
       "         79.52229109,  28.53411868,  48.73419109,  32.81563744,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         37.57393009,  21.21187144,  62.51229109,  24.67980717,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         89.46916234,  42.85987234,  70.56916234,  48.73419109,\n",
       "         70.56916234,  24.67980717,  42.85987234,  28.53411868,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         79.52229109,  48.73419109,  62.51229109,  55.26016234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         21.21187144,  62.51229109,  21.21187144,  32.81563744,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         55.26016234,  70.56916234,  55.26016234,  62.51229109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         24.67980717,  70.56916234,  24.67980717,  37.57393009,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         24.67980717,  32.81563744,  24.67980717,  62.51229109,\n",
       "         48.73419109,  79.52229109,  48.73419109,  70.56916234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         28.53411868,  79.52229109,  28.53411868,  42.85987234,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         42.85987234,  89.46916234,  42.85987234,  62.51229109,\n",
       "         70.56916234,  24.67980717,  42.85987234,  28.53411868,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         42.85987234,  24.67980717,  70.56916234,  28.53411868,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         79.52229109,  48.73419109,  79.52229109,  55.26016234,\n",
       "         62.51229109,  21.21187144,  37.57393009,  24.67980717,\n",
       "         15.28085117,  48.73419109,  15.28085117,  24.67980717,\n",
       "         37.57393009,  21.21187144,  62.51229109,  24.67980717,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         70.56916234,  55.26016234,  70.56916234,  62.51229109,\n",
       "         55.26016234,  18.08978465,  32.81563744,  21.21187144,\n",
       "         18.08978465,  55.26016234,  18.08978465,  28.53411868,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         24.67980717,  32.81563744,  24.67980717,  62.51229109,\n",
       "         62.51229109,  62.51229109,  62.51229109,  70.56916234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         21.21187144,  62.51229109,  21.21187144,  32.81563744,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         28.53411868,  37.57393009,  28.53411868,  70.56916234,\n",
       "         55.26016234,  70.56916234,  55.26016234,  79.52229109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         24.67980717,  70.56916234,  24.67980717,  37.57393009,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         24.67980717,  32.81563744,  24.67980717,  62.51229109,\n",
       "         48.73419109,  79.52229109,  48.73419109,  70.56916234,\n",
       "         62.51229109,  21.21187144,  37.57393009,  24.67980717,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         48.73419109,  28.53411868,  79.52229109,  32.81563744,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         70.56916234,  42.85987234,  89.46916234,  48.73419109,\n",
       "         55.26016234,  18.08978465,  32.81563744,  21.21187144,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         32.81563744,  18.08978465,  55.26016234,  21.21187144,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         62.51229109,  48.73419109,  62.51229109,  55.26016234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         15.28085117,  48.73419109,  15.28085117,  24.67980717,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         21.21187144,  28.53411868,  21.21187144,  55.26016234,\n",
       "         55.26016234,  55.26016234,  55.26016234,  62.51229109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         18.08978465,  55.26016234,  18.08978465,  28.53411868,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         32.81563744,  42.85987234,  32.81563744,  79.52229109,\n",
       "         48.73419109,  62.51229109,  48.73419109,  89.46916234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         21.21187144,  62.51229109,  21.21187144,  32.81563744,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         28.53411868,  37.57393009,  28.53411868,  70.56916234,\n",
       "         42.85987234,  70.56916234,  42.85987234,  79.52229109,\n",
       "         55.26016234,  18.08978465,  32.81563744,  21.21187144,\n",
       "          8.42815264,  32.81563744,   8.42815264,  15.28085117,\n",
       "         55.26016234,  32.81563744,  89.46916234,  37.57393009,\n",
       "         12.75186641,  18.08978465,  12.75186641,  37.57393009,\n",
       "         62.51229109,  37.57393009, 100.52229109,  42.85987234,\n",
       "         48.73419109,  15.28085117,  28.53411868,  18.08978465,\n",
       "         10.47672475,  37.57393009,  10.47672475,  18.08978465,\n",
       "         28.53411868,  15.28085117,  48.73419109,  18.08978465,\n",
       "         15.28085117,  21.21187144,  15.28085117,  42.85987234,\n",
       "         55.26016234,  42.85987234,  55.26016234,  48.73419109,\n",
       "         42.85987234,  12.75186641,  24.67980717,  15.28085117,\n",
       "         12.75186641,  42.85987234,  12.75186641,  21.21187144,\n",
       "         24.67980717,  12.75186641,  42.85987234,  15.28085117,\n",
       "         18.08978465,  24.67980717,  18.08978465,  48.73419109,\n",
       "         48.73419109,  48.73419109,  48.73419109,  55.26016234,\n",
       "         37.57393009,  10.47672475,  21.21187144,  12.75186641,\n",
       "         15.28085117,  48.73419109,  15.28085117,  24.67980717,\n",
       "         21.21187144,  10.47672475,  37.57393009,  12.75186641,\n",
       "         37.57393009,  48.73419109,  37.57393009,  89.46916234,\n",
       "         42.85987234,  55.26016234,  42.85987234, 100.52229109,\n",
       "         32.81563744,   8.42815264,  18.08978465,  10.47672475,\n",
       "         18.08978465,  55.26016234,  18.08978465,  28.53411868,\n",
       "         18.08978465,   8.42815264,  32.81563744,  10.47672475,\n",
       "         32.81563744,  42.85987234,  32.81563744,  79.52229109,\n",
       "         37.57393009,  62.51229109,  37.57393009,  89.46916234]),\n",
       " array([4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 5.,\n",
       "        0., 0., 0., 3., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 4.,\n",
       "        4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 5., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        2., 2., 2., 2., 0., 0., 0., 0., 2., 2., 2., 2., 1., 2., 0., 2., 1.,\n",
       "        1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3., 2., 2., 2., 2., 1., 2.,\n",
       "        3., 2., 3., 3., 3., 3., 1., 1., 1., 1., 3., 3., 3., 3., 2., 2., 2.,\n",
       "        2., 3., 1., 3., 2., 3., 3., 3., 3., 1., 1., 1., 1., 3., 3., 3., 3.,\n",
       "        0., 0., 0., 0., 3., 1., 3., 0., 3., 3., 3., 3., 1., 1., 1., 1., 3.,\n",
       "        3., 3., 3., 0., 0., 0., 0., 3., 1., 3., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        4., 4., 4., 4., 1., 1., 1., 1., 1., 1., 5., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 4., 4., 4., 4.,\n",
       "        1., 1., 1., 5., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3.,\n",
       "        3., 3., 3., 1., 1., 1., 3.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a9a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing & Recording trained agent ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_taxi_viter folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1: reward=11\n",
      "Test Episode 2: reward=4\n",
      "Test Episode 3: reward=7\n",
      "Test Episode 4: reward=7\n",
      "Test Episode 5: reward=12\n",
      "All 5 test episodes recorded in ./videos_taxi_viter folder\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "env_name = \"Taxi-v3\"\n",
    "print(\"\\n=== Testing & Recording trained agent ===\")\n",
    "video_env = RecordVideo(\n",
    "        gym.make(env_name, render_mode=\"rgb_array\"),\n",
    "        video_folder=\"./videos_taxi_viter\",\n",
    "        episode_trigger=lambda ep_id: True  # record every test episode\n",
    "    )\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = video_env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = int(policy[state])\n",
    "        new_state, reward, is_done, is_trunc, _ = video_env.step(action)\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "        if is_done or is_trunc:\n",
    "            break\n",
    "    print(f\"Test Episode {ep+1}: reward={total_reward}\")\n",
    "\n",
    "video_env.close()\n",
    "print(f\"All {num_episodes} test episodes recorded in ./videos_taxi_viter folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b2984",
   "metadata": {},
   "source": [
    "## Tabular Q Learning on the FrozenLake Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5198a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaaae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "EPSILON = 0.1\n",
    "TEST_EPISODES = 20\n",
    "VIDEO_FOLDER = \"videos_frozenlake_ql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc372d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = int\n",
    "Action = int\n",
    "ValuesKey = tt.Tuple[State, Action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310cadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.values: tt.Dict[ValuesKey, float] = defaultdict(float) \n",
    "\n",
    "    def sample_env(self) -> tt.Tuple[State, Action, float, State]:\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, is_trunc, _ = self.env.step(action)\n",
    "        if is_done or is_trunc:\n",
    "            self.state, _ = self.env.reset()\n",
    "        else:\n",
    "            self.state = new_state\n",
    "        return old_state, action, reward, new_state\n",
    "    \n",
    "    def best_value_and_action(self, state: State) -> tt.Tuple[float, Action]:\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        if best_action is None:\n",
    "            best_action = self.env.action_space.sample()\n",
    "            best_value = self.values[(state, best_action)]\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, state: State, action: Action, reward: float, next_state: State):\n",
    "        best_value, _ = self.best_value_and_action(next_state)\n",
    "        # Q learning update\n",
    "        self.values[(state, action)] = (1 - ALPHA) * self.values[(state, action)] + ALPHA * (reward + GAMMA * best_value)\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, is_trunc, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done or is_trunc:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbaf455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1083: Best reward updated: 0.000 -> 0.100\n",
      "1085: Best reward updated: 0.100 -> 0.150\n",
      "1086: Best reward updated: 0.150 -> 0.200\n",
      "1148: Best reward updated: 0.200 -> 0.250\n",
      "1642: Best reward updated: 0.250 -> 0.300\n",
      "1689: Best reward updated: 0.300 -> 0.350\n",
      "1959: Best reward updated: 0.350 -> 0.400\n",
      "3109: Best reward updated: 0.400 -> 0.550\n",
      "3136: Best reward updated: 0.550 -> 0.600\n",
      "3218: Best reward updated: 0.600 -> 0.700\n",
      "10457: Best reward updated: 0.700 -> 0.750\n",
      "14382: Best reward updated: 0.750 -> 0.800\n",
      "23592: Best reward updated: 0.800 -> 0.850\n",
      "Solved in 23592 iterations!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        state, action, reward, next_state = agent.sample_env()\n",
    "        agent.value_update(state, action, reward, next_state)\n",
    "\n",
    "        test_reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            test_reward += agent.play_episode(test_env)\n",
    "        test_reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", test_reward, iter_no)\n",
    "        if test_reward > best_reward:\n",
    "            print(f\"{iter_no}: Best reward updated: {best_reward:.3f} -> {test_reward:.3f}\")\n",
    "            best_reward = test_reward\n",
    "        if test_reward > 0.8:\n",
    "            print(f\"Solved in {iter_no} iterations!\")\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e97d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_agent_video(agent: Agent, env_name: str, video_path: str, num_episodes: int = 1):\n",
    "  \n",
    "    os.makedirs(video_path, exist_ok=True)\n",
    "\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(env, video_path)\n",
    "\n",
    "    print(f\"Recording {num_episodes} episode(s) to {video_path}...\")\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        while not done and not truncated:\n",
    "            _, action = agent.best_value_and_action(obs)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {i+1} finished with reward: {total_reward}\")\n",
    "    env.close()\n",
    "    print(\"Video recording complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456830f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete! Now recording a video of the trained agent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\User\\Desktop\\VS_Code_Projects\\Reinforcement_learning\\videos_frozenlake_ql folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording 5 episode(s) to videos_frozenlake_ql...\n",
      "Episode 1 finished with reward: 1.0\n",
      "Episode 2 finished with reward: 1.0\n",
      "Episode 3 finished with reward: 0.0\n",
      "Episode 4 finished with reward: 1.0\n",
      "Episode 5 finished with reward: 1.0\n",
      "Video recording complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining complete! Now recording a video of the trained agent.\")\n",
    "record_agent_video(agent, ENV_NAME, VIDEO_FOLDER, num_episodes=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a89ccc",
   "metadata": {},
   "source": [
    "## SARSA learning Algorithm on the Taxi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f50e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4082c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Taxi-v3\"\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "NUM_EPISODES = 10000\n",
    "MAX_STEPS = 100\n",
    "VIDEO_DIR = \"videos_taxi_sarsa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.n_states = self.env.observation_space.n\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.Q_table = np.zeros((self.n_states, self.n_actions))\n",
    "\n",
    "    def params(self):\n",
    "        return {\n",
    "            \"Q_table\": self.Q_table,\n",
    "            \"ALPHA\": ALPHA,\n",
    "            \"GAMMA\": GAMMA,\n",
    "            \"EPSILON\": EPSILON,\n",
    "            \"EPSILON_MIN\": EPSILON_MIN,\n",
    "            \"EPSILON_DECAY\": EPSILON_DECAY,\n",
    "            \"MAX_STEPS\": MAX_STEPS,\n",
    "            \"NUM_EPISODES\": NUM_EPISODES,\n",
    "            \"n_actions\": self.n_actions,\n",
    "            \"n_states\": self.n_states\n",
    "        }\n",
    "\n",
    "    def epsilon_greedy(self, state, epsilon, n_actions):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "\n",
    "    def sarsa_update(self, state, action, reward, next_state, next_action, alpha, gamma):\n",
    "        predict = self.Q_table[state, action]\n",
    "        target = reward + gamma * self.Q_table[next_state, next_action]\n",
    "        self.Q_table[state, action] += alpha * (target - predict) # SARSA update rule\n",
    "\n",
    "    def train_agent(self, params_):\n",
    "        epsilon = params_[\"EPSILON\"]\n",
    "        rewards = []\n",
    "\n",
    "        for episode in range(params_[\"NUM_EPISODES\"]):\n",
    "            state, _ = self.env.reset()\n",
    "            action = self.epsilon_greedy(state, epsilon, params_[\"n_actions\"])\n",
    "            total_reward = 0\n",
    "\n",
    "            for _ in range(params_[\"MAX_STEPS\"]):\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                next_action = self.epsilon_greedy(next_state, epsilon, params_[\"n_actions\"])\n",
    "\n",
    "                # SARSA update\n",
    "                self.sarsa_update(state, action, reward, next_state, next_action,\n",
    "                                  params_[\"ALPHA\"], params_[\"GAMMA\"])\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            # Decay epsilon\n",
    "            #epsilon = max(params_[\"EPSILON_MIN\"], epsilon * params_[\"EPSILON_DECAY\"])\n",
    "            if epsilon > params_[\"EPSILON_MIN\"]:\n",
    "                epsilon *= params_[\"EPSILON_DECAY\"]\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "            # Logging every 1000 episodes\n",
    "            if (episode + 1) % 1000 == 0:\n",
    "                avg_reward = np.mean(rewards[-1000:])\n",
    "                print(f\"Episode: {episode+1}, Avg Reward: {avg_reward:.3f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "        return self.Q_table, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c07c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_record(Q_table):\n",
    "\n",
    "    # Create environment with video recording\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")  # For recording\n",
    "    env = gym.wrappers.RecordVideo(env, VIDEO_DIR, episode_trigger=lambda e: True)\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(5):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Greedy policy (no exploration)\n",
    "            action = np.argmax(Q_table[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Videos saved in: {VIDEO_DIR}\")\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef17cd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Avg Reward: -124.175, Epsilon: 0.010\n",
      "Episode: 2000, Avg Reward: -2.405, Epsilon: 0.010\n",
      "Episode: 3000, Avg Reward: 6.792, Epsilon: 0.010\n",
      "Episode: 4000, Avg Reward: 7.424, Epsilon: 0.010\n",
      "Episode: 5000, Avg Reward: 7.555, Epsilon: 0.010\n",
      "Episode: 6000, Avg Reward: 7.376, Epsilon: 0.010\n",
      "Episode: 7000, Avg Reward: 7.196, Epsilon: 0.010\n",
      "Episode: 8000, Avg Reward: 7.420, Epsilon: 0.010\n",
      "Episode: 9000, Avg Reward: 7.182, Epsilon: 0.010\n",
      "Episode: 10000, Avg Reward: 7.449, Epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    S_agent = SarsaAgent()\n",
    "    params_ = S_agent.params()\n",
    "    Q_table, rewards = S_agent.train_agent(params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16d1e0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 12\n",
      "Episode 2: Total Reward = 4\n",
      "Episode 3: Total Reward = 7\n",
      "Episode 4: Total Reward = 9\n",
      "Episode 5: Total Reward = 8\n",
      "Videos saved in: videos_taxi_sarsa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12, 4, 7, 9, 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_record(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107844d",
   "metadata": {},
   "source": [
    "## A doctor agent using RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9751b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d89837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment (The Patient)\n",
    "class PatientEnvironment:\n",
    "    def __init__(self):\n",
    "        self.current_health_state = random.choice([0, 1, 2])\n",
    "        self.states = {0: \"Healthy\", 1: \"Sick\", 2: \"Critical\"}\n",
    "        self.actions = {0: \"Treatment A\", 1: \"Treatment B\"}\n",
    "        self.num_states = len(self.states)\n",
    "        self.num_actions = len(self.actions)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_health_state = random.choice([0, 1, 2])\n",
    "        return self.current_health_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        next_state = self.current_health_state\n",
    "\n",
    "        # Simplified state transition and reward logic\n",
    "        if self.current_health_state == 0: # Healthy\n",
    "            if action == 0: # Treatment A (maintain health)\n",
    "                reward = 1\n",
    "                next_state = 0\n",
    "            else: # Treatment B (mild negative effect on healthy patient)\n",
    "                reward = -0.5\n",
    "                next_state = 1 # Becomes sick\n",
    "        elif self.current_health_state == 1: # Sick\n",
    "            if action == 0: # Treatment A (effective for sick)\n",
    "                reward = 2\n",
    "                next_state = 0 # Becomes healthy\n",
    "            else: # Treatment B (less effective for sick)\n",
    "                reward = 0\n",
    "                next_state = 1 # Stays sick\n",
    "        elif self.current_health_state == 2: # Critical\n",
    "            if action == 0: # Treatment A (might save, but risky)\n",
    "                if random.random() < 0.7: # 70% chance of recovery\n",
    "                    reward = 5\n",
    "                    next_state = 1 # Becomes sick (better than critical)\n",
    "                else: # 30% chance of failure\n",
    "                    reward = -10\n",
    "                    next_state = 2 # Stays critical (or worse, not modelled here)\n",
    "            else: # Treatment B (ineffective for critical)\n",
    "                reward = -2\n",
    "                next_state = 2 # Stays critical or worsens\n",
    "\n",
    "        # If patient becomes healthy, the \"episode\"  ends\n",
    "        if next_state == 0:\n",
    "            done = True\n",
    "\n",
    "        self.current_health_state = next_state\n",
    "        return next_state, reward, done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e765cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RL Agent (Q-learning)\n",
    "class DoctorAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.q_table = np.zeros((env.num_states, env.num_actions))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0 # Exploration-exploitation trade-off\n",
    "        self.epsilon_decay_rate = 0.001\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Epsilon-greedy strategy\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.env.num_actions - 1) # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :]) # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Q-learning formula\n",
    "        old_value = self.q_table[state, action]\n",
    "        next_max = np.max(self.q_table[next_state, :])\n",
    "\n",
    "        new_value = old_value + self.learning_rate * (reward + self.discount_factor * next_max - old_value)\n",
    "        self.q_table[state, action] = new_value\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb38662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n",
      "Episode 100: Total Reward = 1.00, Epsilon = 0.90\n",
      "Episode 200: Total Reward = 2.00, Epsilon = 0.80\n",
      "Episode 300: Total Reward = 2.00, Epsilon = 0.70\n",
      "Episode 400: Total Reward = 2.00, Epsilon = 0.60\n",
      "Episode 500: Total Reward = 2.00, Epsilon = 0.50\n",
      "Episode 600: Total Reward = 2.00, Epsilon = 0.40\n",
      "Episode 700: Total Reward = 7.00, Epsilon = 0.30\n",
      "Episode 800: Total Reward = -3.00, Epsilon = 0.20\n",
      "Episode 900: Total Reward = 2.00, Epsilon = 0.10\n",
      "Episode 1000: Total Reward = 7.00, Epsilon = 0.01\n",
      "Episode 1100: Total Reward = 7.00, Epsilon = 0.01\n",
      "Episode 1200: Total Reward = 1.50, Epsilon = 0.01\n",
      "Episode 1300: Total Reward = 7.00, Epsilon = 0.01\n",
      "Episode 1400: Total Reward = 2.00, Epsilon = 0.01\n",
      "Episode 1500: Total Reward = 2.00, Epsilon = 0.01\n",
      "Episode 1600: Total Reward = 2.00, Epsilon = 0.01\n",
      "Episode 1700: Total Reward = 7.00, Epsilon = 0.01\n",
      "Episode 1800: Total Reward = 2.00, Epsilon = 0.01\n",
      "Episode 1900: Total Reward = 1.50, Epsilon = 0.01\n",
      "Episode 2000: Total Reward = 1.50, Epsilon = 0.01\n",
      "\n",
      "--- Training Finished ---\n",
      "\n",
      "Final Q-table:\n",
      "[[16.1523329  44.53170758]\n",
      " [45.93668736 23.31276814]\n",
      " [45.99594979 19.84704866]]\n",
      "\n",
      "--- Testing the trained agent (10 episodes) ---\n",
      "\n",
      "New Patient (Initial State: Sick)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 2.00\n",
      "\n",
      "New Patient (Initial State: Sick)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 2.00\n",
      "\n",
      "New Patient (Initial State: Sick)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 2.00\n",
      "\n",
      "New Patient (Initial State: Healthy)\n",
      "  Agent chooses: Treatment B (Current State: Healthy)\n",
      "  Patient is now: Sick (Reward: -0.5)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 1.50\n",
      "\n",
      "New Patient (Initial State: Sick)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 2.00\n",
      "\n",
      "New Patient (Initial State: Healthy)\n",
      "  Agent chooses: Treatment B (Current State: Healthy)\n",
      "  Patient is now: Sick (Reward: -0.5)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 1.50\n",
      "\n",
      "New Patient (Initial State: Critical)\n",
      "  Agent chooses: Treatment A (Current State: Critical)\n",
      "  Patient is now: Sick (Reward: 5)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 7.00\n",
      "\n",
      "New Patient (Initial State: Healthy)\n",
      "  Agent chooses: Treatment B (Current State: Healthy)\n",
      "  Patient is now: Sick (Reward: -0.5)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 1.50\n",
      "\n",
      "New Patient (Initial State: Critical)\n",
      "  Agent chooses: Treatment A (Current State: Critical)\n",
      "  Patient is now: Critical (Reward: -10)\n",
      "  Agent chooses: Treatment A (Current State: Critical)\n",
      "  Patient is now: Sick (Reward: 5)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: -3.00\n",
      "\n",
      "New Patient (Initial State: Sick)\n",
      "  Agent chooses: Treatment A (Current State: Sick)\n",
      "  Patient is now: Healthy (Reward: 2). Episode Finished.\n",
      "  Total Test Reward for this patient: 2.00\n",
      "\n",
      "Average Test Reward over 10 episodes: 1.85\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "if __name__ == \"__main__\":\n",
    "    env = PatientEnvironment()\n",
    "    agent = DoctorAgent(env)\n",
    "\n",
    "    num_episodes = 2000\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    print(\"--- Starting Training ---\")\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset() # Reset patient for a new episode\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward:.2f}, Epsilon = {agent.epsilon:.2f}\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "    print(\"\\nFinal Q-table:\")\n",
    "    print(agent.q_table)\n",
    "\n",
    "    print(\"\\n--- Testing the trained agent (10 episodes) ---\")\n",
    "    total_test_rewards = 0\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        test_reward = 0\n",
    "        print(f\"\\nNew Patient (Initial State: {env.states[state]})\")\n",
    "        while not done:\n",
    "            action = np.argmax(agent.q_table[state, :]) # Agent acts purely based on learned Q-values\n",
    "            print(f\"  Agent chooses: {env.actions[action]} (Current State: {env.states[state]})\")\n",
    "            next_state, reward, done = env.step(action)\n",
    "            test_reward += reward\n",
    "            state = next_state\n",
    "            if not done:\n",
    "                print(f\"  Patient is now: {env.states[state]} (Reward: {reward})\")\n",
    "            else:\n",
    "                print(f\"  Patient is now: {env.states[state]} (Reward: {reward}). Episode Finished.\")\n",
    "        total_test_rewards += test_reward\n",
    "        print(f\"  Total Test Reward for this patient: {test_reward:.2f}\")\n",
    "\n",
    "    print(f\"\\nAverage Test Reward over 10 episodes: {total_test_rewards / 10:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef6a968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 617\n",
      "Extra info: {'admissible_actions': [5, 10, 15, 20], 'state_vector': array([-0.25      , -0.08333333, -0.43855   ,  0.33333333, -0.43708271,\n",
      "        0.11068928,  0.42393451,  0.15369781, -0.26537177, -0.28637706,\n",
      "       -0.08265458,  0.31503245, -0.06588095,  0.08514915,  1.33252377,\n",
      "       -0.36103496, -0.28066148, -0.16137368,  1.82694543, -0.58913025,\n",
      "       -0.48484812,  0.09477288, -0.54081085,  0.02363826,  0.15988011,\n",
      "       -0.42123487, -0.23597269, -0.8504507 , -1.13166612, -1.21457448,\n",
      "        0.38335443,  0.91633856,  0.88553353,  0.27268052, -0.29112997,\n",
      "        0.52982805, -0.30634874,  1.86515795,  2.24978588,  0.75969555,\n",
      "        0.74538692,  0.70060293,  0.26431062,  0.37329322,  1.27071747,\n",
      "        0.09261225, -0.01282182]), 'sofa_score': np.float64(9.31060606060606)}\n",
      "\n",
      "Taking action 0:\n",
      "Next state: 617\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Truncated: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Quickstart example for the ICU-Sepsis environment.\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import icu_sepsis\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make('Sepsis/ICU-Sepsis-v2')\n",
    "\n",
    "    state, info = env.reset()\n",
    "    print('Initial state:', state)\n",
    "    print('Extra info:', info)\n",
    "\n",
    "    next_state, reward, terminated, truncated, info = env.step(0)\n",
    "    print('\\nTaking action 0:')\n",
    "    print('Next state:', next_state)\n",
    "    print('Reward:', reward)\n",
    "    print('Terminated:', terminated)\n",
    "    print('Truncated:', truncated)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8e8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('Blackjack-v1', natural=False, sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94525430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a6a3cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(32), Discrete(11), Discrete(2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5e68b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1), np.int64(1), np.int64(0))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c85f4",
   "metadata": {},
   "source": [
    "## First Visit Monte Carlo Prediction on Blackjack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa42e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b09cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c4ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy: Stick if player sum >= 20, else hit\n",
    "def policy(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    return 0 if player_sum >= 20 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1665bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_prediction(env, ploicy, num_episodes=500000, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Estimates the Value function for a given policy using First-Visit Monte Carlo \n",
    "    prediction algorithm.A prediction algorithm is a method used to estimate the \n",
    "    value of a policy ie how good it is to follow a given policy.\n",
    "    \"\"\"\n",
    "    returns_sum = defaultdict(float) # Sum of returns for each state\n",
    "    returns_count = defaultdict(float) # Number of first visits\n",
    "    V = defaultdict(float) # State value estimates\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate a sequence of a 3 tuple (state, action, reward) for an episode\n",
    "        episode_data = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_data.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Compute returns\n",
    "        G = 0.0\n",
    "        visited_states = set()\n",
    "        for t in reversed(range(len(episode_data))):\n",
    "            state_t, action_t, reward_t = episode_data[t]\n",
    "            G = gamma * G + reward_t \n",
    "         # First visit check\n",
    "            if state_t not in visited_states:\n",
    "                visited_states.add(state_t)\n",
    "                returns_sum[state_t] += G\n",
    "                returns_count[state_t] += 1.0\n",
    "                V[state_t] = returns_sum[state_t] / returns_count[state_t]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d212b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = first_visit_mc_prediction(env, policy, num_episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a832e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {(18, 10, 0): -0.23957951350690623,\n",
       "             (12, 10, 0): -0.5756250455572564,\n",
       "             (15, 7, 0): -0.5045871559633027,\n",
       "             (19, 5, 1): 0.4519650655021834,\n",
       "             (17, 5, 1): -0.06858407079646017,\n",
       "             (17, 7, 0): -0.10025817555938038,\n",
       "             (16, 10, 0): -0.5816532258064516,\n",
       "             (21, 4, 1): 1.0,\n",
       "             (11, 10, 0): -0.5868312757201646,\n",
       "             (21, 5, 1): 1.0,\n",
       "             (8, 8, 0): -0.5387323943661971,\n",
       "             (18, 5, 0): 0.23323615160349853,\n",
       "             (12, 1, 0): -0.7807172251616696,\n",
       "             (17, 2, 0): -0.15893760539629004,\n",
       "             (18, 4, 0): 0.19738751814223512,\n",
       "             (5, 10, 0): -0.5754251234229293,\n",
       "             (14, 10, 0): -0.569496110923233,\n",
       "             (17, 3, 0): -0.12164009111617312,\n",
       "             (10, 5, 0): -0.13288426209430496,\n",
       "             (13, 1, 0): -0.7828886844526219,\n",
       "             (7, 2, 0): -0.3005464480874317,\n",
       "             (20, 5, 0): 0.6616666666666666,\n",
       "             (16, 4, 0): -0.17647058823529413,\n",
       "             (20, 8, 0): 0.7799084144247281,\n",
       "             (17, 4, 0): -0.10076857386848848,\n",
       "             (14, 8, 0): -0.5403899721448467,\n",
       "             (14, 2, 0): -0.30850700375811413,\n",
       "             (19, 2, 0): 0.4032520325203252,\n",
       "             (13, 8, 0): -0.48429951690821255,\n",
       "             (15, 10, 0): -0.5758178203506236,\n",
       "             (8, 9, 0): -0.5408631772268135,\n",
       "             (7, 10, 0): -0.576996456800218,\n",
       "             (13, 10, 0): -0.5914080391692332,\n",
       "             (17, 1, 1): -0.6407982261640798,\n",
       "             (12, 2, 0): -0.2669345579793341,\n",
       "             (8, 6, 0): -0.12780656303972365,\n",
       "             (21, 3, 1): 1.0,\n",
       "             (19, 3, 0): 0.400679117147708,\n",
       "             (13, 3, 0): -0.2552166934189406,\n",
       "             (20, 10, 1): 0.44642857142857145,\n",
       "             (13, 9, 0): -0.5417451349654739,\n",
       "             (16, 5, 0): -0.161186848436247,\n",
       "             (19, 10, 0): -0.01115702479338843,\n",
       "             (13, 2, 0): -0.2926239419588875,\n",
       "             (12, 8, 0): -0.5163283318623124,\n",
       "             (20, 3, 0): 0.6502302898943376,\n",
       "             (21, 10, 1): 0.9241454446411549,\n",
       "             (12, 9, 0): -0.5333139365725924,\n",
       "             (14, 4, 0): -0.22741220593249234,\n",
       "             (17, 5, 0): -0.05002174858634189,\n",
       "             (8, 4, 0): -0.20614828209764918,\n",
       "             (19, 7, 0): 0.6322473771397018,\n",
       "             (17, 1, 0): -0.6550552251486831,\n",
       "             (14, 9, 0): -0.5391003460207613,\n",
       "             (16, 6, 0): -0.14912637139374238,\n",
       "             (16, 2, 0): -0.2831299957930164,\n",
       "             (18, 3, 1): 0.19230769230769232,\n",
       "             (8, 10, 0): -0.5800696257615318,\n",
       "             (4, 10, 0): -0.6222222222222222,\n",
       "             (19, 6, 0): 0.4841653247450349,\n",
       "             (20, 10, 0): 0.43116758898681423,\n",
       "             (13, 7, 0): -0.46352064576218566,\n",
       "             (15, 2, 0): -0.2696096315213426,\n",
       "             (16, 3, 0): -0.2285132382892057,\n",
       "             (11, 1, 0): -0.7721382289416847,\n",
       "             (9, 9, 0): -0.5406844106463878,\n",
       "             (20, 2, 0): 0.6327361563517915,\n",
       "             (14, 5, 0): -0.18361866398120175,\n",
       "             (11, 5, 0): -0.21448162369720242,\n",
       "             (20, 7, 0): 0.7680965147453083,\n",
       "             (15, 6, 0): -0.12443778110944528,\n",
       "             (18, 7, 0): 0.38269794721407624,\n",
       "             (17, 10, 0): -0.4580427201394943,\n",
       "             (7, 8, 0): -0.5453551912568306,\n",
       "             (15, 8, 0): -0.5060931899641578,\n",
       "             (14, 4, 1): -0.1598173515981735,\n",
       "             (17, 9, 0): -0.40956521739130436,\n",
       "             (10, 10, 0): -0.5851242188690748,\n",
       "             (10, 8, 0): -0.5119496855345912,\n",
       "             (19, 7, 1): 0.5737373737373738,\n",
       "             (6, 9, 0): -0.5271084337349398,\n",
       "             (15, 5, 1): -0.12217194570135746,\n",
       "             (12, 4, 0): -0.2060979184989739,\n",
       "             (20, 6, 0): 0.7070514565750068,\n",
       "             (17, 8, 0): -0.37583296312749886,\n",
       "             (12, 3, 0): -0.26444833625218916,\n",
       "             (18, 6, 1): 0.25635103926096997,\n",
       "             (9, 1, 0): -0.7486752460257381,\n",
       "             (8, 2, 0): -0.28484848484848485,\n",
       "             (18, 6, 0): 0.2834567901234568,\n",
       "             (14, 3, 1): -0.22580645161290322,\n",
       "             (16, 1, 0): -0.7748513169073916,\n",
       "             (17, 10, 1): -0.49224806201550386,\n",
       "             (12, 6, 0): -0.17706778142729174,\n",
       "             (19, 1, 1): -0.128,\n",
       "             (16, 10, 1): -0.5759007678676905,\n",
       "             (13, 4, 0): -0.2071156289707751,\n",
       "             (14, 5, 1): -0.15030060120240482,\n",
       "             (15, 4, 1): -0.16923076923076924,\n",
       "             (21, 9, 1): 1.0,\n",
       "             (20, 4, 0): 0.676956404007582,\n",
       "             (15, 5, 0): -0.180741306839893,\n",
       "             (9, 6, 0): -0.1966205837173579,\n",
       "             (9, 10, 0): -0.5842134519177321,\n",
       "             (10, 1, 0): -0.7690387016229713,\n",
       "             (10, 4, 0): -0.21876904326630103,\n",
       "             (14, 1, 0): -0.7519224339685724,\n",
       "             (8, 3, 0): -0.22711267605633803,\n",
       "             (16, 9, 0): -0.5675675675675675,\n",
       "             (14, 7, 0): -0.49204152249134947,\n",
       "             (8, 7, 0): -0.46593001841620624,\n",
       "             (13, 6, 0): -0.10789149198520345,\n",
       "             (15, 3, 0): -0.25101965146459027,\n",
       "             (6, 8, 0): -0.5500770416024653,\n",
       "             (18, 3, 0): 0.12582781456953643,\n",
       "             (16, 7, 0): -0.4432234432234432,\n",
       "             (19, 8, 0): 0.5932297447280799,\n",
       "             (7, 7, 0): -0.43923240938166314,\n",
       "             (19, 5, 0): 0.4135135135135135,\n",
       "             (19, 4, 1): 0.40380047505938244,\n",
       "             (14, 3, 0): -0.250936329588015,\n",
       "             (18, 10, 1): -0.2658157602663707,\n",
       "             (7, 1, 0): -0.7847439916405433,\n",
       "             (11, 6, 0): -0.15276273022751896,\n",
       "             (6, 5, 0): -0.2023121387283237,\n",
       "             (19, 1, 0): -0.08568269762299613,\n",
       "             (20, 9, 0): 0.7644616662053695,\n",
       "             (21, 1, 1): 0.6856060606060606,\n",
       "             (19, 9, 0): 0.2970027247956403,\n",
       "             (15, 1, 0): -0.781317885590152,\n",
       "             (20, 5, 1): 0.7142857142857143,\n",
       "             (20, 1, 0): 0.14575235980011106,\n",
       "             (21, 8, 1): 1.0,\n",
       "             (21, 7, 1): 1.0,\n",
       "             (15, 10, 1): -0.5817575083426029,\n",
       "             (19, 4, 0): 0.4281115879828326,\n",
       "             (16, 8, 1): -0.4930232558139535,\n",
       "             (15, 6, 1): -0.21797752808988763,\n",
       "             (18, 1, 1): -0.34513274336283184,\n",
       "             (9, 3, 0): -0.29013678905687545,\n",
       "             (18, 8, 0): 0.10523715415019763,\n",
       "             (13, 10, 1): -0.5441412520064205,\n",
       "             (12, 7, 0): -0.4997067448680352,\n",
       "             (16, 8, 0): -0.5085714285714286,\n",
       "             (13, 4, 1): -0.13978494623655913,\n",
       "             (7, 4, 0): -0.23094170403587444,\n",
       "             (18, 9, 0): -0.17088014981273408,\n",
       "             (14, 8, 1): -0.5727272727272728,\n",
       "             (21, 6, 1): 1.0,\n",
       "             (10, 7, 0): -0.4691358024691358,\n",
       "             (12, 9, 1): -0.5317073170731708,\n",
       "             (12, 5, 0): -0.14775804995486005,\n",
       "             (12, 1, 1): -0.7990430622009569,\n",
       "             (16, 5, 1): -0.08,\n",
       "             (9, 4, 0): -0.1837026447462473,\n",
       "             (18, 4, 1): 0.16488222698072805,\n",
       "             (18, 1, 0): -0.3768531802965088,\n",
       "             (16, 2, 1): -0.29411764705882354,\n",
       "             (17, 6, 1): 0.045576407506702415,\n",
       "             (6, 3, 0): -0.23303834808259588,\n",
       "             (4, 7, 0): -0.4344262295081967,\n",
       "             (7, 5, 0): -0.18201284796573874,\n",
       "             (19, 10, 1): 0.016921397379912665,\n",
       "             (17, 7, 1): -0.125,\n",
       "             (18, 5, 1): 0.23373983739837398,\n",
       "             (10, 3, 0): -0.2644776119402985,\n",
       "             (15, 9, 0): -0.5711077844311377,\n",
       "             (14, 6, 0): -0.131939908556499,\n",
       "             (14, 2, 1): -0.22869955156950672,\n",
       "             (10, 9, 0): -0.5511221945137157,\n",
       "             (19, 2, 1): 0.39337474120082816,\n",
       "             (5, 3, 0): -0.2108843537414966,\n",
       "             (9, 8, 0): -0.5230660042583393,\n",
       "             (10, 6, 0): -0.1792573623559539,\n",
       "             (17, 6, 0): -0.01263537906137184,\n",
       "             (9, 7, 0): -0.4281567489114659,\n",
       "             (20, 1, 1): 0.16775599128540306,\n",
       "             (4, 4, 0): -0.0821256038647343,\n",
       "             (9, 2, 0): -0.2740419378163413,\n",
       "             (13, 1, 1): -0.803680981595092,\n",
       "             (6, 10, 0): -0.546437896307348,\n",
       "             (11, 8, 0): -0.47586980920314254,\n",
       "             (13, 5, 0): -0.15088383838383837,\n",
       "             (15, 3, 1): -0.1702127659574468,\n",
       "             (11, 9, 0): -0.5307388606880993,\n",
       "             (11, 3, 0): -0.24820342730790493,\n",
       "             (8, 1, 0): -0.7764606265876376,\n",
       "             (19, 6, 1): 0.5251641137855579,\n",
       "             (11, 7, 0): -0.4807799442896936,\n",
       "             (16, 1, 1): -0.7787234042553192,\n",
       "             (15, 7, 1): -0.43037974683544306,\n",
       "             (18, 8, 1): 0.08450704225352113,\n",
       "             (17, 3, 1): -0.1372093023255814,\n",
       "             (11, 2, 0): -0.2701313535122787,\n",
       "             (20, 8, 1): 0.7931726907630522,\n",
       "             (16, 9, 1): -0.5688073394495413,\n",
       "             (5, 4, 0): -0.26200873362445415,\n",
       "             (18, 2, 0): 0.09420289855072464,\n",
       "             (7, 9, 0): -0.525974025974026,\n",
       "             (15, 4, 0): -0.2139231327048586,\n",
       "             (14, 10, 1): -0.5439591605218378,\n",
       "             (19, 8, 1): 0.5950782997762863,\n",
       "             (5, 1, 0): -0.7886178861788617,\n",
       "             (6, 4, 0): -0.14825581395348839,\n",
       "             (14, 6, 1): -0.19362186788154898,\n",
       "             (20, 7, 1): 0.7813765182186235,\n",
       "             (5, 2, 0): -0.24890829694323144,\n",
       "             (19, 3, 1): 0.45701357466063347,\n",
       "             (17, 8, 1): -0.3882863340563991,\n",
       "             (12, 5, 1): -0.02912621359223301,\n",
       "             (4, 6, 0): -0.16862745098039217,\n",
       "             (9, 5, 0): -0.17062314540059348,\n",
       "             (20, 9, 1): 0.8048780487804879,\n",
       "             (16, 7, 1): -0.43891402714932126,\n",
       "             (4, 1, 0): -0.7714285714285715,\n",
       "             (8, 5, 0): -0.12852112676056338,\n",
       "             (4, 9, 0): -0.646551724137931,\n",
       "             (5, 5, 0): -0.1457905544147844,\n",
       "             (13, 9, 1): -0.5767441860465117,\n",
       "             (12, 4, 1): -0.21495327102803738,\n",
       "             (13, 7, 1): -0.5075593952483801,\n",
       "             (21, 2, 1): 1.0,\n",
       "             (6, 7, 0): -0.4732272069464544,\n",
       "             (4, 5, 0): -0.1943127962085308,\n",
       "             (10, 2, 0): -0.32867132867132864,\n",
       "             (15, 2, 1): -0.32340425531914896,\n",
       "             (11, 4, 0): -0.2182410423452769,\n",
       "             (7, 6, 0): -0.125,\n",
       "             (17, 2, 1): -0.1655328798185941,\n",
       "             (12, 3, 1): -0.19491525423728814,\n",
       "             (5, 7, 0): -0.47368421052631576,\n",
       "             (5, 8, 0): -0.4716981132075472,\n",
       "             (16, 3, 1): -0.24242424242424243,\n",
       "             (12, 8, 1): -0.5663716814159292,\n",
       "             (19, 9, 1): 0.26683291770573564,\n",
       "             (13, 6, 1): -0.25,\n",
       "             (16, 4, 1): -0.13733905579399142,\n",
       "             (5, 6, 0): -0.15789473684210525,\n",
       "             (15, 1, 1): -0.738359201773836,\n",
       "             (6, 1, 0): -0.7673048600883653,\n",
       "             (18, 2, 1): 0.2017167381974249,\n",
       "             (14, 9, 1): -0.5441176470588235,\n",
       "             (17, 9, 1): -0.42660550458715596,\n",
       "             (4, 2, 0): -0.36283185840707965,\n",
       "             (12, 10, 1): -0.5760266370699223,\n",
       "             (6, 6, 0): -0.19806763285024154,\n",
       "             (18, 7, 1): 0.41025641025641024,\n",
       "             (15, 9, 1): -0.5296523517382413,\n",
       "             (16, 6, 1): -0.11447084233261338,\n",
       "             (17, 4, 1): -0.12678936605316973,\n",
       "             (20, 6, 1): 0.7433628318584071,\n",
       "             (7, 3, 0): -0.255863539445629,\n",
       "             (13, 2, 1): -0.3006134969325153,\n",
       "             (18, 9, 1): -0.15207373271889402,\n",
       "             (13, 5, 1): -0.16083916083916083,\n",
       "             (12, 6, 1): -0.07476635514018691,\n",
       "             (15, 8, 1): -0.5048543689320388,\n",
       "             (20, 4, 1): 0.6266666666666667,\n",
       "             (20, 2, 1): 0.6624472573839663,\n",
       "             (14, 1, 1): -0.7357723577235772,\n",
       "             (4, 8, 0): -0.5,\n",
       "             (13, 3, 1): -0.2582781456953642,\n",
       "             (6, 2, 0): -0.2948328267477204,\n",
       "             (5, 9, 0): -0.5192743764172335,\n",
       "             (12, 2, 1): -0.2,\n",
       "             (4, 3, 0): -0.18781725888324874,\n",
       "             (14, 7, 1): -0.4734607218683652,\n",
       "             (13, 8, 1): -0.5187637969094923,\n",
       "             (12, 7, 1): -0.4403292181069959,\n",
       "             (20, 3, 1): 0.6263269639065817})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V # The value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0177debb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19062a33",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation on FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbad7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce71e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(env, pi, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"Computes the value of the function for a given policy pi\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    V = np.zeros(n_states)\n",
    "    env_unwrapped = env.unwrapped\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(pi[s]):\n",
    "                for prob, next_state, reward, terminated in env_unwrapped.P[s][a]:\n",
    "                    v += action_prob * prob * (reward + gamma * V[next_state]) # Update V using the Bellman equation\n",
    "\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e89744",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"FrozenLake-v1\")\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "    V = iterative_policy_evaluation(env, pi=policy, gamma=0.99, theta=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcfc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01235611, 0.01042444, 0.01933842, 0.00947774],\n",
       "       [0.01478704, 0.        , 0.03889445, 0.        ],\n",
       "       [0.03260247, 0.08433764, 0.13781085, 0.        ],\n",
       "       [0.        , 0.17034482, 0.43357944, 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.reshape(4, 4) # The estimated value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d2179",
   "metadata": {},
   "source": [
    "## Q learning on Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa29222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539e9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent_Blackjack:\n",
    "    def __init__(\n",
    "        self, \n",
    "        env, \n",
    "        alpha: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        gamma: float = 0.95):\n",
    "        \n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, env, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"Epsilon Greedy policy\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "        \n",
    "    def update(self,\n",
    "               obs: tuple[int, int, bool],\n",
    "               action: int,\n",
    "               reward: float,\n",
    "               terminated: bool,\n",
    "               next_obs: tuple[int, int, bool]):\n",
    "        self.q_values[obs, action] = (((1 - self.alpha) * self.q_values[obs, action]) + \n",
    "                                      self.alpha * (reward + self.gamma * (not terminated) * np.max(self.q_values[next_obs])))\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)     \n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054ce4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.1\n",
    "n_episodes = 1000000\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (n_episodes / 2)\n",
    "final_epsilon = 0.1\n",
    "video_dir = \"videos_blackjack_ql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a26afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "Q_agent = QAgent_Blackjack(env=env, alpha=alpha,\n",
    "                           initial_epsilon=initial_epsilon,\n",
    "                           epsilon_decay=epsilon_decay,\n",
    "                           final_epsilon=final_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042e78ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Avg Reward: -0.379, Epsilon: 0.998\n",
      "Episode: 2000, Avg Reward: -0.396, Epsilon: 0.996\n",
      "Episode: 3000, Avg Reward: -0.370, Epsilon: 0.994\n",
      "Episode: 4000, Avg Reward: -0.354, Epsilon: 0.992\n",
      "Episode: 5000, Avg Reward: -0.404, Epsilon: 0.990\n",
      "Episode: 6000, Avg Reward: -0.372, Epsilon: 0.988\n",
      "Episode: 7000, Avg Reward: -0.421, Epsilon: 0.986\n",
      "Episode: 8000, Avg Reward: -0.428, Epsilon: 0.984\n",
      "Episode: 9000, Avg Reward: -0.317, Epsilon: 0.982\n",
      "Episode: 10000, Avg Reward: -0.437, Epsilon: 0.980\n",
      "Episode: 11000, Avg Reward: -0.343, Epsilon: 0.978\n",
      "Episode: 12000, Avg Reward: -0.392, Epsilon: 0.976\n",
      "Episode: 13000, Avg Reward: -0.419, Epsilon: 0.974\n",
      "Episode: 14000, Avg Reward: -0.353, Epsilon: 0.972\n",
      "Episode: 15000, Avg Reward: -0.413, Epsilon: 0.970\n",
      "Episode: 16000, Avg Reward: -0.366, Epsilon: 0.968\n",
      "Episode: 17000, Avg Reward: -0.352, Epsilon: 0.966\n",
      "Episode: 18000, Avg Reward: -0.340, Epsilon: 0.964\n",
      "Episode: 19000, Avg Reward: -0.422, Epsilon: 0.962\n",
      "Episode: 20000, Avg Reward: -0.408, Epsilon: 0.960\n",
      "Episode: 21000, Avg Reward: -0.382, Epsilon: 0.958\n",
      "Episode: 22000, Avg Reward: -0.400, Epsilon: 0.956\n",
      "Episode: 23000, Avg Reward: -0.397, Epsilon: 0.954\n",
      "Episode: 24000, Avg Reward: -0.373, Epsilon: 0.952\n",
      "Episode: 25000, Avg Reward: -0.398, Epsilon: 0.950\n",
      "Episode: 26000, Avg Reward: -0.332, Epsilon: 0.948\n",
      "Episode: 27000, Avg Reward: -0.359, Epsilon: 0.946\n",
      "Episode: 28000, Avg Reward: -0.390, Epsilon: 0.944\n",
      "Episode: 29000, Avg Reward: -0.388, Epsilon: 0.942\n",
      "Episode: 30000, Avg Reward: -0.446, Epsilon: 0.940\n",
      "Episode: 31000, Avg Reward: -0.358, Epsilon: 0.938\n",
      "Episode: 32000, Avg Reward: -0.375, Epsilon: 0.936\n",
      "Episode: 33000, Avg Reward: -0.387, Epsilon: 0.934\n",
      "Episode: 34000, Avg Reward: -0.359, Epsilon: 0.932\n",
      "Episode: 35000, Avg Reward: -0.348, Epsilon: 0.930\n",
      "Episode: 36000, Avg Reward: -0.364, Epsilon: 0.928\n",
      "Episode: 37000, Avg Reward: -0.351, Epsilon: 0.926\n",
      "Episode: 38000, Avg Reward: -0.404, Epsilon: 0.924\n",
      "Episode: 39000, Avg Reward: -0.409, Epsilon: 0.922\n",
      "Episode: 40000, Avg Reward: -0.375, Epsilon: 0.920\n",
      "Episode: 41000, Avg Reward: -0.378, Epsilon: 0.918\n",
      "Episode: 42000, Avg Reward: -0.347, Epsilon: 0.916\n",
      "Episode: 43000, Avg Reward: -0.329, Epsilon: 0.914\n",
      "Episode: 44000, Avg Reward: -0.417, Epsilon: 0.912\n",
      "Episode: 45000, Avg Reward: -0.331, Epsilon: 0.910\n",
      "Episode: 46000, Avg Reward: -0.377, Epsilon: 0.908\n",
      "Episode: 47000, Avg Reward: -0.369, Epsilon: 0.906\n",
      "Episode: 48000, Avg Reward: -0.435, Epsilon: 0.904\n",
      "Episode: 49000, Avg Reward: -0.392, Epsilon: 0.902\n",
      "Episode: 50000, Avg Reward: -0.380, Epsilon: 0.900\n",
      "Episode: 51000, Avg Reward: -0.346, Epsilon: 0.898\n",
      "Episode: 52000, Avg Reward: -0.348, Epsilon: 0.896\n",
      "Episode: 53000, Avg Reward: -0.367, Epsilon: 0.894\n",
      "Episode: 54000, Avg Reward: -0.362, Epsilon: 0.892\n",
      "Episode: 55000, Avg Reward: -0.361, Epsilon: 0.890\n",
      "Episode: 56000, Avg Reward: -0.330, Epsilon: 0.888\n",
      "Episode: 57000, Avg Reward: -0.398, Epsilon: 0.886\n",
      "Episode: 58000, Avg Reward: -0.380, Epsilon: 0.884\n",
      "Episode: 59000, Avg Reward: -0.403, Epsilon: 0.882\n",
      "Episode: 60000, Avg Reward: -0.268, Epsilon: 0.880\n",
      "Episode: 61000, Avg Reward: -0.316, Epsilon: 0.878\n",
      "Episode: 62000, Avg Reward: -0.329, Epsilon: 0.876\n",
      "Episode: 63000, Avg Reward: -0.395, Epsilon: 0.874\n",
      "Episode: 64000, Avg Reward: -0.309, Epsilon: 0.872\n",
      "Episode: 65000, Avg Reward: -0.386, Epsilon: 0.870\n",
      "Episode: 66000, Avg Reward: -0.351, Epsilon: 0.868\n",
      "Episode: 67000, Avg Reward: -0.382, Epsilon: 0.866\n",
      "Episode: 68000, Avg Reward: -0.350, Epsilon: 0.864\n",
      "Episode: 69000, Avg Reward: -0.324, Epsilon: 0.862\n",
      "Episode: 70000, Avg Reward: -0.321, Epsilon: 0.860\n",
      "Episode: 71000, Avg Reward: -0.286, Epsilon: 0.858\n",
      "Episode: 72000, Avg Reward: -0.342, Epsilon: 0.856\n",
      "Episode: 73000, Avg Reward: -0.382, Epsilon: 0.854\n",
      "Episode: 74000, Avg Reward: -0.361, Epsilon: 0.852\n",
      "Episode: 75000, Avg Reward: -0.383, Epsilon: 0.850\n",
      "Episode: 76000, Avg Reward: -0.360, Epsilon: 0.848\n",
      "Episode: 77000, Avg Reward: -0.358, Epsilon: 0.846\n",
      "Episode: 78000, Avg Reward: -0.290, Epsilon: 0.844\n",
      "Episode: 79000, Avg Reward: -0.345, Epsilon: 0.842\n",
      "Episode: 80000, Avg Reward: -0.382, Epsilon: 0.840\n",
      "Episode: 81000, Avg Reward: -0.311, Epsilon: 0.838\n",
      "Episode: 82000, Avg Reward: -0.387, Epsilon: 0.836\n",
      "Episode: 83000, Avg Reward: -0.341, Epsilon: 0.834\n",
      "Episode: 84000, Avg Reward: -0.343, Epsilon: 0.832\n",
      "Episode: 85000, Avg Reward: -0.337, Epsilon: 0.830\n",
      "Episode: 86000, Avg Reward: -0.313, Epsilon: 0.828\n",
      "Episode: 87000, Avg Reward: -0.325, Epsilon: 0.826\n",
      "Episode: 88000, Avg Reward: -0.347, Epsilon: 0.824\n",
      "Episode: 89000, Avg Reward: -0.347, Epsilon: 0.822\n",
      "Episode: 90000, Avg Reward: -0.373, Epsilon: 0.820\n",
      "Episode: 91000, Avg Reward: -0.304, Epsilon: 0.818\n",
      "Episode: 92000, Avg Reward: -0.337, Epsilon: 0.816\n",
      "Episode: 93000, Avg Reward: -0.309, Epsilon: 0.814\n",
      "Episode: 94000, Avg Reward: -0.286, Epsilon: 0.812\n",
      "Episode: 95000, Avg Reward: -0.359, Epsilon: 0.810\n",
      "Episode: 96000, Avg Reward: -0.426, Epsilon: 0.808\n",
      "Episode: 97000, Avg Reward: -0.369, Epsilon: 0.806\n",
      "Episode: 98000, Avg Reward: -0.318, Epsilon: 0.804\n",
      "Episode: 99000, Avg Reward: -0.311, Epsilon: 0.802\n",
      "Episode: 100000, Avg Reward: -0.348, Epsilon: 0.800\n",
      "Episode: 101000, Avg Reward: -0.297, Epsilon: 0.798\n",
      "Episode: 102000, Avg Reward: -0.349, Epsilon: 0.796\n",
      "Episode: 103000, Avg Reward: -0.384, Epsilon: 0.794\n",
      "Episode: 104000, Avg Reward: -0.313, Epsilon: 0.792\n",
      "Episode: 105000, Avg Reward: -0.345, Epsilon: 0.790\n",
      "Episode: 106000, Avg Reward: -0.357, Epsilon: 0.788\n",
      "Episode: 107000, Avg Reward: -0.308, Epsilon: 0.786\n",
      "Episode: 108000, Avg Reward: -0.319, Epsilon: 0.784\n",
      "Episode: 109000, Avg Reward: -0.279, Epsilon: 0.782\n",
      "Episode: 110000, Avg Reward: -0.359, Epsilon: 0.780\n",
      "Episode: 111000, Avg Reward: -0.325, Epsilon: 0.778\n",
      "Episode: 112000, Avg Reward: -0.315, Epsilon: 0.776\n",
      "Episode: 113000, Avg Reward: -0.322, Epsilon: 0.774\n",
      "Episode: 114000, Avg Reward: -0.330, Epsilon: 0.772\n",
      "Episode: 115000, Avg Reward: -0.326, Epsilon: 0.770\n",
      "Episode: 116000, Avg Reward: -0.308, Epsilon: 0.768\n",
      "Episode: 117000, Avg Reward: -0.337, Epsilon: 0.766\n",
      "Episode: 118000, Avg Reward: -0.318, Epsilon: 0.764\n",
      "Episode: 119000, Avg Reward: -0.276, Epsilon: 0.762\n",
      "Episode: 120000, Avg Reward: -0.329, Epsilon: 0.760\n",
      "Episode: 121000, Avg Reward: -0.309, Epsilon: 0.758\n",
      "Episode: 122000, Avg Reward: -0.321, Epsilon: 0.756\n",
      "Episode: 123000, Avg Reward: -0.336, Epsilon: 0.754\n",
      "Episode: 124000, Avg Reward: -0.284, Epsilon: 0.752\n",
      "Episode: 125000, Avg Reward: -0.282, Epsilon: 0.750\n",
      "Episode: 126000, Avg Reward: -0.330, Epsilon: 0.748\n",
      "Episode: 127000, Avg Reward: -0.291, Epsilon: 0.746\n",
      "Episode: 128000, Avg Reward: -0.324, Epsilon: 0.744\n",
      "Episode: 129000, Avg Reward: -0.294, Epsilon: 0.742\n",
      "Episode: 130000, Avg Reward: -0.329, Epsilon: 0.740\n",
      "Episode: 131000, Avg Reward: -0.321, Epsilon: 0.738\n",
      "Episode: 132000, Avg Reward: -0.306, Epsilon: 0.736\n",
      "Episode: 133000, Avg Reward: -0.332, Epsilon: 0.734\n",
      "Episode: 134000, Avg Reward: -0.275, Epsilon: 0.732\n",
      "Episode: 135000, Avg Reward: -0.322, Epsilon: 0.730\n",
      "Episode: 136000, Avg Reward: -0.261, Epsilon: 0.728\n",
      "Episode: 137000, Avg Reward: -0.315, Epsilon: 0.726\n",
      "Episode: 138000, Avg Reward: -0.294, Epsilon: 0.724\n",
      "Episode: 139000, Avg Reward: -0.354, Epsilon: 0.722\n",
      "Episode: 140000, Avg Reward: -0.285, Epsilon: 0.720\n",
      "Episode: 141000, Avg Reward: -0.313, Epsilon: 0.718\n",
      "Episode: 142000, Avg Reward: -0.338, Epsilon: 0.716\n",
      "Episode: 143000, Avg Reward: -0.259, Epsilon: 0.714\n",
      "Episode: 144000, Avg Reward: -0.314, Epsilon: 0.712\n",
      "Episode: 145000, Avg Reward: -0.319, Epsilon: 0.710\n",
      "Episode: 146000, Avg Reward: -0.276, Epsilon: 0.708\n",
      "Episode: 147000, Avg Reward: -0.310, Epsilon: 0.706\n",
      "Episode: 148000, Avg Reward: -0.338, Epsilon: 0.704\n",
      "Episode: 149000, Avg Reward: -0.287, Epsilon: 0.702\n",
      "Episode: 150000, Avg Reward: -0.320, Epsilon: 0.700\n",
      "Episode: 151000, Avg Reward: -0.316, Epsilon: 0.698\n",
      "Episode: 152000, Avg Reward: -0.296, Epsilon: 0.696\n",
      "Episode: 153000, Avg Reward: -0.322, Epsilon: 0.694\n",
      "Episode: 154000, Avg Reward: -0.302, Epsilon: 0.692\n",
      "Episode: 155000, Avg Reward: -0.293, Epsilon: 0.690\n",
      "Episode: 156000, Avg Reward: -0.302, Epsilon: 0.688\n",
      "Episode: 157000, Avg Reward: -0.319, Epsilon: 0.686\n",
      "Episode: 158000, Avg Reward: -0.291, Epsilon: 0.684\n",
      "Episode: 159000, Avg Reward: -0.296, Epsilon: 0.682\n",
      "Episode: 160000, Avg Reward: -0.336, Epsilon: 0.680\n",
      "Episode: 161000, Avg Reward: -0.288, Epsilon: 0.678\n",
      "Episode: 162000, Avg Reward: -0.293, Epsilon: 0.676\n",
      "Episode: 163000, Avg Reward: -0.293, Epsilon: 0.674\n",
      "Episode: 164000, Avg Reward: -0.270, Epsilon: 0.672\n",
      "Episode: 165000, Avg Reward: -0.312, Epsilon: 0.670\n",
      "Episode: 166000, Avg Reward: -0.326, Epsilon: 0.668\n",
      "Episode: 167000, Avg Reward: -0.330, Epsilon: 0.666\n",
      "Episode: 168000, Avg Reward: -0.243, Epsilon: 0.664\n",
      "Episode: 169000, Avg Reward: -0.347, Epsilon: 0.662\n",
      "Episode: 170000, Avg Reward: -0.249, Epsilon: 0.660\n",
      "Episode: 171000, Avg Reward: -0.304, Epsilon: 0.658\n",
      "Episode: 172000, Avg Reward: -0.274, Epsilon: 0.656\n",
      "Episode: 173000, Avg Reward: -0.301, Epsilon: 0.654\n",
      "Episode: 174000, Avg Reward: -0.240, Epsilon: 0.652\n",
      "Episode: 175000, Avg Reward: -0.305, Epsilon: 0.650\n",
      "Episode: 176000, Avg Reward: -0.314, Epsilon: 0.648\n",
      "Episode: 177000, Avg Reward: -0.248, Epsilon: 0.646\n",
      "Episode: 178000, Avg Reward: -0.258, Epsilon: 0.644\n",
      "Episode: 179000, Avg Reward: -0.264, Epsilon: 0.642\n",
      "Episode: 180000, Avg Reward: -0.274, Epsilon: 0.640\n",
      "Episode: 181000, Avg Reward: -0.260, Epsilon: 0.638\n",
      "Episode: 182000, Avg Reward: -0.276, Epsilon: 0.636\n",
      "Episode: 183000, Avg Reward: -0.219, Epsilon: 0.634\n",
      "Episode: 184000, Avg Reward: -0.246, Epsilon: 0.632\n",
      "Episode: 185000, Avg Reward: -0.268, Epsilon: 0.630\n",
      "Episode: 186000, Avg Reward: -0.266, Epsilon: 0.628\n",
      "Episode: 187000, Avg Reward: -0.264, Epsilon: 0.626\n",
      "Episode: 188000, Avg Reward: -0.264, Epsilon: 0.624\n",
      "Episode: 189000, Avg Reward: -0.289, Epsilon: 0.622\n",
      "Episode: 190000, Avg Reward: -0.215, Epsilon: 0.620\n",
      "Episode: 191000, Avg Reward: -0.329, Epsilon: 0.618\n",
      "Episode: 192000, Avg Reward: -0.292, Epsilon: 0.616\n",
      "Episode: 193000, Avg Reward: -0.291, Epsilon: 0.614\n",
      "Episode: 194000, Avg Reward: -0.244, Epsilon: 0.612\n",
      "Episode: 195000, Avg Reward: -0.230, Epsilon: 0.610\n",
      "Episode: 196000, Avg Reward: -0.310, Epsilon: 0.608\n",
      "Episode: 197000, Avg Reward: -0.319, Epsilon: 0.606\n",
      "Episode: 198000, Avg Reward: -0.267, Epsilon: 0.604\n",
      "Episode: 199000, Avg Reward: -0.329, Epsilon: 0.602\n",
      "Episode: 200000, Avg Reward: -0.265, Epsilon: 0.600\n",
      "Episode: 201000, Avg Reward: -0.256, Epsilon: 0.598\n",
      "Episode: 202000, Avg Reward: -0.264, Epsilon: 0.596\n",
      "Episode: 203000, Avg Reward: -0.250, Epsilon: 0.594\n",
      "Episode: 204000, Avg Reward: -0.363, Epsilon: 0.592\n",
      "Episode: 205000, Avg Reward: -0.263, Epsilon: 0.590\n",
      "Episode: 206000, Avg Reward: -0.294, Epsilon: 0.588\n",
      "Episode: 207000, Avg Reward: -0.323, Epsilon: 0.586\n",
      "Episode: 208000, Avg Reward: -0.221, Epsilon: 0.584\n",
      "Episode: 209000, Avg Reward: -0.266, Epsilon: 0.582\n",
      "Episode: 210000, Avg Reward: -0.290, Epsilon: 0.580\n",
      "Episode: 211000, Avg Reward: -0.264, Epsilon: 0.578\n",
      "Episode: 212000, Avg Reward: -0.227, Epsilon: 0.576\n",
      "Episode: 213000, Avg Reward: -0.284, Epsilon: 0.574\n",
      "Episode: 214000, Avg Reward: -0.305, Epsilon: 0.572\n",
      "Episode: 215000, Avg Reward: -0.260, Epsilon: 0.570\n",
      "Episode: 216000, Avg Reward: -0.273, Epsilon: 0.568\n",
      "Episode: 217000, Avg Reward: -0.249, Epsilon: 0.566\n",
      "Episode: 218000, Avg Reward: -0.231, Epsilon: 0.564\n",
      "Episode: 219000, Avg Reward: -0.258, Epsilon: 0.562\n",
      "Episode: 220000, Avg Reward: -0.285, Epsilon: 0.560\n",
      "Episode: 221000, Avg Reward: -0.260, Epsilon: 0.558\n",
      "Episode: 222000, Avg Reward: -0.267, Epsilon: 0.556\n",
      "Episode: 223000, Avg Reward: -0.287, Epsilon: 0.554\n",
      "Episode: 224000, Avg Reward: -0.211, Epsilon: 0.552\n",
      "Episode: 225000, Avg Reward: -0.261, Epsilon: 0.550\n",
      "Episode: 226000, Avg Reward: -0.305, Epsilon: 0.548\n",
      "Episode: 227000, Avg Reward: -0.263, Epsilon: 0.546\n",
      "Episode: 228000, Avg Reward: -0.260, Epsilon: 0.544\n",
      "Episode: 229000, Avg Reward: -0.240, Epsilon: 0.542\n",
      "Episode: 230000, Avg Reward: -0.242, Epsilon: 0.540\n",
      "Episode: 231000, Avg Reward: -0.222, Epsilon: 0.538\n",
      "Episode: 232000, Avg Reward: -0.283, Epsilon: 0.536\n",
      "Episode: 233000, Avg Reward: -0.290, Epsilon: 0.534\n",
      "Episode: 234000, Avg Reward: -0.214, Epsilon: 0.532\n",
      "Episode: 235000, Avg Reward: -0.320, Epsilon: 0.530\n",
      "Episode: 236000, Avg Reward: -0.256, Epsilon: 0.528\n",
      "Episode: 237000, Avg Reward: -0.259, Epsilon: 0.526\n",
      "Episode: 238000, Avg Reward: -0.252, Epsilon: 0.524\n",
      "Episode: 239000, Avg Reward: -0.316, Epsilon: 0.522\n",
      "Episode: 240000, Avg Reward: -0.255, Epsilon: 0.520\n",
      "Episode: 241000, Avg Reward: -0.265, Epsilon: 0.518\n",
      "Episode: 242000, Avg Reward: -0.247, Epsilon: 0.516\n",
      "Episode: 243000, Avg Reward: -0.278, Epsilon: 0.514\n",
      "Episode: 244000, Avg Reward: -0.258, Epsilon: 0.512\n",
      "Episode: 245000, Avg Reward: -0.256, Epsilon: 0.510\n",
      "Episode: 246000, Avg Reward: -0.274, Epsilon: 0.508\n",
      "Episode: 247000, Avg Reward: -0.243, Epsilon: 0.506\n",
      "Episode: 248000, Avg Reward: -0.305, Epsilon: 0.504\n",
      "Episode: 249000, Avg Reward: -0.255, Epsilon: 0.502\n",
      "Episode: 250000, Avg Reward: -0.322, Epsilon: 0.500\n",
      "Episode: 251000, Avg Reward: -0.284, Epsilon: 0.498\n",
      "Episode: 252000, Avg Reward: -0.264, Epsilon: 0.496\n",
      "Episode: 253000, Avg Reward: -0.266, Epsilon: 0.494\n",
      "Episode: 254000, Avg Reward: -0.273, Epsilon: 0.492\n",
      "Episode: 255000, Avg Reward: -0.253, Epsilon: 0.490\n",
      "Episode: 256000, Avg Reward: -0.278, Epsilon: 0.488\n",
      "Episode: 257000, Avg Reward: -0.317, Epsilon: 0.486\n",
      "Episode: 258000, Avg Reward: -0.233, Epsilon: 0.484\n",
      "Episode: 259000, Avg Reward: -0.253, Epsilon: 0.482\n",
      "Episode: 260000, Avg Reward: -0.244, Epsilon: 0.480\n",
      "Episode: 261000, Avg Reward: -0.296, Epsilon: 0.478\n",
      "Episode: 262000, Avg Reward: -0.223, Epsilon: 0.476\n",
      "Episode: 263000, Avg Reward: -0.269, Epsilon: 0.474\n",
      "Episode: 264000, Avg Reward: -0.256, Epsilon: 0.472\n",
      "Episode: 265000, Avg Reward: -0.267, Epsilon: 0.470\n",
      "Episode: 266000, Avg Reward: -0.248, Epsilon: 0.468\n",
      "Episode: 267000, Avg Reward: -0.278, Epsilon: 0.466\n",
      "Episode: 268000, Avg Reward: -0.230, Epsilon: 0.464\n",
      "Episode: 269000, Avg Reward: -0.172, Epsilon: 0.462\n",
      "Episode: 270000, Avg Reward: -0.257, Epsilon: 0.460\n",
      "Episode: 271000, Avg Reward: -0.239, Epsilon: 0.458\n",
      "Episode: 272000, Avg Reward: -0.275, Epsilon: 0.456\n",
      "Episode: 273000, Avg Reward: -0.271, Epsilon: 0.454\n",
      "Episode: 274000, Avg Reward: -0.293, Epsilon: 0.452\n",
      "Episode: 275000, Avg Reward: -0.290, Epsilon: 0.450\n",
      "Episode: 276000, Avg Reward: -0.236, Epsilon: 0.448\n",
      "Episode: 277000, Avg Reward: -0.296, Epsilon: 0.446\n",
      "Episode: 278000, Avg Reward: -0.265, Epsilon: 0.444\n",
      "Episode: 279000, Avg Reward: -0.225, Epsilon: 0.442\n",
      "Episode: 280000, Avg Reward: -0.262, Epsilon: 0.440\n",
      "Episode: 281000, Avg Reward: -0.278, Epsilon: 0.438\n",
      "Episode: 282000, Avg Reward: -0.249, Epsilon: 0.436\n",
      "Episode: 283000, Avg Reward: -0.216, Epsilon: 0.434\n",
      "Episode: 284000, Avg Reward: -0.242, Epsilon: 0.432\n",
      "Episode: 285000, Avg Reward: -0.197, Epsilon: 0.430\n",
      "Episode: 286000, Avg Reward: -0.289, Epsilon: 0.428\n",
      "Episode: 287000, Avg Reward: -0.279, Epsilon: 0.426\n",
      "Episode: 288000, Avg Reward: -0.241, Epsilon: 0.424\n",
      "Episode: 289000, Avg Reward: -0.218, Epsilon: 0.422\n",
      "Episode: 290000, Avg Reward: -0.253, Epsilon: 0.420\n",
      "Episode: 291000, Avg Reward: -0.274, Epsilon: 0.418\n",
      "Episode: 292000, Avg Reward: -0.229, Epsilon: 0.416\n",
      "Episode: 293000, Avg Reward: -0.258, Epsilon: 0.414\n",
      "Episode: 294000, Avg Reward: -0.233, Epsilon: 0.412\n",
      "Episode: 295000, Avg Reward: -0.173, Epsilon: 0.410\n",
      "Episode: 296000, Avg Reward: -0.282, Epsilon: 0.408\n",
      "Episode: 297000, Avg Reward: -0.255, Epsilon: 0.406\n",
      "Episode: 298000, Avg Reward: -0.218, Epsilon: 0.404\n",
      "Episode: 299000, Avg Reward: -0.228, Epsilon: 0.402\n",
      "Episode: 300000, Avg Reward: -0.280, Epsilon: 0.400\n",
      "Episode: 301000, Avg Reward: -0.246, Epsilon: 0.398\n",
      "Episode: 302000, Avg Reward: -0.283, Epsilon: 0.396\n",
      "Episode: 303000, Avg Reward: -0.273, Epsilon: 0.394\n",
      "Episode: 304000, Avg Reward: -0.236, Epsilon: 0.392\n",
      "Episode: 305000, Avg Reward: -0.230, Epsilon: 0.390\n",
      "Episode: 306000, Avg Reward: -0.205, Epsilon: 0.388\n",
      "Episode: 307000, Avg Reward: -0.240, Epsilon: 0.386\n",
      "Episode: 308000, Avg Reward: -0.196, Epsilon: 0.384\n",
      "Episode: 309000, Avg Reward: -0.239, Epsilon: 0.382\n",
      "Episode: 310000, Avg Reward: -0.216, Epsilon: 0.380\n",
      "Episode: 311000, Avg Reward: -0.212, Epsilon: 0.378\n",
      "Episode: 312000, Avg Reward: -0.272, Epsilon: 0.376\n",
      "Episode: 313000, Avg Reward: -0.222, Epsilon: 0.374\n",
      "Episode: 314000, Avg Reward: -0.237, Epsilon: 0.372\n",
      "Episode: 315000, Avg Reward: -0.224, Epsilon: 0.370\n",
      "Episode: 316000, Avg Reward: -0.281, Epsilon: 0.368\n",
      "Episode: 317000, Avg Reward: -0.256, Epsilon: 0.366\n",
      "Episode: 318000, Avg Reward: -0.222, Epsilon: 0.364\n",
      "Episode: 319000, Avg Reward: -0.161, Epsilon: 0.362\n",
      "Episode: 320000, Avg Reward: -0.286, Epsilon: 0.360\n",
      "Episode: 321000, Avg Reward: -0.221, Epsilon: 0.358\n",
      "Episode: 322000, Avg Reward: -0.232, Epsilon: 0.356\n",
      "Episode: 323000, Avg Reward: -0.302, Epsilon: 0.354\n",
      "Episode: 324000, Avg Reward: -0.193, Epsilon: 0.352\n",
      "Episode: 325000, Avg Reward: -0.237, Epsilon: 0.350\n",
      "Episode: 326000, Avg Reward: -0.222, Epsilon: 0.348\n",
      "Episode: 327000, Avg Reward: -0.247, Epsilon: 0.346\n",
      "Episode: 328000, Avg Reward: -0.196, Epsilon: 0.344\n",
      "Episode: 329000, Avg Reward: -0.194, Epsilon: 0.342\n",
      "Episode: 330000, Avg Reward: -0.221, Epsilon: 0.340\n",
      "Episode: 331000, Avg Reward: -0.189, Epsilon: 0.338\n",
      "Episode: 332000, Avg Reward: -0.256, Epsilon: 0.336\n",
      "Episode: 333000, Avg Reward: -0.219, Epsilon: 0.334\n",
      "Episode: 334000, Avg Reward: -0.294, Epsilon: 0.332\n",
      "Episode: 335000, Avg Reward: -0.227, Epsilon: 0.330\n",
      "Episode: 336000, Avg Reward: -0.251, Epsilon: 0.328\n",
      "Episode: 337000, Avg Reward: -0.231, Epsilon: 0.326\n",
      "Episode: 338000, Avg Reward: -0.189, Epsilon: 0.324\n",
      "Episode: 339000, Avg Reward: -0.216, Epsilon: 0.322\n",
      "Episode: 340000, Avg Reward: -0.270, Epsilon: 0.320\n",
      "Episode: 341000, Avg Reward: -0.204, Epsilon: 0.318\n",
      "Episode: 342000, Avg Reward: -0.229, Epsilon: 0.316\n",
      "Episode: 343000, Avg Reward: -0.217, Epsilon: 0.314\n",
      "Episode: 344000, Avg Reward: -0.231, Epsilon: 0.312\n",
      "Episode: 345000, Avg Reward: -0.214, Epsilon: 0.310\n",
      "Episode: 346000, Avg Reward: -0.163, Epsilon: 0.308\n",
      "Episode: 347000, Avg Reward: -0.228, Epsilon: 0.306\n",
      "Episode: 348000, Avg Reward: -0.221, Epsilon: 0.304\n",
      "Episode: 349000, Avg Reward: -0.208, Epsilon: 0.302\n",
      "Episode: 350000, Avg Reward: -0.254, Epsilon: 0.300\n",
      "Episode: 351000, Avg Reward: -0.214, Epsilon: 0.298\n",
      "Episode: 352000, Avg Reward: -0.191, Epsilon: 0.296\n",
      "Episode: 353000, Avg Reward: -0.248, Epsilon: 0.294\n",
      "Episode: 354000, Avg Reward: -0.183, Epsilon: 0.292\n",
      "Episode: 355000, Avg Reward: -0.228, Epsilon: 0.290\n",
      "Episode: 356000, Avg Reward: -0.204, Epsilon: 0.288\n",
      "Episode: 357000, Avg Reward: -0.242, Epsilon: 0.286\n",
      "Episode: 358000, Avg Reward: -0.187, Epsilon: 0.284\n",
      "Episode: 359000, Avg Reward: -0.195, Epsilon: 0.282\n",
      "Episode: 360000, Avg Reward: -0.213, Epsilon: 0.280\n",
      "Episode: 361000, Avg Reward: -0.140, Epsilon: 0.278\n",
      "Episode: 362000, Avg Reward: -0.206, Epsilon: 0.276\n",
      "Episode: 363000, Avg Reward: -0.143, Epsilon: 0.274\n",
      "Episode: 364000, Avg Reward: -0.268, Epsilon: 0.272\n",
      "Episode: 365000, Avg Reward: -0.207, Epsilon: 0.270\n",
      "Episode: 366000, Avg Reward: -0.240, Epsilon: 0.268\n",
      "Episode: 367000, Avg Reward: -0.258, Epsilon: 0.266\n",
      "Episode: 368000, Avg Reward: -0.225, Epsilon: 0.264\n",
      "Episode: 369000, Avg Reward: -0.204, Epsilon: 0.262\n",
      "Episode: 370000, Avg Reward: -0.144, Epsilon: 0.260\n",
      "Episode: 371000, Avg Reward: -0.249, Epsilon: 0.258\n",
      "Episode: 372000, Avg Reward: -0.230, Epsilon: 0.256\n",
      "Episode: 373000, Avg Reward: -0.230, Epsilon: 0.254\n",
      "Episode: 374000, Avg Reward: -0.264, Epsilon: 0.252\n",
      "Episode: 375000, Avg Reward: -0.202, Epsilon: 0.250\n",
      "Episode: 376000, Avg Reward: -0.213, Epsilon: 0.248\n",
      "Episode: 377000, Avg Reward: -0.218, Epsilon: 0.246\n",
      "Episode: 378000, Avg Reward: -0.247, Epsilon: 0.244\n",
      "Episode: 379000, Avg Reward: -0.237, Epsilon: 0.242\n",
      "Episode: 380000, Avg Reward: -0.184, Epsilon: 0.240\n",
      "Episode: 381000, Avg Reward: -0.210, Epsilon: 0.238\n",
      "Episode: 382000, Avg Reward: -0.145, Epsilon: 0.236\n",
      "Episode: 383000, Avg Reward: -0.166, Epsilon: 0.234\n",
      "Episode: 384000, Avg Reward: -0.189, Epsilon: 0.232\n",
      "Episode: 385000, Avg Reward: -0.199, Epsilon: 0.230\n",
      "Episode: 386000, Avg Reward: -0.226, Epsilon: 0.228\n",
      "Episode: 387000, Avg Reward: -0.220, Epsilon: 0.226\n",
      "Episode: 388000, Avg Reward: -0.196, Epsilon: 0.224\n",
      "Episode: 389000, Avg Reward: -0.258, Epsilon: 0.222\n",
      "Episode: 390000, Avg Reward: -0.209, Epsilon: 0.220\n",
      "Episode: 391000, Avg Reward: -0.230, Epsilon: 0.218\n",
      "Episode: 392000, Avg Reward: -0.187, Epsilon: 0.216\n",
      "Episode: 393000, Avg Reward: -0.239, Epsilon: 0.214\n",
      "Episode: 394000, Avg Reward: -0.192, Epsilon: 0.212\n",
      "Episode: 395000, Avg Reward: -0.201, Epsilon: 0.210\n",
      "Episode: 396000, Avg Reward: -0.199, Epsilon: 0.208\n",
      "Episode: 397000, Avg Reward: -0.235, Epsilon: 0.206\n",
      "Episode: 398000, Avg Reward: -0.227, Epsilon: 0.204\n",
      "Episode: 399000, Avg Reward: -0.189, Epsilon: 0.202\n",
      "Episode: 400000, Avg Reward: -0.271, Epsilon: 0.200\n",
      "Episode: 401000, Avg Reward: -0.190, Epsilon: 0.198\n",
      "Episode: 402000, Avg Reward: -0.227, Epsilon: 0.196\n",
      "Episode: 403000, Avg Reward: -0.261, Epsilon: 0.194\n",
      "Episode: 404000, Avg Reward: -0.201, Epsilon: 0.192\n",
      "Episode: 405000, Avg Reward: -0.244, Epsilon: 0.190\n",
      "Episode: 406000, Avg Reward: -0.238, Epsilon: 0.188\n",
      "Episode: 407000, Avg Reward: -0.197, Epsilon: 0.186\n",
      "Episode: 408000, Avg Reward: -0.207, Epsilon: 0.184\n",
      "Episode: 409000, Avg Reward: -0.254, Epsilon: 0.182\n",
      "Episode: 410000, Avg Reward: -0.182, Epsilon: 0.180\n",
      "Episode: 411000, Avg Reward: -0.180, Epsilon: 0.178\n",
      "Episode: 412000, Avg Reward: -0.167, Epsilon: 0.176\n",
      "Episode: 413000, Avg Reward: -0.185, Epsilon: 0.174\n",
      "Episode: 414000, Avg Reward: -0.217, Epsilon: 0.172\n",
      "Episode: 415000, Avg Reward: -0.130, Epsilon: 0.170\n",
      "Episode: 416000, Avg Reward: -0.207, Epsilon: 0.168\n",
      "Episode: 417000, Avg Reward: -0.196, Epsilon: 0.166\n",
      "Episode: 418000, Avg Reward: -0.219, Epsilon: 0.164\n",
      "Episode: 419000, Avg Reward: -0.190, Epsilon: 0.162\n",
      "Episode: 420000, Avg Reward: -0.186, Epsilon: 0.160\n",
      "Episode: 421000, Avg Reward: -0.189, Epsilon: 0.158\n",
      "Episode: 422000, Avg Reward: -0.261, Epsilon: 0.156\n",
      "Episode: 423000, Avg Reward: -0.200, Epsilon: 0.154\n",
      "Episode: 424000, Avg Reward: -0.233, Epsilon: 0.152\n",
      "Episode: 425000, Avg Reward: -0.209, Epsilon: 0.150\n",
      "Episode: 426000, Avg Reward: -0.218, Epsilon: 0.148\n",
      "Episode: 427000, Avg Reward: -0.199, Epsilon: 0.146\n",
      "Episode: 428000, Avg Reward: -0.214, Epsilon: 0.144\n",
      "Episode: 429000, Avg Reward: -0.215, Epsilon: 0.142\n",
      "Episode: 430000, Avg Reward: -0.222, Epsilon: 0.140\n",
      "Episode: 431000, Avg Reward: -0.214, Epsilon: 0.138\n",
      "Episode: 432000, Avg Reward: -0.143, Epsilon: 0.136\n",
      "Episode: 433000, Avg Reward: -0.202, Epsilon: 0.134\n",
      "Episode: 434000, Avg Reward: -0.171, Epsilon: 0.132\n",
      "Episode: 435000, Avg Reward: -0.210, Epsilon: 0.130\n",
      "Episode: 436000, Avg Reward: -0.209, Epsilon: 0.128\n",
      "Episode: 437000, Avg Reward: -0.253, Epsilon: 0.126\n",
      "Episode: 438000, Avg Reward: -0.236, Epsilon: 0.124\n",
      "Episode: 439000, Avg Reward: -0.184, Epsilon: 0.122\n",
      "Episode: 440000, Avg Reward: -0.220, Epsilon: 0.120\n",
      "Episode: 441000, Avg Reward: -0.196, Epsilon: 0.118\n",
      "Episode: 442000, Avg Reward: -0.178, Epsilon: 0.116\n",
      "Episode: 443000, Avg Reward: -0.187, Epsilon: 0.114\n",
      "Episode: 444000, Avg Reward: -0.190, Epsilon: 0.112\n",
      "Episode: 445000, Avg Reward: -0.222, Epsilon: 0.110\n",
      "Episode: 446000, Avg Reward: -0.224, Epsilon: 0.108\n",
      "Episode: 447000, Avg Reward: -0.210, Epsilon: 0.106\n",
      "Episode: 448000, Avg Reward: -0.160, Epsilon: 0.104\n",
      "Episode: 449000, Avg Reward: -0.140, Epsilon: 0.102\n",
      "Episode: 450000, Avg Reward: -0.170, Epsilon: 0.100\n",
      "Episode: 451000, Avg Reward: -0.239, Epsilon: 0.100\n",
      "Episode: 452000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 453000, Avg Reward: -0.151, Epsilon: 0.100\n",
      "Episode: 454000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 455000, Avg Reward: -0.168, Epsilon: 0.100\n",
      "Episode: 456000, Avg Reward: -0.148, Epsilon: 0.100\n",
      "Episode: 457000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 458000, Avg Reward: -0.161, Epsilon: 0.100\n",
      "Episode: 459000, Avg Reward: -0.155, Epsilon: 0.100\n",
      "Episode: 460000, Avg Reward: -0.229, Epsilon: 0.100\n",
      "Episode: 461000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 462000, Avg Reward: -0.176, Epsilon: 0.100\n",
      "Episode: 463000, Avg Reward: -0.213, Epsilon: 0.100\n",
      "Episode: 464000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 465000, Avg Reward: -0.148, Epsilon: 0.100\n",
      "Episode: 466000, Avg Reward: -0.178, Epsilon: 0.100\n",
      "Episode: 467000, Avg Reward: -0.168, Epsilon: 0.100\n",
      "Episode: 468000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 469000, Avg Reward: -0.148, Epsilon: 0.100\n",
      "Episode: 470000, Avg Reward: -0.184, Epsilon: 0.100\n",
      "Episode: 471000, Avg Reward: -0.178, Epsilon: 0.100\n",
      "Episode: 472000, Avg Reward: -0.240, Epsilon: 0.100\n",
      "Episode: 473000, Avg Reward: -0.208, Epsilon: 0.100\n",
      "Episode: 474000, Avg Reward: -0.220, Epsilon: 0.100\n",
      "Episode: 475000, Avg Reward: -0.214, Epsilon: 0.100\n",
      "Episode: 476000, Avg Reward: -0.162, Epsilon: 0.100\n",
      "Episode: 477000, Avg Reward: -0.208, Epsilon: 0.100\n",
      "Episode: 478000, Avg Reward: -0.184, Epsilon: 0.100\n",
      "Episode: 479000, Avg Reward: -0.168, Epsilon: 0.100\n",
      "Episode: 480000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 481000, Avg Reward: -0.198, Epsilon: 0.100\n",
      "Episode: 482000, Avg Reward: -0.162, Epsilon: 0.100\n",
      "Episode: 483000, Avg Reward: -0.248, Epsilon: 0.100\n",
      "Episode: 484000, Avg Reward: -0.219, Epsilon: 0.100\n",
      "Episode: 485000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 486000, Avg Reward: -0.148, Epsilon: 0.100\n",
      "Episode: 487000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 488000, Avg Reward: -0.240, Epsilon: 0.100\n",
      "Episode: 489000, Avg Reward: -0.205, Epsilon: 0.100\n",
      "Episode: 490000, Avg Reward: -0.156, Epsilon: 0.100\n",
      "Episode: 491000, Avg Reward: -0.142, Epsilon: 0.100\n",
      "Episode: 492000, Avg Reward: -0.193, Epsilon: 0.100\n",
      "Episode: 493000, Avg Reward: -0.178, Epsilon: 0.100\n",
      "Episode: 494000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 495000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 496000, Avg Reward: -0.147, Epsilon: 0.100\n",
      "Episode: 497000, Avg Reward: -0.244, Epsilon: 0.100\n",
      "Episode: 498000, Avg Reward: -0.142, Epsilon: 0.100\n",
      "Episode: 499000, Avg Reward: -0.179, Epsilon: 0.100\n",
      "Episode: 500000, Avg Reward: -0.158, Epsilon: 0.100\n",
      "Episode: 501000, Avg Reward: -0.137, Epsilon: 0.100\n",
      "Episode: 502000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 503000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 504000, Avg Reward: -0.170, Epsilon: 0.100\n",
      "Episode: 505000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 506000, Avg Reward: -0.226, Epsilon: 0.100\n",
      "Episode: 507000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 508000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 509000, Avg Reward: -0.158, Epsilon: 0.100\n",
      "Episode: 510000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 511000, Avg Reward: -0.226, Epsilon: 0.100\n",
      "Episode: 512000, Avg Reward: -0.119, Epsilon: 0.100\n",
      "Episode: 513000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 514000, Avg Reward: -0.221, Epsilon: 0.100\n",
      "Episode: 515000, Avg Reward: -0.147, Epsilon: 0.100\n",
      "Episode: 516000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 517000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 518000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 519000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 520000, Avg Reward: -0.184, Epsilon: 0.100\n",
      "Episode: 521000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 522000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 523000, Avg Reward: -0.176, Epsilon: 0.100\n",
      "Episode: 524000, Avg Reward: -0.228, Epsilon: 0.100\n",
      "Episode: 525000, Avg Reward: -0.205, Epsilon: 0.100\n",
      "Episode: 526000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 527000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 528000, Avg Reward: -0.175, Epsilon: 0.100\n",
      "Episode: 529000, Avg Reward: -0.189, Epsilon: 0.100\n",
      "Episode: 530000, Avg Reward: -0.222, Epsilon: 0.100\n",
      "Episode: 531000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 532000, Avg Reward: -0.202, Epsilon: 0.100\n",
      "Episode: 533000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 534000, Avg Reward: -0.218, Epsilon: 0.100\n",
      "Episode: 535000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 536000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 537000, Avg Reward: -0.271, Epsilon: 0.100\n",
      "Episode: 538000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 539000, Avg Reward: -0.202, Epsilon: 0.100\n",
      "Episode: 540000, Avg Reward: -0.134, Epsilon: 0.100\n",
      "Episode: 541000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 542000, Avg Reward: -0.179, Epsilon: 0.100\n",
      "Episode: 543000, Avg Reward: -0.175, Epsilon: 0.100\n",
      "Episode: 544000, Avg Reward: -0.193, Epsilon: 0.100\n",
      "Episode: 545000, Avg Reward: -0.158, Epsilon: 0.100\n",
      "Episode: 546000, Avg Reward: -0.217, Epsilon: 0.100\n",
      "Episode: 547000, Avg Reward: -0.175, Epsilon: 0.100\n",
      "Episode: 548000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 549000, Avg Reward: -0.157, Epsilon: 0.100\n",
      "Episode: 550000, Avg Reward: -0.202, Epsilon: 0.100\n",
      "Episode: 551000, Avg Reward: -0.135, Epsilon: 0.100\n",
      "Episode: 552000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 553000, Avg Reward: -0.226, Epsilon: 0.100\n",
      "Episode: 554000, Avg Reward: -0.175, Epsilon: 0.100\n",
      "Episode: 555000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 556000, Avg Reward: -0.145, Epsilon: 0.100\n",
      "Episode: 557000, Avg Reward: -0.203, Epsilon: 0.100\n",
      "Episode: 558000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 559000, Avg Reward: -0.164, Epsilon: 0.100\n",
      "Episode: 560000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 561000, Avg Reward: -0.232, Epsilon: 0.100\n",
      "Episode: 562000, Avg Reward: -0.198, Epsilon: 0.100\n",
      "Episode: 563000, Avg Reward: -0.235, Epsilon: 0.100\n",
      "Episode: 564000, Avg Reward: -0.251, Epsilon: 0.100\n",
      "Episode: 565000, Avg Reward: -0.215, Epsilon: 0.100\n",
      "Episode: 566000, Avg Reward: -0.172, Epsilon: 0.100\n",
      "Episode: 567000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 568000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 569000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 570000, Avg Reward: -0.167, Epsilon: 0.100\n",
      "Episode: 571000, Avg Reward: -0.178, Epsilon: 0.100\n",
      "Episode: 572000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 573000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 574000, Avg Reward: -0.223, Epsilon: 0.100\n",
      "Episode: 575000, Avg Reward: -0.180, Epsilon: 0.100\n",
      "Episode: 576000, Avg Reward: -0.219, Epsilon: 0.100\n",
      "Episode: 577000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 578000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 579000, Avg Reward: -0.220, Epsilon: 0.100\n",
      "Episode: 580000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 581000, Avg Reward: -0.182, Epsilon: 0.100\n",
      "Episode: 582000, Avg Reward: -0.120, Epsilon: 0.100\n",
      "Episode: 583000, Avg Reward: -0.193, Epsilon: 0.100\n",
      "Episode: 584000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 585000, Avg Reward: -0.215, Epsilon: 0.100\n",
      "Episode: 586000, Avg Reward: -0.158, Epsilon: 0.100\n",
      "Episode: 587000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 588000, Avg Reward: -0.180, Epsilon: 0.100\n",
      "Episode: 589000, Avg Reward: -0.189, Epsilon: 0.100\n",
      "Episode: 590000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 591000, Avg Reward: -0.202, Epsilon: 0.100\n",
      "Episode: 592000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 593000, Avg Reward: -0.144, Epsilon: 0.100\n",
      "Episode: 594000, Avg Reward: -0.238, Epsilon: 0.100\n",
      "Episode: 595000, Avg Reward: -0.253, Epsilon: 0.100\n",
      "Episode: 596000, Avg Reward: -0.162, Epsilon: 0.100\n",
      "Episode: 597000, Avg Reward: -0.181, Epsilon: 0.100\n",
      "Episode: 598000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 599000, Avg Reward: -0.239, Epsilon: 0.100\n",
      "Episode: 600000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 601000, Avg Reward: -0.190, Epsilon: 0.100\n",
      "Episode: 602000, Avg Reward: -0.240, Epsilon: 0.100\n",
      "Episode: 603000, Avg Reward: -0.161, Epsilon: 0.100\n",
      "Episode: 604000, Avg Reward: -0.215, Epsilon: 0.100\n",
      "Episode: 605000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 606000, Avg Reward: -0.197, Epsilon: 0.100\n",
      "Episode: 607000, Avg Reward: -0.162, Epsilon: 0.100\n",
      "Episode: 608000, Avg Reward: -0.154, Epsilon: 0.100\n",
      "Episode: 609000, Avg Reward: -0.244, Epsilon: 0.100\n",
      "Episode: 610000, Avg Reward: -0.214, Epsilon: 0.100\n",
      "Episode: 611000, Avg Reward: -0.161, Epsilon: 0.100\n",
      "Episode: 612000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 613000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 614000, Avg Reward: -0.161, Epsilon: 0.100\n",
      "Episode: 615000, Avg Reward: -0.217, Epsilon: 0.100\n",
      "Episode: 616000, Avg Reward: -0.125, Epsilon: 0.100\n",
      "Episode: 617000, Avg Reward: -0.182, Epsilon: 0.100\n",
      "Episode: 618000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 619000, Avg Reward: -0.239, Epsilon: 0.100\n",
      "Episode: 620000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 621000, Avg Reward: -0.190, Epsilon: 0.100\n",
      "Episode: 622000, Avg Reward: -0.189, Epsilon: 0.100\n",
      "Episode: 623000, Avg Reward: -0.202, Epsilon: 0.100\n",
      "Episode: 624000, Avg Reward: -0.200, Epsilon: 0.100\n",
      "Episode: 625000, Avg Reward: -0.176, Epsilon: 0.100\n",
      "Episode: 626000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 627000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 628000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 629000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 630000, Avg Reward: -0.235, Epsilon: 0.100\n",
      "Episode: 631000, Avg Reward: -0.213, Epsilon: 0.100\n",
      "Episode: 632000, Avg Reward: -0.161, Epsilon: 0.100\n",
      "Episode: 633000, Avg Reward: -0.133, Epsilon: 0.100\n",
      "Episode: 634000, Avg Reward: -0.193, Epsilon: 0.100\n",
      "Episode: 635000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 636000, Avg Reward: -0.175, Epsilon: 0.100\n",
      "Episode: 637000, Avg Reward: -0.242, Epsilon: 0.100\n",
      "Episode: 638000, Avg Reward: -0.171, Epsilon: 0.100\n",
      "Episode: 639000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 640000, Avg Reward: -0.217, Epsilon: 0.100\n",
      "Episode: 641000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 642000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 643000, Avg Reward: -0.203, Epsilon: 0.100\n",
      "Episode: 644000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 645000, Avg Reward: -0.197, Epsilon: 0.100\n",
      "Episode: 646000, Avg Reward: -0.156, Epsilon: 0.100\n",
      "Episode: 647000, Avg Reward: -0.148, Epsilon: 0.100\n",
      "Episode: 648000, Avg Reward: -0.178, Epsilon: 0.100\n",
      "Episode: 649000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 650000, Avg Reward: -0.173, Epsilon: 0.100\n",
      "Episode: 651000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 652000, Avg Reward: -0.158, Epsilon: 0.100\n",
      "Episode: 653000, Avg Reward: -0.156, Epsilon: 0.100\n",
      "Episode: 654000, Avg Reward: -0.237, Epsilon: 0.100\n",
      "Episode: 655000, Avg Reward: -0.205, Epsilon: 0.100\n",
      "Episode: 656000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 657000, Avg Reward: -0.218, Epsilon: 0.100\n",
      "Episode: 658000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 659000, Avg Reward: -0.139, Epsilon: 0.100\n",
      "Episode: 660000, Avg Reward: -0.202, Epsilon: 0.100\n",
      "Episode: 661000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 662000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 663000, Avg Reward: -0.154, Epsilon: 0.100\n",
      "Episode: 664000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 665000, Avg Reward: -0.199, Epsilon: 0.100\n",
      "Episode: 666000, Avg Reward: -0.239, Epsilon: 0.100\n",
      "Episode: 667000, Avg Reward: -0.237, Epsilon: 0.100\n",
      "Episode: 668000, Avg Reward: -0.230, Epsilon: 0.100\n",
      "Episode: 669000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 670000, Avg Reward: -0.198, Epsilon: 0.100\n",
      "Episode: 671000, Avg Reward: -0.180, Epsilon: 0.100\n",
      "Episode: 672000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 673000, Avg Reward: -0.228, Epsilon: 0.100\n",
      "Episode: 674000, Avg Reward: -0.179, Epsilon: 0.100\n",
      "Episode: 675000, Avg Reward: -0.146, Epsilon: 0.100\n",
      "Episode: 676000, Avg Reward: -0.208, Epsilon: 0.100\n",
      "Episode: 677000, Avg Reward: -0.235, Epsilon: 0.100\n",
      "Episode: 678000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 679000, Avg Reward: -0.222, Epsilon: 0.100\n",
      "Episode: 680000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 681000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 682000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 683000, Avg Reward: -0.157, Epsilon: 0.100\n",
      "Episode: 684000, Avg Reward: -0.220, Epsilon: 0.100\n",
      "Episode: 685000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 686000, Avg Reward: -0.214, Epsilon: 0.100\n",
      "Episode: 687000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 688000, Avg Reward: -0.208, Epsilon: 0.100\n",
      "Episode: 689000, Avg Reward: -0.218, Epsilon: 0.100\n",
      "Episode: 690000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 691000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 692000, Avg Reward: -0.159, Epsilon: 0.100\n",
      "Episode: 693000, Avg Reward: -0.241, Epsilon: 0.100\n",
      "Episode: 694000, Avg Reward: -0.224, Epsilon: 0.100\n",
      "Episode: 695000, Avg Reward: -0.181, Epsilon: 0.100\n",
      "Episode: 696000, Avg Reward: -0.177, Epsilon: 0.100\n",
      "Episode: 697000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 698000, Avg Reward: -0.225, Epsilon: 0.100\n",
      "Episode: 699000, Avg Reward: -0.132, Epsilon: 0.100\n",
      "Episode: 700000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 701000, Avg Reward: -0.126, Epsilon: 0.100\n",
      "Episode: 702000, Avg Reward: -0.187, Epsilon: 0.100\n",
      "Episode: 703000, Avg Reward: -0.205, Epsilon: 0.100\n",
      "Episode: 704000, Avg Reward: -0.213, Epsilon: 0.100\n",
      "Episode: 705000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 706000, Avg Reward: -0.177, Epsilon: 0.100\n",
      "Episode: 707000, Avg Reward: -0.222, Epsilon: 0.100\n",
      "Episode: 708000, Avg Reward: -0.162, Epsilon: 0.100\n",
      "Episode: 709000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 710000, Avg Reward: -0.223, Epsilon: 0.100\n",
      "Episode: 711000, Avg Reward: -0.139, Epsilon: 0.100\n",
      "Episode: 712000, Avg Reward: -0.182, Epsilon: 0.100\n",
      "Episode: 713000, Avg Reward: -0.170, Epsilon: 0.100\n",
      "Episode: 714000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 715000, Avg Reward: -0.200, Epsilon: 0.100\n",
      "Episode: 716000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 717000, Avg Reward: -0.176, Epsilon: 0.100\n",
      "Episode: 718000, Avg Reward: -0.164, Epsilon: 0.100\n",
      "Episode: 719000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 720000, Avg Reward: -0.157, Epsilon: 0.100\n",
      "Episode: 721000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 722000, Avg Reward: -0.225, Epsilon: 0.100\n",
      "Episode: 723000, Avg Reward: -0.203, Epsilon: 0.100\n",
      "Episode: 724000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 725000, Avg Reward: -0.235, Epsilon: 0.100\n",
      "Episode: 726000, Avg Reward: -0.140, Epsilon: 0.100\n",
      "Episode: 727000, Avg Reward: -0.233, Epsilon: 0.100\n",
      "Episode: 728000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 729000, Avg Reward: -0.144, Epsilon: 0.100\n",
      "Episode: 730000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 731000, Avg Reward: -0.200, Epsilon: 0.100\n",
      "Episode: 732000, Avg Reward: -0.239, Epsilon: 0.100\n",
      "Episode: 733000, Avg Reward: -0.198, Epsilon: 0.100\n",
      "Episode: 734000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 735000, Avg Reward: -0.149, Epsilon: 0.100\n",
      "Episode: 736000, Avg Reward: -0.208, Epsilon: 0.100\n",
      "Episode: 737000, Avg Reward: -0.214, Epsilon: 0.100\n",
      "Episode: 738000, Avg Reward: -0.184, Epsilon: 0.100\n",
      "Episode: 739000, Avg Reward: -0.184, Epsilon: 0.100\n",
      "Episode: 740000, Avg Reward: -0.200, Epsilon: 0.100\n",
      "Episode: 741000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 742000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 743000, Avg Reward: -0.182, Epsilon: 0.100\n",
      "Episode: 744000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 745000, Avg Reward: -0.165, Epsilon: 0.100\n",
      "Episode: 746000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 747000, Avg Reward: -0.152, Epsilon: 0.100\n",
      "Episode: 748000, Avg Reward: -0.218, Epsilon: 0.100\n",
      "Episode: 749000, Avg Reward: -0.143, Epsilon: 0.100\n",
      "Episode: 750000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 751000, Avg Reward: -0.181, Epsilon: 0.100\n",
      "Episode: 752000, Avg Reward: -0.203, Epsilon: 0.100\n",
      "Episode: 753000, Avg Reward: -0.257, Epsilon: 0.100\n",
      "Episode: 754000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 755000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 756000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 757000, Avg Reward: -0.221, Epsilon: 0.100\n",
      "Episode: 758000, Avg Reward: -0.199, Epsilon: 0.100\n",
      "Episode: 759000, Avg Reward: -0.155, Epsilon: 0.100\n",
      "Episode: 760000, Avg Reward: -0.213, Epsilon: 0.100\n",
      "Episode: 761000, Avg Reward: -0.167, Epsilon: 0.100\n",
      "Episode: 762000, Avg Reward: -0.180, Epsilon: 0.100\n",
      "Episode: 763000, Avg Reward: -0.228, Epsilon: 0.100\n",
      "Episode: 764000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 765000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 766000, Avg Reward: -0.213, Epsilon: 0.100\n",
      "Episode: 767000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 768000, Avg Reward: -0.187, Epsilon: 0.100\n",
      "Episode: 769000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 770000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 771000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 772000, Avg Reward: -0.168, Epsilon: 0.100\n",
      "Episode: 773000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 774000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 775000, Avg Reward: -0.219, Epsilon: 0.100\n",
      "Episode: 776000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 777000, Avg Reward: -0.167, Epsilon: 0.100\n",
      "Episode: 778000, Avg Reward: -0.221, Epsilon: 0.100\n",
      "Episode: 779000, Avg Reward: -0.252, Epsilon: 0.100\n",
      "Episode: 780000, Avg Reward: -0.167, Epsilon: 0.100\n",
      "Episode: 781000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 782000, Avg Reward: -0.184, Epsilon: 0.100\n",
      "Episode: 783000, Avg Reward: -0.250, Epsilon: 0.100\n",
      "Episode: 784000, Avg Reward: -0.232, Epsilon: 0.100\n",
      "Episode: 785000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 786000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 787000, Avg Reward: -0.223, Epsilon: 0.100\n",
      "Episode: 788000, Avg Reward: -0.148, Epsilon: 0.100\n",
      "Episode: 789000, Avg Reward: -0.161, Epsilon: 0.100\n",
      "Episode: 790000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 791000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 792000, Avg Reward: -0.167, Epsilon: 0.100\n",
      "Episode: 793000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 794000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 795000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 796000, Avg Reward: -0.182, Epsilon: 0.100\n",
      "Episode: 797000, Avg Reward: -0.184, Epsilon: 0.100\n",
      "Episode: 798000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 799000, Avg Reward: -0.220, Epsilon: 0.100\n",
      "Episode: 800000, Avg Reward: -0.158, Epsilon: 0.100\n",
      "Episode: 801000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 802000, Avg Reward: -0.255, Epsilon: 0.100\n",
      "Episode: 803000, Avg Reward: -0.235, Epsilon: 0.100\n",
      "Episode: 804000, Avg Reward: -0.155, Epsilon: 0.100\n",
      "Episode: 805000, Avg Reward: -0.159, Epsilon: 0.100\n",
      "Episode: 806000, Avg Reward: -0.225, Epsilon: 0.100\n",
      "Episode: 807000, Avg Reward: -0.224, Epsilon: 0.100\n",
      "Episode: 808000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 809000, Avg Reward: -0.225, Epsilon: 0.100\n",
      "Episode: 810000, Avg Reward: -0.197, Epsilon: 0.100\n",
      "Episode: 811000, Avg Reward: -0.178, Epsilon: 0.100\n",
      "Episode: 812000, Avg Reward: -0.208, Epsilon: 0.100\n",
      "Episode: 813000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 814000, Avg Reward: -0.233, Epsilon: 0.100\n",
      "Episode: 815000, Avg Reward: -0.180, Epsilon: 0.100\n",
      "Episode: 816000, Avg Reward: -0.248, Epsilon: 0.100\n",
      "Episode: 817000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 818000, Avg Reward: -0.157, Epsilon: 0.100\n",
      "Episode: 819000, Avg Reward: -0.170, Epsilon: 0.100\n",
      "Episode: 820000, Avg Reward: -0.181, Epsilon: 0.100\n",
      "Episode: 821000, Avg Reward: -0.180, Epsilon: 0.100\n",
      "Episode: 822000, Avg Reward: -0.152, Epsilon: 0.100\n",
      "Episode: 823000, Avg Reward: -0.179, Epsilon: 0.100\n",
      "Episode: 824000, Avg Reward: -0.099, Epsilon: 0.100\n",
      "Episode: 825000, Avg Reward: -0.205, Epsilon: 0.100\n",
      "Episode: 826000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 827000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 828000, Avg Reward: -0.153, Epsilon: 0.100\n",
      "Episode: 829000, Avg Reward: -0.203, Epsilon: 0.100\n",
      "Episode: 830000, Avg Reward: -0.189, Epsilon: 0.100\n",
      "Episode: 831000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 832000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 833000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 834000, Avg Reward: -0.250, Epsilon: 0.100\n",
      "Episode: 835000, Avg Reward: -0.223, Epsilon: 0.100\n",
      "Episode: 836000, Avg Reward: -0.214, Epsilon: 0.100\n",
      "Episode: 837000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 838000, Avg Reward: -0.224, Epsilon: 0.100\n",
      "Episode: 839000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 840000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 841000, Avg Reward: -0.168, Epsilon: 0.100\n",
      "Episode: 842000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 843000, Avg Reward: -0.162, Epsilon: 0.100\n",
      "Episode: 844000, Avg Reward: -0.176, Epsilon: 0.100\n",
      "Episode: 845000, Avg Reward: -0.235, Epsilon: 0.100\n",
      "Episode: 846000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 847000, Avg Reward: -0.171, Epsilon: 0.100\n",
      "Episode: 848000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 849000, Avg Reward: -0.187, Epsilon: 0.100\n",
      "Episode: 850000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 851000, Avg Reward: -0.198, Epsilon: 0.100\n",
      "Episode: 852000, Avg Reward: -0.198, Epsilon: 0.100\n",
      "Episode: 853000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 854000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 855000, Avg Reward: -0.228, Epsilon: 0.100\n",
      "Episode: 856000, Avg Reward: -0.248, Epsilon: 0.100\n",
      "Episode: 857000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 858000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 859000, Avg Reward: -0.197, Epsilon: 0.100\n",
      "Episode: 860000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 861000, Avg Reward: -0.190, Epsilon: 0.100\n",
      "Episode: 862000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 863000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 864000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 865000, Avg Reward: -0.190, Epsilon: 0.100\n",
      "Episode: 866000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 867000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 868000, Avg Reward: -0.163, Epsilon: 0.100\n",
      "Episode: 869000, Avg Reward: -0.165, Epsilon: 0.100\n",
      "Episode: 870000, Avg Reward: -0.189, Epsilon: 0.100\n",
      "Episode: 871000, Avg Reward: -0.227, Epsilon: 0.100\n",
      "Episode: 872000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 873000, Avg Reward: -0.203, Epsilon: 0.100\n",
      "Episode: 874000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 875000, Avg Reward: -0.241, Epsilon: 0.100\n",
      "Episode: 876000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 877000, Avg Reward: -0.172, Epsilon: 0.100\n",
      "Episode: 878000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 879000, Avg Reward: -0.182, Epsilon: 0.100\n",
      "Episode: 880000, Avg Reward: -0.187, Epsilon: 0.100\n",
      "Episode: 881000, Avg Reward: -0.160, Epsilon: 0.100\n",
      "Episode: 882000, Avg Reward: -0.251, Epsilon: 0.100\n",
      "Episode: 883000, Avg Reward: -0.156, Epsilon: 0.100\n",
      "Episode: 884000, Avg Reward: -0.213, Epsilon: 0.100\n",
      "Episode: 885000, Avg Reward: -0.200, Epsilon: 0.100\n",
      "Episode: 886000, Avg Reward: -0.182, Epsilon: 0.100\n",
      "Episode: 887000, Avg Reward: -0.190, Epsilon: 0.100\n",
      "Episode: 888000, Avg Reward: -0.262, Epsilon: 0.100\n",
      "Episode: 889000, Avg Reward: -0.131, Epsilon: 0.100\n",
      "Episode: 890000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 891000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 892000, Avg Reward: -0.227, Epsilon: 0.100\n",
      "Episode: 893000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 894000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 895000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 896000, Avg Reward: -0.231, Epsilon: 0.100\n",
      "Episode: 897000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 898000, Avg Reward: -0.175, Epsilon: 0.100\n",
      "Episode: 899000, Avg Reward: -0.243, Epsilon: 0.100\n",
      "Episode: 900000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 901000, Avg Reward: -0.187, Epsilon: 0.100\n",
      "Episode: 902000, Avg Reward: -0.251, Epsilon: 0.100\n",
      "Episode: 903000, Avg Reward: -0.219, Epsilon: 0.100\n",
      "Episode: 904000, Avg Reward: -0.202, Epsilon: 0.100\n",
      "Episode: 905000, Avg Reward: -0.173, Epsilon: 0.100\n",
      "Episode: 906000, Avg Reward: -0.143, Epsilon: 0.100\n",
      "Episode: 907000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 908000, Avg Reward: -0.179, Epsilon: 0.100\n",
      "Episode: 909000, Avg Reward: -0.267, Epsilon: 0.100\n",
      "Episode: 910000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 911000, Avg Reward: -0.233, Epsilon: 0.100\n",
      "Episode: 912000, Avg Reward: -0.146, Epsilon: 0.100\n",
      "Episode: 913000, Avg Reward: -0.178, Epsilon: 0.100\n",
      "Episode: 914000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 915000, Avg Reward: -0.189, Epsilon: 0.100\n",
      "Episode: 916000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 917000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 918000, Avg Reward: -0.188, Epsilon: 0.100\n",
      "Episode: 919000, Avg Reward: -0.211, Epsilon: 0.100\n",
      "Episode: 920000, Avg Reward: -0.200, Epsilon: 0.100\n",
      "Episode: 921000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 922000, Avg Reward: -0.147, Epsilon: 0.100\n",
      "Episode: 923000, Avg Reward: -0.217, Epsilon: 0.100\n",
      "Episode: 924000, Avg Reward: -0.191, Epsilon: 0.100\n",
      "Episode: 925000, Avg Reward: -0.229, Epsilon: 0.100\n",
      "Episode: 926000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 927000, Avg Reward: -0.210, Epsilon: 0.100\n",
      "Episode: 928000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 929000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 930000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 931000, Avg Reward: -0.235, Epsilon: 0.100\n",
      "Episode: 932000, Avg Reward: -0.143, Epsilon: 0.100\n",
      "Episode: 933000, Avg Reward: -0.133, Epsilon: 0.100\n",
      "Episode: 934000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 935000, Avg Reward: -0.243, Epsilon: 0.100\n",
      "Episode: 936000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 937000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 938000, Avg Reward: -0.225, Epsilon: 0.100\n",
      "Episode: 939000, Avg Reward: -0.220, Epsilon: 0.100\n",
      "Episode: 940000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 941000, Avg Reward: -0.176, Epsilon: 0.100\n",
      "Episode: 942000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 943000, Avg Reward: -0.139, Epsilon: 0.100\n",
      "Episode: 944000, Avg Reward: -0.192, Epsilon: 0.100\n",
      "Episode: 945000, Avg Reward: -0.194, Epsilon: 0.100\n",
      "Episode: 946000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 947000, Avg Reward: -0.195, Epsilon: 0.100\n",
      "Episode: 948000, Avg Reward: -0.205, Epsilon: 0.100\n",
      "Episode: 949000, Avg Reward: -0.164, Epsilon: 0.100\n",
      "Episode: 950000, Avg Reward: -0.164, Epsilon: 0.100\n",
      "Episode: 951000, Avg Reward: -0.183, Epsilon: 0.100\n",
      "Episode: 952000, Avg Reward: -0.190, Epsilon: 0.100\n",
      "Episode: 953000, Avg Reward: -0.159, Epsilon: 0.100\n",
      "Episode: 954000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 955000, Avg Reward: -0.227, Epsilon: 0.100\n",
      "Episode: 956000, Avg Reward: -0.181, Epsilon: 0.100\n",
      "Episode: 957000, Avg Reward: -0.219, Epsilon: 0.100\n",
      "Episode: 958000, Avg Reward: -0.232, Epsilon: 0.100\n",
      "Episode: 959000, Avg Reward: -0.150, Epsilon: 0.100\n",
      "Episode: 960000, Avg Reward: -0.196, Epsilon: 0.100\n",
      "Episode: 961000, Avg Reward: -0.225, Epsilon: 0.100\n",
      "Episode: 962000, Avg Reward: -0.165, Epsilon: 0.100\n",
      "Episode: 963000, Avg Reward: -0.162, Epsilon: 0.100\n",
      "Episode: 964000, Avg Reward: -0.155, Epsilon: 0.100\n",
      "Episode: 965000, Avg Reward: -0.177, Epsilon: 0.100\n",
      "Episode: 966000, Avg Reward: -0.173, Epsilon: 0.100\n",
      "Episode: 967000, Avg Reward: -0.218, Epsilon: 0.100\n",
      "Episode: 968000, Avg Reward: -0.220, Epsilon: 0.100\n",
      "Episode: 969000, Avg Reward: -0.137, Epsilon: 0.100\n",
      "Episode: 970000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 971000, Avg Reward: -0.232, Epsilon: 0.100\n",
      "Episode: 972000, Avg Reward: -0.185, Epsilon: 0.100\n",
      "Episode: 973000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 974000, Avg Reward: -0.224, Epsilon: 0.100\n",
      "Episode: 975000, Avg Reward: -0.186, Epsilon: 0.100\n",
      "Episode: 976000, Avg Reward: -0.207, Epsilon: 0.100\n",
      "Episode: 977000, Avg Reward: -0.257, Epsilon: 0.100\n",
      "Episode: 978000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 979000, Avg Reward: -0.181, Epsilon: 0.100\n",
      "Episode: 980000, Avg Reward: -0.209, Epsilon: 0.100\n",
      "Episode: 981000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 982000, Avg Reward: -0.190, Epsilon: 0.100\n",
      "Episode: 983000, Avg Reward: -0.212, Epsilon: 0.100\n",
      "Episode: 984000, Avg Reward: -0.166, Epsilon: 0.100\n",
      "Episode: 985000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 986000, Avg Reward: -0.197, Epsilon: 0.100\n",
      "Episode: 987000, Avg Reward: -0.169, Epsilon: 0.100\n",
      "Episode: 988000, Avg Reward: -0.204, Epsilon: 0.100\n",
      "Episode: 989000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 990000, Avg Reward: -0.135, Epsilon: 0.100\n",
      "Episode: 991000, Avg Reward: -0.201, Epsilon: 0.100\n",
      "Episode: 992000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 993000, Avg Reward: -0.174, Epsilon: 0.100\n",
      "Episode: 994000, Avg Reward: -0.256, Epsilon: 0.100\n",
      "Episode: 995000, Avg Reward: -0.216, Epsilon: 0.100\n",
      "Episode: 996000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 997000, Avg Reward: -0.206, Epsilon: 0.100\n",
      "Episode: 998000, Avg Reward: -0.168, Epsilon: 0.100\n",
      "Episode: 999000, Avg Reward: -0.168, Epsilon: 0.100\n",
      "Episode: 1000000, Avg Reward: -0.201, Epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        action = Q_agent.get_action(env, obs)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        Q_agent.update(obs, action, reward, terminated, next_obs)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    Q_agent.decay_epsilon()\n",
    "    rewards.append(total_reward)\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        avg_reward = np.mean(rewards[-1000:])\n",
    "        print(f\"Episode: {episode+1}, Avg Reward: {avg_reward:.3f}, Epsilon: {Q_agent.epsilon:.3f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6e15480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_record(Q_table, env_name):\n",
    "\n",
    "    # Create environment with video recording\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")  # For recording\n",
    "    env = gym.wrappers.RecordVideo(env, video_dir, episode_trigger=lambda e: True)\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(5):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Greedy policy (no exploration)\n",
    "            action = np.argmax(Q_table[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Videos saved in: {video_dir}\")\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a54a7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 1.0\n",
      "Episode 2: Total Reward = 1.0\n",
      "Episode 3: Total Reward = 1.0\n",
      "Episode 4: Total Reward = -1.0\n",
      "Episode 5: Total Reward = -1.0\n",
      "Videos saved in: videos_blackjack_ql\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, -1.0, -1.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_record(Q_agent.q_values,\"Blackjack-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f97a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da028f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acdc507",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vals = np.array([[1, 2, 3], [1, -1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d040fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selctor = ptan.actions.ArgmaxActionSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e82fbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selctor(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b7988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc925f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1, selector=ptan.actions.ArgmaxActionSelector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b7e70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selctor(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ec774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
