{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1108101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fdf153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tic_tac_toe/tictactoemoves.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67dd3d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>player</th>\n",
       "      <th>move</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C1  C2  C3  C4  C5  C6  C7  C8  C9  player  move\n",
       "0   1   1   1   1   0   0   1   0   0      -1    -1\n",
       "1   1   1   1   1   0   0   0   1   0      -1    -1\n",
       "2   1   1   1   1   0   0   0   0   1      -1    -1\n",
       "3   1   1   1   1   0   0   0  -1  -1      -1    -1\n",
       "4   1   1   1   1   0   0  -1   0  -1      -1    -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1eea6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'player']].values\n",
    "y = df['move'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86b1199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  0  0  1  0  0 -1]\n",
      " [ 1  1  1  1  0  0  0  1  0 -1]\n",
      " [ 1  1  1  1  0  0  0  0  1 -1]\n",
      " [ 1  1  1  1  0  0  0 -1 -1 -1]\n",
      " [ 1  1  1  1  0  0 -1  0 -1 -1]\n",
      " [ 1  1  1  1  0  0 -1 -1  0 -1]\n",
      " [ 1  1  1  1  0 -1  0  0 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ec1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(y[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cd0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = np.zeros([len(y), 10])\n",
    "\n",
    "for i in range(0, len(y)):\n",
    "    if(y[i] == -1):\n",
    "        y_encoded[i][0] = 1.0\n",
    "    else:\n",
    "        y_encoded[i][int(y[i])] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca3237a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa7f7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b0e13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5478, 10]), torch.Size([5478, 10]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099b7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908f4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c719c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83457a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class TicTacToeTrainer(nn.Module):\\n\\n    def __init__(self, input_size=10, hidden_size=128, output_size=10):\\n        super(TicTacToeTrainer, self).__init__()\\n        self.fc1 = nn.Linear(input_size, hidden_size)\\n        self.bn1 = nn.BatchNorm1d(hidden_size)\\n        self.dropout1 = nn.Dropout(0.5)   # 50% dropout\\n\\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\\n        self.bn2 = nn.BatchNorm1d(hidden_size)\\n        self.dropout2 = nn.Dropout(0.5)\\n\\n        self.fc3 = nn.Linear(hidden_size, output_size)\\n        self.relu = nn.ReLU()\\n\\n\\n    def forward(self, x):\\n        x = self.relu(self.bn1(self.fc1(x)))\\n        x = self.dropout1(x)\\n        x = self.relu(self.bn2(self.fc2(x)))\\n        x = self.dropout2(x)\\n        return self.fc3(x)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class TicTacToeTrainer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=10, hidden_size=128, output_size=10):\n",
    "        super(TicTacToeTrainer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)   # 50% dropout\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc3(x)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95935424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeTrainer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TicTacToeTrainer, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(10, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 10) #logits as output\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00c98bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNet = TicTacToeTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96f4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SNet = SNet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e38600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.to(device)\n",
    "#y_train = y_train.to(device)\n",
    "#X_test = X_test.to(device)\n",
    "#y_test = y_test.to(device)\n",
    "X_train = X_tensor.to(device)\n",
    "y_train = y_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a32741ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  torch.optim.Adam(SNet.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1533d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 2.269651\n",
      "Epoch [20/10000], Loss: 2.217851\n",
      "Epoch [30/10000], Loss: 2.136278\n",
      "Epoch [40/10000], Loss: 2.015136\n",
      "Epoch [50/10000], Loss: 1.870151\n",
      "Epoch [60/10000], Loss: 1.721081\n",
      "Epoch [70/10000], Loss: 1.584432\n",
      "Epoch [80/10000], Loss: 1.464432\n",
      "Epoch [90/10000], Loss: 1.355095\n",
      "Epoch [100/10000], Loss: 1.259590\n",
      "Epoch [110/10000], Loss: 1.177744\n",
      "Epoch [120/10000], Loss: 1.106914\n",
      "Epoch [130/10000], Loss: 1.049495\n",
      "Epoch [140/10000], Loss: 1.007549\n",
      "Epoch [150/10000], Loss: 0.975959\n",
      "Epoch [160/10000], Loss: 0.948550\n",
      "Epoch [170/10000], Loss: 0.923673\n",
      "Epoch [180/10000], Loss: 0.901376\n",
      "Epoch [190/10000], Loss: 0.880226\n",
      "Epoch [200/10000], Loss: 0.858440\n",
      "Epoch [210/10000], Loss: 0.835407\n",
      "Epoch [220/10000], Loss: 0.812607\n",
      "Epoch [230/10000], Loss: 0.792878\n",
      "Epoch [240/10000], Loss: 0.776143\n",
      "Epoch [250/10000], Loss: 0.761246\n",
      "Epoch [260/10000], Loss: 0.747837\n",
      "Epoch [270/10000], Loss: 0.736103\n",
      "Epoch [280/10000], Loss: 0.726008\n",
      "Epoch [290/10000], Loss: 0.717188\n",
      "Epoch [300/10000], Loss: 0.709121\n",
      "Epoch [310/10000], Loss: 0.701300\n",
      "Epoch [320/10000], Loss: 0.693690\n",
      "Epoch [330/10000], Loss: 0.686179\n",
      "Epoch [340/10000], Loss: 0.678703\n",
      "Epoch [350/10000], Loss: 0.671201\n",
      "Epoch [360/10000], Loss: 0.663731\n",
      "Epoch [370/10000], Loss: 0.656579\n",
      "Epoch [380/10000], Loss: 0.649360\n",
      "Epoch [390/10000], Loss: 0.642467\n",
      "Epoch [400/10000], Loss: 0.635718\n",
      "Epoch [410/10000], Loss: 0.629336\n",
      "Epoch [420/10000], Loss: 0.622479\n",
      "Epoch [430/10000], Loss: 0.615949\n",
      "Epoch [440/10000], Loss: 0.609554\n",
      "Epoch [450/10000], Loss: 0.603228\n",
      "Epoch [460/10000], Loss: 0.596913\n",
      "Epoch [470/10000], Loss: 0.590920\n",
      "Epoch [480/10000], Loss: 0.584536\n",
      "Epoch [490/10000], Loss: 0.578416\n",
      "Epoch [500/10000], Loss: 0.572157\n",
      "Epoch [510/10000], Loss: 0.566055\n",
      "Epoch [520/10000], Loss: 0.559486\n",
      "Epoch [530/10000], Loss: 0.553182\n",
      "Epoch [540/10000], Loss: 0.546877\n",
      "Epoch [550/10000], Loss: 0.540893\n",
      "Epoch [560/10000], Loss: 0.534432\n",
      "Epoch [570/10000], Loss: 0.528138\n",
      "Epoch [580/10000], Loss: 0.522179\n",
      "Epoch [590/10000], Loss: 0.516243\n",
      "Epoch [600/10000], Loss: 0.510707\n",
      "Epoch [610/10000], Loss: 0.505288\n",
      "Epoch [620/10000], Loss: 0.500636\n",
      "Epoch [630/10000], Loss: 0.495246\n",
      "Epoch [640/10000], Loss: 0.490333\n",
      "Epoch [650/10000], Loss: 0.485516\n",
      "Epoch [660/10000], Loss: 0.480851\n",
      "Epoch [670/10000], Loss: 0.476732\n",
      "Epoch [680/10000], Loss: 0.471995\n",
      "Epoch [690/10000], Loss: 0.467720\n",
      "Epoch [700/10000], Loss: 0.464249\n",
      "Epoch [710/10000], Loss: 0.459877\n",
      "Epoch [720/10000], Loss: 0.456086\n",
      "Epoch [730/10000], Loss: 0.452451\n",
      "Epoch [740/10000], Loss: 0.448749\n",
      "Epoch [750/10000], Loss: 0.445038\n",
      "Epoch [760/10000], Loss: 0.441448\n",
      "Epoch [770/10000], Loss: 0.438367\n",
      "Epoch [780/10000], Loss: 0.435434\n",
      "Epoch [790/10000], Loss: 0.431832\n",
      "Epoch [800/10000], Loss: 0.428551\n",
      "Epoch [810/10000], Loss: 0.425426\n",
      "Epoch [820/10000], Loss: 0.423219\n",
      "Epoch [830/10000], Loss: 0.419945\n",
      "Epoch [840/10000], Loss: 0.416936\n",
      "Epoch [850/10000], Loss: 0.414169\n",
      "Epoch [860/10000], Loss: 0.411333\n",
      "Epoch [870/10000], Loss: 0.408743\n",
      "Epoch [880/10000], Loss: 0.406237\n",
      "Epoch [890/10000], Loss: 0.403523\n",
      "Epoch [900/10000], Loss: 0.400715\n",
      "Epoch [910/10000], Loss: 0.397997\n",
      "Epoch [920/10000], Loss: 0.396243\n",
      "Epoch [930/10000], Loss: 0.393311\n",
      "Epoch [940/10000], Loss: 0.390857\n",
      "Epoch [950/10000], Loss: 0.388363\n",
      "Epoch [960/10000], Loss: 0.386048\n",
      "Epoch [970/10000], Loss: 0.384596\n",
      "Epoch [980/10000], Loss: 0.381706\n",
      "Epoch [990/10000], Loss: 0.379607\n",
      "Epoch [1000/10000], Loss: 0.377559\n",
      "Epoch [1010/10000], Loss: 0.375559\n",
      "Epoch [1020/10000], Loss: 0.373483\n",
      "Epoch [1030/10000], Loss: 0.371344\n",
      "Epoch [1040/10000], Loss: 0.369605\n",
      "Epoch [1050/10000], Loss: 0.367280\n",
      "Epoch [1060/10000], Loss: 0.365498\n",
      "Epoch [1070/10000], Loss: 0.363683\n",
      "Epoch [1080/10000], Loss: 0.361847\n",
      "Epoch [1090/10000], Loss: 0.359916\n",
      "Epoch [1100/10000], Loss: 0.357933\n",
      "Epoch [1110/10000], Loss: 0.356164\n",
      "Epoch [1120/10000], Loss: 0.355303\n",
      "Epoch [1130/10000], Loss: 0.352720\n",
      "Epoch [1140/10000], Loss: 0.350883\n",
      "Epoch [1150/10000], Loss: 0.349203\n",
      "Epoch [1160/10000], Loss: 0.348686\n",
      "Epoch [1170/10000], Loss: 0.346428\n",
      "Epoch [1180/10000], Loss: 0.344215\n",
      "Epoch [1190/10000], Loss: 0.342526\n",
      "Epoch [1200/10000], Loss: 0.340919\n",
      "Epoch [1210/10000], Loss: 0.339310\n",
      "Epoch [1220/10000], Loss: 0.339960\n",
      "Epoch [1230/10000], Loss: 0.336273\n",
      "Epoch [1240/10000], Loss: 0.334854\n",
      "Epoch [1250/10000], Loss: 0.333219\n",
      "Epoch [1260/10000], Loss: 0.331625\n",
      "Epoch [1270/10000], Loss: 0.330579\n",
      "Epoch [1280/10000], Loss: 0.328720\n",
      "Epoch [1290/10000], Loss: 0.327140\n",
      "Epoch [1300/10000], Loss: 0.325882\n",
      "Epoch [1310/10000], Loss: 0.324509\n",
      "Epoch [1320/10000], Loss: 0.323091\n",
      "Epoch [1330/10000], Loss: 0.321662\n",
      "Epoch [1340/10000], Loss: 0.320358\n",
      "Epoch [1350/10000], Loss: 0.318982\n",
      "Epoch [1360/10000], Loss: 0.317218\n",
      "Epoch [1370/10000], Loss: 0.315749\n",
      "Epoch [1380/10000], Loss: 0.314589\n",
      "Epoch [1390/10000], Loss: 0.312931\n",
      "Epoch [1400/10000], Loss: 0.311553\n",
      "Epoch [1410/10000], Loss: 0.310824\n",
      "Epoch [1420/10000], Loss: 0.309123\n",
      "Epoch [1430/10000], Loss: 0.307458\n",
      "Epoch [1440/10000], Loss: 0.306010\n",
      "Epoch [1450/10000], Loss: 0.304749\n",
      "Epoch [1460/10000], Loss: 0.303450\n",
      "Epoch [1470/10000], Loss: 0.302764\n",
      "Epoch [1480/10000], Loss: 0.300866\n",
      "Epoch [1490/10000], Loss: 0.299702\n",
      "Epoch [1500/10000], Loss: 0.298761\n",
      "Epoch [1510/10000], Loss: 0.296964\n",
      "Epoch [1520/10000], Loss: 0.295917\n",
      "Epoch [1530/10000], Loss: 0.294478\n",
      "Epoch [1540/10000], Loss: 0.293297\n",
      "Epoch [1550/10000], Loss: 0.292646\n",
      "Epoch [1560/10000], Loss: 0.290871\n",
      "Epoch [1570/10000], Loss: 0.289918\n",
      "Epoch [1580/10000], Loss: 0.288368\n",
      "Epoch [1590/10000], Loss: 0.287363\n",
      "Epoch [1600/10000], Loss: 0.286049\n",
      "Epoch [1610/10000], Loss: 0.285004\n",
      "Epoch [1620/10000], Loss: 0.283829\n",
      "Epoch [1630/10000], Loss: 0.282537\n",
      "Epoch [1640/10000], Loss: 0.281316\n",
      "Epoch [1650/10000], Loss: 0.279996\n",
      "Epoch [1660/10000], Loss: 0.279476\n",
      "Epoch [1670/10000], Loss: 0.277927\n",
      "Epoch [1680/10000], Loss: 0.276445\n",
      "Epoch [1690/10000], Loss: 0.275221\n",
      "Epoch [1700/10000], Loss: 0.274272\n",
      "Epoch [1710/10000], Loss: 0.272669\n",
      "Epoch [1720/10000], Loss: 0.271658\n",
      "Epoch [1730/10000], Loss: 0.270800\n",
      "Epoch [1740/10000], Loss: 0.268963\n",
      "Epoch [1750/10000], Loss: 0.267539\n",
      "Epoch [1760/10000], Loss: 0.266109\n",
      "Epoch [1770/10000], Loss: 0.264732\n",
      "Epoch [1780/10000], Loss: 0.263549\n",
      "Epoch [1790/10000], Loss: 0.262247\n",
      "Epoch [1800/10000], Loss: 0.260695\n",
      "Epoch [1810/10000], Loss: 0.260106\n",
      "Epoch [1820/10000], Loss: 0.259328\n",
      "Epoch [1830/10000], Loss: 0.257654\n",
      "Epoch [1840/10000], Loss: 0.256422\n",
      "Epoch [1850/10000], Loss: 0.255152\n",
      "Epoch [1860/10000], Loss: 0.254262\n",
      "Epoch [1870/10000], Loss: 0.252875\n",
      "Epoch [1880/10000], Loss: 0.251724\n",
      "Epoch [1890/10000], Loss: 0.251079\n",
      "Epoch [1900/10000], Loss: 0.249726\n",
      "Epoch [1910/10000], Loss: 0.248471\n",
      "Epoch [1920/10000], Loss: 0.247711\n",
      "Epoch [1930/10000], Loss: 0.247363\n",
      "Epoch [1940/10000], Loss: 0.246057\n",
      "Epoch [1950/10000], Loss: 0.244640\n",
      "Epoch [1960/10000], Loss: 0.243856\n",
      "Epoch [1970/10000], Loss: 0.242500\n",
      "Epoch [1980/10000], Loss: 0.242037\n",
      "Epoch [1990/10000], Loss: 0.241435\n",
      "Epoch [2000/10000], Loss: 0.239702\n",
      "Epoch [2010/10000], Loss: 0.238615\n",
      "Epoch [2020/10000], Loss: 0.238590\n",
      "Epoch [2030/10000], Loss: 0.237194\n",
      "Epoch [2040/10000], Loss: 0.236131\n",
      "Epoch [2050/10000], Loss: 0.235128\n",
      "Epoch [2060/10000], Loss: 0.233977\n",
      "Epoch [2070/10000], Loss: 0.232857\n",
      "Epoch [2080/10000], Loss: 0.232653\n",
      "Epoch [2090/10000], Loss: 0.230842\n",
      "Epoch [2100/10000], Loss: 0.230003\n",
      "Epoch [2110/10000], Loss: 0.228856\n",
      "Epoch [2120/10000], Loss: 0.227698\n",
      "Epoch [2130/10000], Loss: 0.226849\n",
      "Epoch [2140/10000], Loss: 0.225994\n",
      "Epoch [2150/10000], Loss: 0.224253\n",
      "Epoch [2160/10000], Loss: 0.223426\n",
      "Epoch [2170/10000], Loss: 0.222476\n",
      "Epoch [2180/10000], Loss: 0.221388\n",
      "Epoch [2190/10000], Loss: 0.219738\n",
      "Epoch [2200/10000], Loss: 0.218758\n",
      "Epoch [2210/10000], Loss: 0.218150\n",
      "Epoch [2220/10000], Loss: 0.216976\n",
      "Epoch [2230/10000], Loss: 0.215576\n",
      "Epoch [2240/10000], Loss: 0.214806\n",
      "Epoch [2250/10000], Loss: 0.213452\n",
      "Epoch [2260/10000], Loss: 0.212914\n",
      "Epoch [2270/10000], Loss: 0.212338\n",
      "Epoch [2280/10000], Loss: 0.211010\n",
      "Epoch [2290/10000], Loss: 0.209824\n",
      "Epoch [2300/10000], Loss: 0.209187\n",
      "Epoch [2310/10000], Loss: 0.207625\n",
      "Epoch [2320/10000], Loss: 0.206669\n",
      "Epoch [2330/10000], Loss: 0.206571\n",
      "Epoch [2340/10000], Loss: 0.204921\n",
      "Epoch [2350/10000], Loss: 0.204072\n",
      "Epoch [2360/10000], Loss: 0.203085\n",
      "Epoch [2370/10000], Loss: 0.202408\n",
      "Epoch [2380/10000], Loss: 0.201194\n",
      "Epoch [2390/10000], Loss: 0.200427\n",
      "Epoch [2400/10000], Loss: 0.200234\n",
      "Epoch [2410/10000], Loss: 0.198914\n",
      "Epoch [2420/10000], Loss: 0.197852\n",
      "Epoch [2430/10000], Loss: 0.196968\n",
      "Epoch [2440/10000], Loss: 0.195899\n",
      "Epoch [2450/10000], Loss: 0.195133\n",
      "Epoch [2460/10000], Loss: 0.194605\n",
      "Epoch [2470/10000], Loss: 0.193370\n",
      "Epoch [2480/10000], Loss: 0.192485\n",
      "Epoch [2490/10000], Loss: 0.192225\n",
      "Epoch [2500/10000], Loss: 0.191097\n",
      "Epoch [2510/10000], Loss: 0.190282\n",
      "Epoch [2520/10000], Loss: 0.188909\n",
      "Epoch [2530/10000], Loss: 0.188438\n",
      "Epoch [2540/10000], Loss: 0.187869\n",
      "Epoch [2550/10000], Loss: 0.186879\n",
      "Epoch [2560/10000], Loss: 0.185581\n",
      "Epoch [2570/10000], Loss: 0.185415\n",
      "Epoch [2580/10000], Loss: 0.184266\n",
      "Epoch [2590/10000], Loss: 0.183343\n",
      "Epoch [2600/10000], Loss: 0.182543\n",
      "Epoch [2610/10000], Loss: 0.181699\n",
      "Epoch [2620/10000], Loss: 0.183616\n",
      "Epoch [2630/10000], Loss: 0.180905\n",
      "Epoch [2640/10000], Loss: 0.179398\n",
      "Epoch [2650/10000], Loss: 0.178563\n",
      "Epoch [2660/10000], Loss: 0.178232\n",
      "Epoch [2670/10000], Loss: 0.177146\n",
      "Epoch [2680/10000], Loss: 0.176747\n",
      "Epoch [2690/10000], Loss: 0.176048\n",
      "Epoch [2700/10000], Loss: 0.175398\n",
      "Epoch [2710/10000], Loss: 0.174387\n",
      "Epoch [2720/10000], Loss: 0.173456\n",
      "Epoch [2730/10000], Loss: 0.173449\n",
      "Epoch [2740/10000], Loss: 0.172623\n",
      "Epoch [2750/10000], Loss: 0.171510\n",
      "Epoch [2760/10000], Loss: 0.171612\n",
      "Epoch [2770/10000], Loss: 0.170086\n",
      "Epoch [2780/10000], Loss: 0.169483\n",
      "Epoch [2790/10000], Loss: 0.168938\n",
      "Epoch [2800/10000], Loss: 0.168207\n",
      "Epoch [2810/10000], Loss: 0.167639\n",
      "Epoch [2820/10000], Loss: 0.166987\n",
      "Epoch [2830/10000], Loss: 0.166117\n",
      "Epoch [2840/10000], Loss: 0.165323\n",
      "Epoch [2850/10000], Loss: 0.164808\n",
      "Epoch [2860/10000], Loss: 0.164382\n",
      "Epoch [2870/10000], Loss: 0.163721\n",
      "Epoch [2880/10000], Loss: 0.163325\n",
      "Epoch [2890/10000], Loss: 0.162006\n",
      "Epoch [2900/10000], Loss: 0.161529\n",
      "Epoch [2910/10000], Loss: 0.160875\n",
      "Epoch [2920/10000], Loss: 0.160179\n",
      "Epoch [2930/10000], Loss: 0.159884\n",
      "Epoch [2940/10000], Loss: 0.160164\n",
      "Epoch [2950/10000], Loss: 0.158465\n",
      "Epoch [2960/10000], Loss: 0.157894\n",
      "Epoch [2970/10000], Loss: 0.157062\n",
      "Epoch [2980/10000], Loss: 0.156717\n",
      "Epoch [2990/10000], Loss: 0.155902\n",
      "Epoch [3000/10000], Loss: 0.155337\n",
      "Epoch [3010/10000], Loss: 0.154953\n",
      "Epoch [3020/10000], Loss: 0.155480\n",
      "Epoch [3030/10000], Loss: 0.153776\n",
      "Epoch [3040/10000], Loss: 0.153038\n",
      "Epoch [3050/10000], Loss: 0.152496\n",
      "Epoch [3060/10000], Loss: 0.151524\n",
      "Epoch [3070/10000], Loss: 0.151293\n",
      "Epoch [3080/10000], Loss: 0.150505\n",
      "Epoch [3090/10000], Loss: 0.149988\n",
      "Epoch [3100/10000], Loss: 0.149887\n",
      "Epoch [3110/10000], Loss: 0.149920\n",
      "Epoch [3120/10000], Loss: 0.148169\n",
      "Epoch [3130/10000], Loss: 0.147842\n",
      "Epoch [3140/10000], Loss: 0.146944\n",
      "Epoch [3150/10000], Loss: 0.146833\n",
      "Epoch [3160/10000], Loss: 0.146312\n",
      "Epoch [3170/10000], Loss: 0.145645\n",
      "Epoch [3180/10000], Loss: 0.145114\n",
      "Epoch [3190/10000], Loss: 0.145249\n",
      "Epoch [3200/10000], Loss: 0.143649\n",
      "Epoch [3210/10000], Loss: 0.143571\n",
      "Epoch [3220/10000], Loss: 0.142937\n",
      "Epoch [3230/10000], Loss: 0.142330\n",
      "Epoch [3240/10000], Loss: 0.142452\n",
      "Epoch [3250/10000], Loss: 0.141254\n",
      "Epoch [3260/10000], Loss: 0.140856\n",
      "Epoch [3270/10000], Loss: 0.140596\n",
      "Epoch [3280/10000], Loss: 0.139465\n",
      "Epoch [3290/10000], Loss: 0.139093\n",
      "Epoch [3300/10000], Loss: 0.138768\n",
      "Epoch [3310/10000], Loss: 0.139526\n",
      "Epoch [3320/10000], Loss: 0.138354\n",
      "Epoch [3330/10000], Loss: 0.137384\n",
      "Epoch [3340/10000], Loss: 0.136333\n",
      "Epoch [3350/10000], Loss: 0.136189\n",
      "Epoch [3360/10000], Loss: 0.135770\n",
      "Epoch [3370/10000], Loss: 0.134777\n",
      "Epoch [3380/10000], Loss: 0.134629\n",
      "Epoch [3390/10000], Loss: 0.133941\n",
      "Epoch [3400/10000], Loss: 0.133470\n",
      "Epoch [3410/10000], Loss: 0.132915\n",
      "Epoch [3420/10000], Loss: 0.132436\n",
      "Epoch [3430/10000], Loss: 0.131701\n",
      "Epoch [3440/10000], Loss: 0.131789\n",
      "Epoch [3450/10000], Loss: 0.131638\n",
      "Epoch [3460/10000], Loss: 0.130442\n",
      "Epoch [3470/10000], Loss: 0.129970\n",
      "Epoch [3480/10000], Loss: 0.129849\n",
      "Epoch [3490/10000], Loss: 0.128747\n",
      "Epoch [3500/10000], Loss: 0.128460\n",
      "Epoch [3510/10000], Loss: 0.128543\n",
      "Epoch [3520/10000], Loss: 0.127216\n",
      "Epoch [3530/10000], Loss: 0.126681\n",
      "Epoch [3540/10000], Loss: 0.126834\n",
      "Epoch [3550/10000], Loss: 0.126126\n",
      "Epoch [3560/10000], Loss: 0.125906\n",
      "Epoch [3570/10000], Loss: 0.124984\n",
      "Epoch [3580/10000], Loss: 0.124331\n",
      "Epoch [3590/10000], Loss: 0.124010\n",
      "Epoch [3600/10000], Loss: 0.123442\n",
      "Epoch [3610/10000], Loss: 0.123306\n",
      "Epoch [3620/10000], Loss: 0.122486\n",
      "Epoch [3630/10000], Loss: 0.123829\n",
      "Epoch [3640/10000], Loss: 0.122418\n",
      "Epoch [3650/10000], Loss: 0.121404\n",
      "Epoch [3660/10000], Loss: 0.120623\n",
      "Epoch [3670/10000], Loss: 0.120198\n",
      "Epoch [3680/10000], Loss: 0.120136\n",
      "Epoch [3690/10000], Loss: 0.120641\n",
      "Epoch [3700/10000], Loss: 0.119399\n",
      "Epoch [3710/10000], Loss: 0.118518\n",
      "Epoch [3720/10000], Loss: 0.117865\n",
      "Epoch [3730/10000], Loss: 0.118072\n",
      "Epoch [3740/10000], Loss: 0.117674\n",
      "Epoch [3750/10000], Loss: 0.116627\n",
      "Epoch [3760/10000], Loss: 0.117164\n",
      "Epoch [3770/10000], Loss: 0.116452\n",
      "Epoch [3780/10000], Loss: 0.115762\n",
      "Epoch [3790/10000], Loss: 0.115463\n",
      "Epoch [3800/10000], Loss: 0.114517\n",
      "Epoch [3810/10000], Loss: 0.114016\n",
      "Epoch [3820/10000], Loss: 0.113655\n",
      "Epoch [3830/10000], Loss: 0.115617\n",
      "Epoch [3840/10000], Loss: 0.113294\n",
      "Epoch [3850/10000], Loss: 0.112615\n",
      "Epoch [3860/10000], Loss: 0.112077\n",
      "Epoch [3870/10000], Loss: 0.111813\n",
      "Epoch [3880/10000], Loss: 0.111414\n",
      "Epoch [3890/10000], Loss: 0.111034\n",
      "Epoch [3900/10000], Loss: 0.110488\n",
      "Epoch [3910/10000], Loss: 0.110166\n",
      "Epoch [3920/10000], Loss: 0.110842\n",
      "Epoch [3930/10000], Loss: 0.110061\n",
      "Epoch [3940/10000], Loss: 0.109374\n",
      "Epoch [3950/10000], Loss: 0.108519\n",
      "Epoch [3960/10000], Loss: 0.108026\n",
      "Epoch [3970/10000], Loss: 0.108567\n",
      "Epoch [3980/10000], Loss: 0.107178\n",
      "Epoch [3990/10000], Loss: 0.107183\n",
      "Epoch [4000/10000], Loss: 0.106223\n",
      "Epoch [4010/10000], Loss: 0.106449\n",
      "Epoch [4020/10000], Loss: 0.106404\n",
      "Epoch [4030/10000], Loss: 0.105496\n",
      "Epoch [4040/10000], Loss: 0.105387\n",
      "Epoch [4050/10000], Loss: 0.104877\n",
      "Epoch [4060/10000], Loss: 0.104165\n",
      "Epoch [4070/10000], Loss: 0.103670\n",
      "Epoch [4080/10000], Loss: 0.103617\n",
      "Epoch [4090/10000], Loss: 0.104981\n",
      "Epoch [4100/10000], Loss: 0.103221\n",
      "Epoch [4110/10000], Loss: 0.102251\n",
      "Epoch [4120/10000], Loss: 0.101769\n",
      "Epoch [4130/10000], Loss: 0.102422\n",
      "Epoch [4140/10000], Loss: 0.101112\n",
      "Epoch [4150/10000], Loss: 0.101144\n",
      "Epoch [4160/10000], Loss: 0.100203\n",
      "Epoch [4170/10000], Loss: 0.100636\n",
      "Epoch [4180/10000], Loss: 0.099628\n",
      "Epoch [4190/10000], Loss: 0.099768\n",
      "Epoch [4200/10000], Loss: 0.098787\n",
      "Epoch [4210/10000], Loss: 0.099120\n",
      "Epoch [4220/10000], Loss: 0.098679\n",
      "Epoch [4230/10000], Loss: 0.097638\n",
      "Epoch [4240/10000], Loss: 0.097980\n",
      "Epoch [4250/10000], Loss: 0.097089\n",
      "Epoch [4260/10000], Loss: 0.097474\n",
      "Epoch [4270/10000], Loss: 0.096845\n",
      "Epoch [4280/10000], Loss: 0.096106\n",
      "Epoch [4290/10000], Loss: 0.095768\n",
      "Epoch [4300/10000], Loss: 0.096330\n",
      "Epoch [4310/10000], Loss: 0.095349\n",
      "Epoch [4320/10000], Loss: 0.095083\n",
      "Epoch [4330/10000], Loss: 0.094385\n",
      "Epoch [4340/10000], Loss: 0.093970\n",
      "Epoch [4350/10000], Loss: 0.093925\n",
      "Epoch [4360/10000], Loss: 0.093385\n",
      "Epoch [4370/10000], Loss: 0.092871\n",
      "Epoch [4380/10000], Loss: 0.092476\n",
      "Epoch [4390/10000], Loss: 0.092186\n",
      "Epoch [4400/10000], Loss: 0.094390\n",
      "Epoch [4410/10000], Loss: 0.092165\n",
      "Epoch [4420/10000], Loss: 0.091317\n",
      "Epoch [4430/10000], Loss: 0.090803\n",
      "Epoch [4440/10000], Loss: 0.090617\n",
      "Epoch [4450/10000], Loss: 0.090230\n",
      "Epoch [4460/10000], Loss: 0.089908\n",
      "Epoch [4470/10000], Loss: 0.089790\n",
      "Epoch [4480/10000], Loss: 0.089234\n",
      "Epoch [4490/10000], Loss: 0.088849\n",
      "Epoch [4500/10000], Loss: 0.088889\n",
      "Epoch [4510/10000], Loss: 0.089007\n",
      "Epoch [4520/10000], Loss: 0.087825\n",
      "Epoch [4530/10000], Loss: 0.088018\n",
      "Epoch [4540/10000], Loss: 0.086937\n",
      "Epoch [4550/10000], Loss: 0.087153\n",
      "Epoch [4560/10000], Loss: 0.086709\n",
      "Epoch [4570/10000], Loss: 0.086261\n",
      "Epoch [4580/10000], Loss: 0.085872\n",
      "Epoch [4590/10000], Loss: 0.086133\n",
      "Epoch [4600/10000], Loss: 0.086252\n",
      "Epoch [4610/10000], Loss: 0.084880\n",
      "Epoch [4620/10000], Loss: 0.084920\n",
      "Epoch [4630/10000], Loss: 0.084797\n",
      "Epoch [4640/10000], Loss: 0.084116\n",
      "Epoch [4650/10000], Loss: 0.084351\n",
      "Epoch [4660/10000], Loss: 0.083401\n",
      "Epoch [4670/10000], Loss: 0.083372\n",
      "Epoch [4680/10000], Loss: 0.083091\n",
      "Epoch [4690/10000], Loss: 0.083114\n",
      "Epoch [4700/10000], Loss: 0.082214\n",
      "Epoch [4710/10000], Loss: 0.082138\n",
      "Epoch [4720/10000], Loss: 0.082217\n",
      "Epoch [4730/10000], Loss: 0.081469\n",
      "Epoch [4740/10000], Loss: 0.081758\n",
      "Epoch [4750/10000], Loss: 0.081337\n",
      "Epoch [4760/10000], Loss: 0.080721\n",
      "Epoch [4770/10000], Loss: 0.080390\n",
      "Epoch [4780/10000], Loss: 0.079644\n",
      "Epoch [4790/10000], Loss: 0.079982\n",
      "Epoch [4800/10000], Loss: 0.079567\n",
      "Epoch [4810/10000], Loss: 0.078876\n",
      "Epoch [4820/10000], Loss: 0.079175\n",
      "Epoch [4830/10000], Loss: 0.079466\n",
      "Epoch [4840/10000], Loss: 0.078177\n",
      "Epoch [4850/10000], Loss: 0.077854\n",
      "Epoch [4860/10000], Loss: 0.077671\n",
      "Epoch [4870/10000], Loss: 0.077121\n",
      "Epoch [4880/10000], Loss: 0.077134\n",
      "Epoch [4890/10000], Loss: 0.077494\n",
      "Epoch [4900/10000], Loss: 0.076435\n",
      "Epoch [4910/10000], Loss: 0.076900\n",
      "Epoch [4920/10000], Loss: 0.075598\n",
      "Epoch [4930/10000], Loss: 0.075627\n",
      "Epoch [4940/10000], Loss: 0.075672\n",
      "Epoch [4950/10000], Loss: 0.074772\n",
      "Epoch [4960/10000], Loss: 0.074578\n",
      "Epoch [4970/10000], Loss: 0.075140\n",
      "Epoch [4980/10000], Loss: 0.074861\n",
      "Epoch [4990/10000], Loss: 0.074018\n",
      "Epoch [5000/10000], Loss: 0.073758\n",
      "Epoch [5010/10000], Loss: 0.073297\n",
      "Epoch [5020/10000], Loss: 0.073460\n",
      "Epoch [5030/10000], Loss: 0.073199\n",
      "Epoch [5040/10000], Loss: 0.072588\n",
      "Epoch [5050/10000], Loss: 0.072104\n",
      "Epoch [5060/10000], Loss: 0.071598\n",
      "Epoch [5070/10000], Loss: 0.071627\n",
      "Epoch [5080/10000], Loss: 0.071033\n",
      "Epoch [5090/10000], Loss: 0.070835\n",
      "Epoch [5100/10000], Loss: 0.073689\n",
      "Epoch [5110/10000], Loss: 0.071456\n",
      "Epoch [5120/10000], Loss: 0.070612\n",
      "Epoch [5130/10000], Loss: 0.069912\n",
      "Epoch [5140/10000], Loss: 0.069806\n",
      "Epoch [5150/10000], Loss: 0.070119\n",
      "Epoch [5160/10000], Loss: 0.069169\n",
      "Epoch [5170/10000], Loss: 0.068849\n",
      "Epoch [5180/10000], Loss: 0.068751\n",
      "Epoch [5190/10000], Loss: 0.068361\n",
      "Epoch [5200/10000], Loss: 0.068088\n",
      "Epoch [5210/10000], Loss: 0.068521\n",
      "Epoch [5220/10000], Loss: 0.068395\n",
      "Epoch [5230/10000], Loss: 0.067355\n",
      "Epoch [5240/10000], Loss: 0.067364\n",
      "Epoch [5250/10000], Loss: 0.067146\n",
      "Epoch [5260/10000], Loss: 0.066574\n",
      "Epoch [5270/10000], Loss: 0.067036\n",
      "Epoch [5280/10000], Loss: 0.066812\n",
      "Epoch [5290/10000], Loss: 0.065905\n",
      "Epoch [5300/10000], Loss: 0.065690\n",
      "Epoch [5310/10000], Loss: 0.065426\n",
      "Epoch [5320/10000], Loss: 0.065518\n",
      "Epoch [5330/10000], Loss: 0.065248\n",
      "Epoch [5340/10000], Loss: 0.064756\n",
      "Epoch [5350/10000], Loss: 0.064551\n",
      "Epoch [5360/10000], Loss: 0.064426\n",
      "Epoch [5370/10000], Loss: 0.064005\n",
      "Epoch [5380/10000], Loss: 0.063912\n",
      "Epoch [5390/10000], Loss: 0.064344\n",
      "Epoch [5400/10000], Loss: 0.063542\n",
      "Epoch [5410/10000], Loss: 0.063014\n",
      "Epoch [5420/10000], Loss: 0.063050\n",
      "Epoch [5430/10000], Loss: 0.063062\n",
      "Epoch [5440/10000], Loss: 0.062496\n",
      "Epoch [5450/10000], Loss: 0.062027\n",
      "Epoch [5460/10000], Loss: 0.063697\n",
      "Epoch [5470/10000], Loss: 0.062052\n",
      "Epoch [5480/10000], Loss: 0.061379\n",
      "Epoch [5490/10000], Loss: 0.061111\n",
      "Epoch [5500/10000], Loss: 0.060977\n",
      "Epoch [5510/10000], Loss: 0.060747\n",
      "Epoch [5520/10000], Loss: 0.060464\n",
      "Epoch [5530/10000], Loss: 0.060597\n",
      "Epoch [5540/10000], Loss: 0.059885\n",
      "Epoch [5550/10000], Loss: 0.059696\n",
      "Epoch [5560/10000], Loss: 0.062158\n",
      "Epoch [5570/10000], Loss: 0.059988\n",
      "Epoch [5580/10000], Loss: 0.059056\n",
      "Epoch [5590/10000], Loss: 0.058928\n",
      "Epoch [5600/10000], Loss: 0.058505\n",
      "Epoch [5610/10000], Loss: 0.058833\n",
      "Epoch [5620/10000], Loss: 0.058096\n",
      "Epoch [5630/10000], Loss: 0.057854\n",
      "Epoch [5640/10000], Loss: 0.058470\n",
      "Epoch [5650/10000], Loss: 0.057692\n",
      "Epoch [5660/10000], Loss: 0.057140\n",
      "Epoch [5670/10000], Loss: 0.057177\n",
      "Epoch [5680/10000], Loss: 0.057143\n",
      "Epoch [5690/10000], Loss: 0.057159\n",
      "Epoch [5700/10000], Loss: 0.056782\n",
      "Epoch [5710/10000], Loss: 0.056057\n",
      "Epoch [5720/10000], Loss: 0.056444\n",
      "Epoch [5730/10000], Loss: 0.056158\n",
      "Epoch [5740/10000], Loss: 0.055874\n",
      "Epoch [5750/10000], Loss: 0.055947\n",
      "Epoch [5760/10000], Loss: 0.055266\n",
      "Epoch [5770/10000], Loss: 0.055174\n",
      "Epoch [5780/10000], Loss: 0.054584\n",
      "Epoch [5790/10000], Loss: 0.054545\n",
      "Epoch [5800/10000], Loss: 0.054297\n",
      "Epoch [5810/10000], Loss: 0.054909\n",
      "Epoch [5820/10000], Loss: 0.054055\n",
      "Epoch [5830/10000], Loss: 0.054019\n",
      "Epoch [5840/10000], Loss: 0.053455\n",
      "Epoch [5850/10000], Loss: 0.053209\n",
      "Epoch [5860/10000], Loss: 0.053499\n",
      "Epoch [5870/10000], Loss: 0.053066\n",
      "Epoch [5880/10000], Loss: 0.052380\n",
      "Epoch [5890/10000], Loss: 0.052475\n",
      "Epoch [5900/10000], Loss: 0.052647\n",
      "Epoch [5910/10000], Loss: 0.053116\n",
      "Epoch [5920/10000], Loss: 0.052253\n",
      "Epoch [5930/10000], Loss: 0.051552\n",
      "Epoch [5940/10000], Loss: 0.051272\n",
      "Epoch [5950/10000], Loss: 0.051149\n",
      "Epoch [5960/10000], Loss: 0.050868\n",
      "Epoch [5970/10000], Loss: 0.051191\n",
      "Epoch [5980/10000], Loss: 0.050683\n",
      "Epoch [5990/10000], Loss: 0.050294\n",
      "Epoch [6000/10000], Loss: 0.050544\n",
      "Epoch [6010/10000], Loss: 0.050240\n",
      "Epoch [6020/10000], Loss: 0.049647\n",
      "Epoch [6030/10000], Loss: 0.051099\n",
      "Epoch [6040/10000], Loss: 0.049547\n",
      "Epoch [6050/10000], Loss: 0.049298\n",
      "Epoch [6060/10000], Loss: 0.049057\n",
      "Epoch [6070/10000], Loss: 0.048794\n",
      "Epoch [6080/10000], Loss: 0.048615\n",
      "Epoch [6090/10000], Loss: 0.048491\n",
      "Epoch [6100/10000], Loss: 0.048579\n",
      "Epoch [6110/10000], Loss: 0.048021\n",
      "Epoch [6120/10000], Loss: 0.047746\n",
      "Epoch [6130/10000], Loss: 0.048050\n",
      "Epoch [6140/10000], Loss: 0.047704\n",
      "Epoch [6150/10000], Loss: 0.048512\n",
      "Epoch [6160/10000], Loss: 0.047015\n",
      "Epoch [6170/10000], Loss: 0.047206\n",
      "Epoch [6180/10000], Loss: 0.046769\n",
      "Epoch [6190/10000], Loss: 0.046347\n",
      "Epoch [6200/10000], Loss: 0.046717\n",
      "Epoch [6210/10000], Loss: 0.047756\n",
      "Epoch [6220/10000], Loss: 0.046945\n",
      "Epoch [6230/10000], Loss: 0.045957\n",
      "Epoch [6240/10000], Loss: 0.045476\n",
      "Epoch [6250/10000], Loss: 0.045536\n",
      "Epoch [6260/10000], Loss: 0.045189\n",
      "Epoch [6270/10000], Loss: 0.045080\n",
      "Epoch [6280/10000], Loss: 0.045517\n",
      "Epoch [6290/10000], Loss: 0.045259\n",
      "Epoch [6300/10000], Loss: 0.044802\n",
      "Epoch [6310/10000], Loss: 0.044351\n",
      "Epoch [6320/10000], Loss: 0.044044\n",
      "Epoch [6330/10000], Loss: 0.044169\n",
      "Epoch [6340/10000], Loss: 0.046100\n",
      "Epoch [6350/10000], Loss: 0.044531\n",
      "Epoch [6360/10000], Loss: 0.043812\n",
      "Epoch [6370/10000], Loss: 0.043276\n",
      "Epoch [6380/10000], Loss: 0.043239\n",
      "Epoch [6390/10000], Loss: 0.043125\n",
      "Epoch [6400/10000], Loss: 0.042709\n",
      "Epoch [6410/10000], Loss: 0.042713\n",
      "Epoch [6420/10000], Loss: 0.042948\n",
      "Epoch [6430/10000], Loss: 0.042590\n",
      "Epoch [6440/10000], Loss: 0.042278\n",
      "Epoch [6450/10000], Loss: 0.042135\n",
      "Epoch [6460/10000], Loss: 0.041616\n",
      "Epoch [6470/10000], Loss: 0.041678\n",
      "Epoch [6480/10000], Loss: 0.041745\n",
      "Epoch [6490/10000], Loss: 0.041781\n",
      "Epoch [6500/10000], Loss: 0.041035\n",
      "Epoch [6510/10000], Loss: 0.041175\n",
      "Epoch [6520/10000], Loss: 0.040830\n",
      "Epoch [6530/10000], Loss: 0.041163\n",
      "Epoch [6540/10000], Loss: 0.042859\n",
      "Epoch [6550/10000], Loss: 0.041007\n",
      "Epoch [6560/10000], Loss: 0.040409\n",
      "Epoch [6570/10000], Loss: 0.040140\n",
      "Epoch [6580/10000], Loss: 0.039702\n",
      "Epoch [6590/10000], Loss: 0.039805\n",
      "Epoch [6600/10000], Loss: 0.039232\n",
      "Epoch [6610/10000], Loss: 0.039281\n",
      "Epoch [6620/10000], Loss: 0.039077\n",
      "Epoch [6630/10000], Loss: 0.039008\n",
      "Epoch [6640/10000], Loss: 0.038763\n",
      "Epoch [6650/10000], Loss: 0.038685\n",
      "Epoch [6660/10000], Loss: 0.038975\n",
      "Epoch [6670/10000], Loss: 0.039077\n",
      "Epoch [6680/10000], Loss: 0.038477\n",
      "Epoch [6690/10000], Loss: 0.037815\n",
      "Epoch [6700/10000], Loss: 0.038305\n",
      "Epoch [6710/10000], Loss: 0.037730\n",
      "Epoch [6720/10000], Loss: 0.037542\n",
      "Epoch [6730/10000], Loss: 0.037411\n",
      "Epoch [6740/10000], Loss: 0.037308\n",
      "Epoch [6750/10000], Loss: 0.037593\n",
      "Epoch [6760/10000], Loss: 0.036800\n",
      "Epoch [6770/10000], Loss: 0.037009\n",
      "Epoch [6780/10000], Loss: 0.036594\n",
      "Epoch [6790/10000], Loss: 0.036414\n",
      "Epoch [6800/10000], Loss: 0.036699\n",
      "Epoch [6810/10000], Loss: 0.038446\n",
      "Epoch [6820/10000], Loss: 0.036673\n",
      "Epoch [6830/10000], Loss: 0.036095\n",
      "Epoch [6840/10000], Loss: 0.035564\n",
      "Epoch [6850/10000], Loss: 0.035414\n",
      "Epoch [6860/10000], Loss: 0.035537\n",
      "Epoch [6870/10000], Loss: 0.035117\n",
      "Epoch [6880/10000], Loss: 0.034974\n",
      "Epoch [6890/10000], Loss: 0.035596\n",
      "Epoch [6900/10000], Loss: 0.035081\n",
      "Epoch [6910/10000], Loss: 0.034704\n",
      "Epoch [6920/10000], Loss: 0.034569\n",
      "Epoch [6930/10000], Loss: 0.034407\n",
      "Epoch [6940/10000], Loss: 0.034126\n",
      "Epoch [6950/10000], Loss: 0.035043\n",
      "Epoch [6960/10000], Loss: 0.034499\n",
      "Epoch [6970/10000], Loss: 0.033682\n",
      "Epoch [6980/10000], Loss: 0.033485\n",
      "Epoch [6990/10000], Loss: 0.033347\n",
      "Epoch [7000/10000], Loss: 0.033498\n",
      "Epoch [7010/10000], Loss: 0.033104\n",
      "Epoch [7020/10000], Loss: 0.033573\n",
      "Epoch [7030/10000], Loss: 0.032913\n",
      "Epoch [7040/10000], Loss: 0.032814\n",
      "Epoch [7050/10000], Loss: 0.032692\n",
      "Epoch [7060/10000], Loss: 0.032461\n",
      "Epoch [7070/10000], Loss: 0.032432\n",
      "Epoch [7080/10000], Loss: 0.034510\n",
      "Epoch [7090/10000], Loss: 0.032844\n",
      "Epoch [7100/10000], Loss: 0.032297\n",
      "Epoch [7110/10000], Loss: 0.031887\n",
      "Epoch [7120/10000], Loss: 0.031604\n",
      "Epoch [7130/10000], Loss: 0.031505\n",
      "Epoch [7140/10000], Loss: 0.031499\n",
      "Epoch [7150/10000], Loss: 0.031289\n",
      "Epoch [7160/10000], Loss: 0.031301\n",
      "Epoch [7170/10000], Loss: 0.030999\n",
      "Epoch [7180/10000], Loss: 0.031095\n",
      "Epoch [7190/10000], Loss: 0.030800\n",
      "Epoch [7200/10000], Loss: 0.031499\n",
      "Epoch [7210/10000], Loss: 0.030438\n",
      "Epoch [7220/10000], Loss: 0.030644\n",
      "Epoch [7230/10000], Loss: 0.030289\n",
      "Epoch [7240/10000], Loss: 0.030289\n",
      "Epoch [7250/10000], Loss: 0.030338\n",
      "Epoch [7260/10000], Loss: 0.030196\n",
      "Epoch [7270/10000], Loss: 0.029668\n",
      "Epoch [7280/10000], Loss: 0.030012\n",
      "Epoch [7290/10000], Loss: 0.029501\n",
      "Epoch [7300/10000], Loss: 0.029371\n",
      "Epoch [7310/10000], Loss: 0.029220\n",
      "Epoch [7320/10000], Loss: 0.030044\n",
      "Epoch [7330/10000], Loss: 0.030235\n",
      "Epoch [7340/10000], Loss: 0.028847\n",
      "Epoch [7350/10000], Loss: 0.028754\n",
      "Epoch [7360/10000], Loss: 0.028533\n",
      "Epoch [7370/10000], Loss: 0.028530\n",
      "Epoch [7380/10000], Loss: 0.028303\n",
      "Epoch [7390/10000], Loss: 0.028196\n",
      "Epoch [7400/10000], Loss: 0.028516\n",
      "Epoch [7410/10000], Loss: 0.028010\n",
      "Epoch [7420/10000], Loss: 0.028013\n",
      "Epoch [7430/10000], Loss: 0.027780\n",
      "Epoch [7440/10000], Loss: 0.027852\n",
      "Epoch [7450/10000], Loss: 0.027741\n",
      "Epoch [7460/10000], Loss: 0.027388\n",
      "Epoch [7470/10000], Loss: 0.027453\n",
      "Epoch [7480/10000], Loss: 0.027233\n",
      "Epoch [7490/10000], Loss: 0.027011\n",
      "Epoch [7500/10000], Loss: 0.027608\n",
      "Epoch [7510/10000], Loss: 0.027662\n",
      "Epoch [7520/10000], Loss: 0.026817\n",
      "Epoch [7530/10000], Loss: 0.026567\n",
      "Epoch [7540/10000], Loss: 0.026584\n",
      "Epoch [7550/10000], Loss: 0.026395\n",
      "Epoch [7560/10000], Loss: 0.026227\n",
      "Epoch [7570/10000], Loss: 0.026142\n",
      "Epoch [7580/10000], Loss: 0.027500\n",
      "Epoch [7590/10000], Loss: 0.026758\n",
      "Epoch [7600/10000], Loss: 0.026094\n",
      "Epoch [7610/10000], Loss: 0.025772\n",
      "Epoch [7620/10000], Loss: 0.025588\n",
      "Epoch [7630/10000], Loss: 0.025539\n",
      "Epoch [7640/10000], Loss: 0.025390\n",
      "Epoch [7650/10000], Loss: 0.025324\n",
      "Epoch [7660/10000], Loss: 0.025365\n",
      "Epoch [7670/10000], Loss: 0.025034\n",
      "Epoch [7680/10000], Loss: 0.025052\n",
      "Epoch [7690/10000], Loss: 0.025516\n",
      "Epoch [7700/10000], Loss: 0.025237\n",
      "Epoch [7710/10000], Loss: 0.024825\n",
      "Epoch [7720/10000], Loss: 0.024696\n",
      "Epoch [7730/10000], Loss: 0.024847\n",
      "Epoch [7740/10000], Loss: 0.028754\n",
      "Epoch [7750/10000], Loss: 0.203092\n",
      "Epoch [7760/10000], Loss: 0.130203\n",
      "Epoch [7770/10000], Loss: 0.041952\n",
      "Epoch [7780/10000], Loss: 0.029707\n",
      "Epoch [7790/10000], Loss: 0.025854\n",
      "Epoch [7800/10000], Loss: 0.025216\n",
      "Epoch [7810/10000], Loss: 0.024522\n",
      "Epoch [7820/10000], Loss: 0.024234\n",
      "Epoch [7830/10000], Loss: 0.024078\n",
      "Epoch [7840/10000], Loss: 0.023983\n",
      "Epoch [7850/10000], Loss: 0.023912\n",
      "Epoch [7860/10000], Loss: 0.023844\n",
      "Epoch [7870/10000], Loss: 0.023791\n",
      "Epoch [7880/10000], Loss: 0.023739\n",
      "Epoch [7890/10000], Loss: 0.023690\n",
      "Epoch [7900/10000], Loss: 0.023641\n",
      "Epoch [7910/10000], Loss: 0.023596\n",
      "Epoch [7920/10000], Loss: 0.023551\n",
      "Epoch [7930/10000], Loss: 0.023507\n",
      "Epoch [7940/10000], Loss: 0.023466\n",
      "Epoch [7950/10000], Loss: 0.023421\n",
      "Epoch [7960/10000], Loss: 0.023379\n",
      "Epoch [7970/10000], Loss: 0.023341\n",
      "Epoch [7980/10000], Loss: 0.023295\n",
      "Epoch [7990/10000], Loss: 0.023254\n",
      "Epoch [8000/10000], Loss: 0.023211\n",
      "Epoch [8010/10000], Loss: 0.023170\n",
      "Epoch [8020/10000], Loss: 0.023131\n",
      "Epoch [8030/10000], Loss: 0.023088\n",
      "Epoch [8040/10000], Loss: 0.023046\n",
      "Epoch [8050/10000], Loss: 0.023005\n",
      "Epoch [8060/10000], Loss: 0.022963\n",
      "Epoch [8070/10000], Loss: 0.022921\n",
      "Epoch [8080/10000], Loss: 0.022881\n",
      "Epoch [8090/10000], Loss: 0.022840\n",
      "Epoch [8100/10000], Loss: 0.022796\n",
      "Epoch [8110/10000], Loss: 0.022756\n",
      "Epoch [8120/10000], Loss: 0.022714\n",
      "Epoch [8130/10000], Loss: 0.022673\n",
      "Epoch [8140/10000], Loss: 0.022631\n",
      "Epoch [8150/10000], Loss: 0.022593\n",
      "Epoch [8160/10000], Loss: 0.022549\n",
      "Epoch [8170/10000], Loss: 0.022507\n",
      "Epoch [8180/10000], Loss: 0.022466\n",
      "Epoch [8190/10000], Loss: 0.022424\n",
      "Epoch [8200/10000], Loss: 0.022383\n",
      "Epoch [8210/10000], Loss: 0.022340\n",
      "Epoch [8220/10000], Loss: 0.022302\n",
      "Epoch [8230/10000], Loss: 0.022258\n",
      "Epoch [8240/10000], Loss: 0.022215\n",
      "Epoch [8250/10000], Loss: 0.022172\n",
      "Epoch [8260/10000], Loss: 0.022132\n",
      "Epoch [8270/10000], Loss: 0.022089\n",
      "Epoch [8280/10000], Loss: 0.022049\n",
      "Epoch [8290/10000], Loss: 0.022004\n",
      "Epoch [8300/10000], Loss: 0.021962\n",
      "Epoch [8310/10000], Loss: 0.021920\n",
      "Epoch [8320/10000], Loss: 0.021878\n",
      "Epoch [8330/10000], Loss: 0.021834\n",
      "Epoch [8340/10000], Loss: 0.021792\n",
      "Epoch [8350/10000], Loss: 0.021754\n",
      "Epoch [8360/10000], Loss: 0.021708\n",
      "Epoch [8370/10000], Loss: 0.021664\n",
      "Epoch [8380/10000], Loss: 0.021620\n",
      "Epoch [8390/10000], Loss: 0.021577\n",
      "Epoch [8400/10000], Loss: 0.021535\n",
      "Epoch [8410/10000], Loss: 0.021490\n",
      "Epoch [8420/10000], Loss: 0.021449\n",
      "Epoch [8430/10000], Loss: 0.021403\n",
      "Epoch [8440/10000], Loss: 0.021358\n",
      "Epoch [8450/10000], Loss: 0.021314\n",
      "Epoch [8460/10000], Loss: 0.021271\n",
      "Epoch [8470/10000], Loss: 0.021225\n",
      "Epoch [8480/10000], Loss: 0.021182\n",
      "Epoch [8490/10000], Loss: 0.021136\n",
      "Epoch [8500/10000], Loss: 0.021093\n",
      "Epoch [8510/10000], Loss: 0.021046\n",
      "Epoch [8520/10000], Loss: 0.021005\n",
      "Epoch [8530/10000], Loss: 0.020958\n",
      "Epoch [8540/10000], Loss: 0.020914\n",
      "Epoch [8550/10000], Loss: 0.020868\n",
      "Epoch [8560/10000], Loss: 0.020825\n",
      "Epoch [8570/10000], Loss: 0.020779\n",
      "Epoch [8580/10000], Loss: 0.020734\n",
      "Epoch [8590/10000], Loss: 0.020690\n",
      "Epoch [8600/10000], Loss: 0.020640\n",
      "Epoch [8610/10000], Loss: 0.020598\n",
      "Epoch [8620/10000], Loss: 0.020552\n",
      "Epoch [8630/10000], Loss: 0.020505\n",
      "Epoch [8640/10000], Loss: 0.020460\n",
      "Epoch [8650/10000], Loss: 0.020413\n",
      "Epoch [8660/10000], Loss: 0.020367\n",
      "Epoch [8670/10000], Loss: 0.020325\n",
      "Epoch [8680/10000], Loss: 0.020277\n",
      "Epoch [8690/10000], Loss: 0.020234\n",
      "Epoch [8700/10000], Loss: 0.020188\n",
      "Epoch [8710/10000], Loss: 0.020143\n",
      "Epoch [8720/10000], Loss: 0.020096\n",
      "Epoch [8730/10000], Loss: 0.020049\n",
      "Epoch [8740/10000], Loss: 0.020002\n",
      "Epoch [8750/10000], Loss: 0.019953\n",
      "Epoch [8760/10000], Loss: 0.019908\n",
      "Epoch [8770/10000], Loss: 0.019864\n",
      "Epoch [8780/10000], Loss: 0.019818\n",
      "Epoch [8790/10000], Loss: 0.019772\n",
      "Epoch [8800/10000], Loss: 0.019724\n",
      "Epoch [8810/10000], Loss: 0.019687\n",
      "Epoch [8820/10000], Loss: 0.019633\n",
      "Epoch [8830/10000], Loss: 0.019585\n",
      "Epoch [8840/10000], Loss: 0.019538\n",
      "Epoch [8850/10000], Loss: 0.019493\n",
      "Epoch [8860/10000], Loss: 0.019442\n",
      "Epoch [8870/10000], Loss: 0.019396\n",
      "Epoch [8880/10000], Loss: 0.019361\n",
      "Epoch [8890/10000], Loss: 0.019303\n",
      "Epoch [8900/10000], Loss: 0.019259\n",
      "Epoch [8910/10000], Loss: 0.019209\n",
      "Epoch [8920/10000], Loss: 0.019163\n",
      "Epoch [8930/10000], Loss: 0.019114\n",
      "Epoch [8940/10000], Loss: 0.019069\n",
      "Epoch [8950/10000], Loss: 0.019026\n",
      "Epoch [8960/10000], Loss: 0.018981\n",
      "Epoch [8970/10000], Loss: 0.018923\n",
      "Epoch [8980/10000], Loss: 0.018896\n",
      "Epoch [8990/10000], Loss: 0.018836\n",
      "Epoch [9000/10000], Loss: 0.018784\n",
      "Epoch [9010/10000], Loss: 0.018736\n",
      "Epoch [9020/10000], Loss: 0.018686\n",
      "Epoch [9030/10000], Loss: 0.018651\n",
      "Epoch [9040/10000], Loss: 0.018592\n",
      "Epoch [9050/10000], Loss: 0.018554\n",
      "Epoch [9060/10000], Loss: 0.018495\n",
      "Epoch [9070/10000], Loss: 0.018452\n",
      "Epoch [9080/10000], Loss: 0.018419\n",
      "Epoch [9090/10000], Loss: 0.018345\n",
      "Epoch [9100/10000], Loss: 0.018300\n",
      "Epoch [9110/10000], Loss: 0.018284\n",
      "Epoch [9120/10000], Loss: 0.018225\n",
      "Epoch [9130/10000], Loss: 0.018173\n",
      "Epoch [9140/10000], Loss: 0.018106\n",
      "Epoch [9150/10000], Loss: 0.018061\n",
      "Epoch [9160/10000], Loss: 0.018024\n",
      "Epoch [9170/10000], Loss: 0.017973\n",
      "Epoch [9180/10000], Loss: 0.017933\n",
      "Epoch [9190/10000], Loss: 0.017871\n",
      "Epoch [9200/10000], Loss: 0.017870\n",
      "Epoch [9210/10000], Loss: 0.017791\n",
      "Epoch [9220/10000], Loss: 0.017717\n",
      "Epoch [9230/10000], Loss: 0.017776\n",
      "Epoch [9240/10000], Loss: 0.017616\n",
      "Epoch [9250/10000], Loss: 0.017666\n",
      "Epoch [9260/10000], Loss: 0.017547\n",
      "Epoch [9270/10000], Loss: 0.017484\n",
      "Epoch [9280/10000], Loss: 0.017488\n",
      "Epoch [9290/10000], Loss: 0.017365\n",
      "Epoch [9300/10000], Loss: 0.017581\n",
      "Epoch [9310/10000], Loss: 0.017295\n",
      "Epoch [9320/10000], Loss: 0.017294\n",
      "Epoch [9330/10000], Loss: 0.017312\n",
      "Epoch [9340/10000], Loss: 0.017116\n",
      "Epoch [9350/10000], Loss: 0.017255\n",
      "Epoch [9360/10000], Loss: 0.017034\n",
      "Epoch [9370/10000], Loss: 0.017073\n",
      "Epoch [9380/10000], Loss: 0.017046\n",
      "Epoch [9390/10000], Loss: 0.016866\n",
      "Epoch [9400/10000], Loss: 0.016871\n",
      "Epoch [9410/10000], Loss: 0.017492\n",
      "Epoch [9420/10000], Loss: 0.017173\n",
      "Epoch [9430/10000], Loss: 0.016905\n",
      "Epoch [9440/10000], Loss: 0.016721\n",
      "Epoch [9450/10000], Loss: 0.016571\n",
      "Epoch [9460/10000], Loss: 0.016613\n",
      "Epoch [9470/10000], Loss: 0.016591\n",
      "Epoch [9480/10000], Loss: 0.016427\n",
      "Epoch [9490/10000], Loss: 0.016523\n",
      "Epoch [9500/10000], Loss: 0.016335\n",
      "Epoch [9510/10000], Loss: 0.016520\n",
      "Epoch [9520/10000], Loss: 0.016249\n",
      "Epoch [9530/10000], Loss: 0.016337\n",
      "Epoch [9540/10000], Loss: 0.016175\n",
      "Epoch [9550/10000], Loss: 0.016227\n",
      "Epoch [9560/10000], Loss: 0.016041\n",
      "Epoch [9570/10000], Loss: 0.016215\n",
      "Epoch [9580/10000], Loss: 0.015948\n",
      "Epoch [9590/10000], Loss: 0.016086\n",
      "Epoch [9600/10000], Loss: 0.015851\n",
      "Epoch [9610/10000], Loss: 0.015959\n",
      "Epoch [9620/10000], Loss: 0.015739\n",
      "Epoch [9630/10000], Loss: 0.015748\n",
      "Epoch [9640/10000], Loss: 0.016408\n",
      "Epoch [9650/10000], Loss: 0.016001\n",
      "Epoch [9660/10000], Loss: 0.015724\n",
      "Epoch [9670/10000], Loss: 0.015551\n",
      "Epoch [9680/10000], Loss: 0.015461\n",
      "Epoch [9690/10000], Loss: 0.015596\n",
      "Epoch [9700/10000], Loss: 0.015390\n",
      "Epoch [9710/10000], Loss: 0.015384\n",
      "Epoch [9720/10000], Loss: 0.015454\n",
      "Epoch [9730/10000], Loss: 0.015206\n",
      "Epoch [9740/10000], Loss: 0.015397\n",
      "Epoch [9750/10000], Loss: 0.015288\n",
      "Epoch [9760/10000], Loss: 0.015079\n",
      "Epoch [9770/10000], Loss: 0.015064\n",
      "Epoch [9780/10000], Loss: 0.014973\n",
      "Epoch [9790/10000], Loss: 0.015491\n",
      "Epoch [9800/10000], Loss: 0.014993\n",
      "Epoch [9810/10000], Loss: 0.014832\n",
      "Epoch [9820/10000], Loss: 0.014882\n",
      "Epoch [9830/10000], Loss: 0.014835\n",
      "Epoch [9840/10000], Loss: 0.014718\n",
      "Epoch [9850/10000], Loss: 0.014844\n",
      "Epoch [9860/10000], Loss: 0.014656\n",
      "Epoch [9870/10000], Loss: 0.014650\n",
      "Epoch [9880/10000], Loss: 0.014702\n",
      "Epoch [9890/10000], Loss: 0.014460\n",
      "Epoch [9900/10000], Loss: 0.014536\n",
      "Epoch [9910/10000], Loss: 0.014444\n",
      "Epoch [9920/10000], Loss: 0.014439\n",
      "Epoch [9930/10000], Loss: 0.014479\n",
      "Epoch [9940/10000], Loss: 0.014283\n",
      "Epoch [9950/10000], Loss: 0.014189\n",
      "Epoch [9960/10000], Loss: 0.014311\n",
      "Epoch [9970/10000], Loss: 0.014075\n",
      "Epoch [9980/10000], Loss: 0.014252\n",
      "Epoch [9990/10000], Loss: 0.014196\n",
      "Epoch [10000/10000], Loss: 0.013959\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000 # 1 epoch, 1 traversal through the training set\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SNet = SNet.to(device)\n",
    "SNet.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() #zero out the gradients\n",
    "    outputs = SNet(X_train) #run the forward pass\n",
    "    loss = criterion(outputs, y_train) # compute the loss\n",
    "    loss.backward() #gradient calculation happens from here\n",
    "    optimizer.step() #takes the step based on the gradients and lr\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b005579d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m SNet.eval()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     outputs = SNet(\u001b[43mX_test\u001b[49m)\n\u001b[32m      4\u001b[39m     _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m     _, actual = torch.max(y_test.data, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "SNet.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = SNet(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    _, actual = torch.max(y_test.data, 1)\n",
    "\n",
    "    accuracy = (predicted == actual).sum().item()/actual.size(0)*100.0\n",
    "\n",
    "    print(f'Test Accuracy : {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0b7318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "SNet.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = SNet(X_train)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    _, actual = torch.max(y_train.data, 1)\n",
    "\n",
    "    accuracy = (predicted == actual).sum().item()/actual.size(0)*100.0\n",
    "\n",
    "    print(f'Training Accuracy : {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "731d6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(SNet.state_dict(), \"TicTacToe_Weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
